{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7528396d",
   "metadata": {},
   "source": [
    "### Runtime & Environment Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7ebe8",
   "metadata": {},
   "source": [
    "### Core Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12393f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizer, DebertaV2Model, DebertaV2Tokenizer,\n",
    "    RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_curve, auc, precision_recall_curve, roc_auc_score, average_precision_score, classification_report,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ca253",
   "metadata": {},
   "source": [
    "### Data Loading & Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Fill NaN values in 'Extracted Code' column with empty strings\n",
    "df['Extracted Code'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10321242",
   "metadata": {},
   "source": [
    "### Custom PyTorch Dataset (Text + Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcea719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCodeDataset(Dataset):\n",
    "    def __init__(self, texts, codes, labels, tokenizer_text, tokenizer_code, max_length=512):\n",
    "        # Initialize dataset with texts, codes, labels, and tokenizers for each modality\n",
    "        self.texts = texts\n",
    "        self.codes = codes\n",
    "        self.labels = labels\n",
    "        self.tokenizer_text = tokenizer_text\n",
    "        self.tokenizer_code = tokenizer_code\n",
    "        self.max_length = max_length  # Maximum token length for padding/truncation\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of samples in the dataset\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the text, code, and label at the given index\n",
    "        text = str(self.texts[idx])   # Ensure text is a string\n",
    "        code = str(self.codes[idx])   # Ensure code is a string\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize text input using the text tokenizer\n",
    "        text_inputs = self.tokenizer_text(\n",
    "            text, padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize code input using the code tokenizer\n",
    "        code_inputs = self.tokenizer_code(\n",
    "            code, padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Extract token IDs and attention masks, remove extra batch dimension\n",
    "        input_ids_text = text_inputs['input_ids'].squeeze()\n",
    "        attention_mask_text = text_inputs['attention_mask'].squeeze()\n",
    "        input_ids_code = code_inputs['input_ids'].squeeze()\n",
    "        attention_mask_code = code_inputs['attention_mask'].squeeze()\n",
    "\n",
    "        # Return a dictionary of tokenized inputs and corresponding label\n",
    "        return {\n",
    "            'input_ids_text': input_ids_text,\n",
    "            'attention_mask_text': attention_mask_text,\n",
    "            'input_ids_code': input_ids_code,\n",
    "            'attention_mask_code': attention_mask_code,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aef2d6",
   "metadata": {},
   "source": [
    "### Dual-Encoder Model (DeBERTa-v3 for text + CodeBERT for code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Load pretrained transformer models for text and code\n",
    "        self.text_model = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
    "        self.code_model = RobertaModel.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "        # Get hidden sizes from each model's configuration\n",
    "        text_hidden_size = self.text_model.config.hidden_size\n",
    "        code_hidden_size = self.code_model.config.hidden_size\n",
    "\n",
    "        # Align code model's hidden dimension to match text model's hidden size\n",
    "        self.adjust_code_hidden_size = nn.Linear(code_hidden_size, text_hidden_size)\n",
    "\n",
    "        # Combined hidden size after concatenating text and (aligned) code representations\n",
    "        combined_hidden_size = text_hidden_size + text_hidden_size  # after aligning code -> text size\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Multi-layer feedforward classifier network\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512)\n",
    "        )\n",
    "\n",
    "        # Skip connection to directly project combined embeddings\n",
    "        self.skip_linear = nn.Linear(combined_hidden_size, 512)\n",
    "\n",
    "        # Final classifier takes concatenated outputs from both classifier and skip path\n",
    "        self.final_classifier = nn.Linear(1024, 2)  # concatenate classifier_output (512) + skip_output (512)\n",
    "\n",
    "    def forward(self, input_ids_text, attention_mask_text, input_ids_code, attention_mask_code=None):\n",
    "        # Forward pass through text and code transformer encoders\n",
    "        outputs_text = self.text_model(input_ids=input_ids_text, attention_mask=attention_mask_text)\n",
    "        outputs_code = self.code_model(input_ids=input_ids_code, attention_mask=attention_mask_code)\n",
    "\n",
    "        # Extract [CLS] token embeddings from both models\n",
    "        pooled_output_text = outputs_text.last_hidden_state[:, 0]\n",
    "        pooled_output_code = self.adjust_code_hidden_size(outputs_code.last_hidden_state[:, 0])\n",
    "\n",
    "        # Concatenate text and code embeddings along the feature dimension\n",
    "        combined_output = torch.cat((pooled_output_text, pooled_output_code), dim=1)\n",
    "        combined_output = self.dropout(combined_output)\n",
    "\n",
    "        # Pass through the main classifier and the skip connection\n",
    "        classifier_output = self.classifier(combined_output)\n",
    "        skip_output = self.skip_linear(combined_output)\n",
    "\n",
    "        # Concatenate both outputs for the final classification stage\n",
    "        final_input = torch.cat((classifier_output, skip_output), dim=1)\n",
    "\n",
    "        # Compute final logits (binary classification output)\n",
    "        logits = self.final_classifier(final_input)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0b0d2",
   "metadata": {},
   "source": [
    "### Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer for natural language text (DeBERTa v3)\n",
    "tokenizer_deberta = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "# Load pretrained tokenizer for source code (CodeBERT)\n",
    "tokenizer_codebert = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd92a99",
   "metadata": {},
   "source": [
    "### Training & Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler, class_weights):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Define loss function with class weighting to handle imbalance\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Reset gradients before each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs to device (GPU/CPU), excluding labels\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Get predicted class indices\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Update accuracy metrics\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update model parameters and learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log progress every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)} - Training Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Compute average loss and overall accuracy\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return average_loss, train_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    # Set model to evaluation mode (disables dropout, gradient tracking)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    # Disable gradient calculation for faster evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            # Move inputs and labels to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass through model\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Compute validation loss\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert predictions to class indices and move to CPU\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Collect predictions and true labels for metrics\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Log progress every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Batch {batch_idx + 1}/{len(val_loader)} - Validation Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='binary', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Return average loss and evaluation metrics\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1, predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68baaf2b",
   "metadata": {},
   "source": [
    "### Plotting Helpers (Loss, Accuracy, ROC, PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adde798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_validation_loss(train_losses, val_losses):\n",
    "    # Generate a list of epoch numbers for the x-axis\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Create a new figure for the loss plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot as line graphs if multiple epochs are present\n",
    "    if len(train_losses) > 1:\n",
    "        plt.plot(epochs, train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    else:\n",
    "        # Use scatter plot if only a single epoch (for clarity)\n",
    "        plt.scatter(epochs, train_losses, label='Training Loss')\n",
    "        plt.scatter(epochs, val_losses, label='Validation Loss')\n",
    "        plt.xlim(0.5, 1.5)  # Adjust x-axis limits for single-point display\n",
    "\n",
    "    # Add title, labels, ticks, and legend\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss')\n",
    "    plt.xticks(epochs); plt.legend(); plt.show()\n",
    "\n",
    "\n",
    "def plot_training_validation_accuracy(train_accuracies, val_accuracies):\n",
    "    # Generate a list of epoch numbers for the x-axis\n",
    "    epochs = range(1, len(train_accuracies) + 1)\n",
    "\n",
    "    # Create a new figure for the accuracy plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot line or scatter based on the number of epochs\n",
    "    if len(train_accuracies) > 1:\n",
    "        plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    else:\n",
    "        # For single epoch, use scatter plot to mark points\n",
    "        plt.scatter(epochs, train_accuracies, label='Training Accuracy')\n",
    "        plt.scatter(epochs, val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlim(0.5, 1.5)  # Adjust x-axis limits\n",
    "\n",
    "    # Add title, labels, and legend for readability\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Accuracy')\n",
    "    plt.xticks(epochs); plt.legend(); plt.show()\n",
    "\n",
    "\n",
    "def plot_auc_roc_curve(true_labels, predictions):\n",
    "    # Compute False Positive Rate, True Positive Rate, and thresholds\n",
    "    fpr, tpr, _ = roc_curve(true_labels, predictions)\n",
    "\n",
    "    # Calculate the Area Under the Curve (AUC)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Create a new figure for the ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the ROC curve and the diagonal reference line\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "\n",
    "    # Set axis limits and labels\n",
    "    plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\"); plt.show()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(true_labels, predictions):\n",
    "    # Compute precision, recall, and thresholds\n",
    "    precision, recall, _ = precision_recall_curve(true_labels, predictions)\n",
    "\n",
    "    # Calculate the Area Under the Precision-Recall Curve (AUC)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    # Create a new figure for the Precision-Recall curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot Precision vs Recall curve\n",
    "    plt.plot(recall, precision, lw=2, label=f'PR (AUC = {pr_auc:.2f})')\n",
    "\n",
    "    # Add axis labels, title, and legend\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"upper right\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b6ccc",
   "metadata": {},
   "source": [
    "### Saving Model & Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba16f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_tokenizer(model, tokenizer_text, tokenizer_code, path='.'):\n",
    "    # Create the target directory if it doesn't already exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Save the model's state dictionary (weights and biases)\n",
    "    torch.save(model.state_dict(), os.path.join(path, 'model_state_dict.pt'))\n",
    "\n",
    "    # Save the text tokenizer configuration and vocabulary\n",
    "    tokenizer_text.save_pretrained(os.path.join(path, 'tokenizer_text'))\n",
    "\n",
    "    # Save the code tokenizer configuration and vocabulary\n",
    "    tokenizer_code.save_pretrained(os.path.join(path, 'tokenizer_code'))\n",
    "\n",
    "    # Confirm save operation completion\n",
    "    print(f'Model state dictionary and tokenizers saved to {path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39d438",
   "metadata": {},
   "source": [
    "### Hyperparameters & Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 512\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bddf26",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation (Model Selection + Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(dataset, class_weights, k_folds=10):\n",
    "    # Stratified K-Fold to preserve label distribution across folds\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    # Track global best model metrics across all folds\n",
    "    best_auc = 0\n",
    "    best_model_info = {}\n",
    "\n",
    "    # To store curves for the overall best model\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    fold_train_accuracies = []\n",
    "    fold_val_accuracies = []\n",
    "\n",
    "    # Split indices for each fold using labels for stratification\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(dataset)), dataset.labels)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "\n",
    "        # Initialize a new model and optimizer per fold\n",
    "        model = CombinedModel().to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Samplers for train/validation subsets\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        # DataLoaders using the samplers\n",
    "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "\n",
    "        # Free unused GPU memory and set up LR scheduler\n",
    "        torch.cuda.empty_cache()\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * EPOCHS\n",
    "        )\n",
    "\n",
    "        # Per-epoch logs for this fold\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "        epoch_train_accuracies = []\n",
    "        epoch_val_accuracies = []\n",
    "\n",
    "        # Best trackers within the fold (for early stopping & best snapshot)\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_accuracy = 0\n",
    "        early_stopping_counter = 0\n",
    "        best_fold_metrics = {\n",
    "            'epoch': 0, 'val_accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0,\n",
    "            'train_loss': 0, 'train_accuracy': 0, 'val_loss': 0, 'auc_score': 0\n",
    "        }\n",
    "        best_true_labels = []\n",
    "        best_predictions = []\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Train/validate over epochs for this fold\n",
    "        for epoch in range(EPOCHS):\n",
    "            # One epoch of training and evaluation\n",
    "            train_loss, train_accuracy = train(model, train_loader, optimizer, scheduler, class_weights)\n",
    "            val_loss, val_accuracy, val_precision, val_recall, val_f1, predictions, true_labels = evaluate(model, val_loader)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Log epoch metrics\n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_val_losses.append(val_loss)\n",
    "            epoch_train_accuracies.append(train_accuracy)\n",
    "            epoch_val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Compute AUC for model selection within the fold\n",
    "            epoch_auc_score = roc_auc_score(true_labels, predictions)\n",
    "\n",
    "            # Console summary for this epoch\n",
    "            print(\n",
    "                f'Fold {fold+1}, Epoch {epoch+1}, '\n",
    "                f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "                f'Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, '\n",
    "                f'Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}, '\n",
    "                f'AUC: {epoch_auc_score:.4f}'\n",
    "            )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Improvement check: prioritize lower val loss; tie-break with higher AUC\n",
    "            improved = (val_loss < best_val_loss) or (val_loss <= best_val_loss and epoch_auc_score > best_fold_metrics['auc_score'])\n",
    "            if improved:\n",
    "                # Update best fold snapshot\n",
    "                best_val_loss = min(val_loss, best_val_loss)\n",
    "                best_fold_metrics = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'precision': val_precision,\n",
    "                    'recall': val_recall,\n",
    "                    'f1': val_f1,\n",
    "                    'train_loss': train_loss,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'val_loss': val_loss,\n",
    "                    'auc_score': epoch_auc_score\n",
    "                }\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_true_labels = true_labels\n",
    "                best_predictions = predictions\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                # Increment early stopping counter and break if patience exceeded\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "        # Plot full-epoch curves for this fold\n",
    "        print(\"Training-Validation Loss Curve for all epochs (this fold)\")\n",
    "        plot_training_validation_loss(epoch_train_losses, epoch_val_losses)\n",
    "\n",
    "        print(\"Training-Validation Accuracy Curve for all epochs (this fold)\")\n",
    "        plot_training_validation_accuracy(epoch_train_accuracies, epoch_val_accuracies)\n",
    "\n",
    "        # Plot curves truncated up to the best epoch for clarity\n",
    "        best_epoch = best_fold_metrics['epoch']\n",
    "        print(f\"Curves up to best epoch ({best_epoch})\")\n",
    "        plot_training_validation_loss(epoch_train_losses[:best_epoch], epoch_val_losses[:best_epoch])\n",
    "        plot_training_validation_accuracy(epoch_train_accuracies[:best_epoch], epoch_val_accuracies[:best_epoch])\n",
    "\n",
    "        # Plot ROC and PR curves for the best snapshot in this fold\n",
    "        print(\"AUC-ROC curve for the fold-wise best model\")\n",
    "        plot_auc_roc_curve(best_true_labels, best_predictions)\n",
    "\n",
    "        print(\"Precision-Recall curve for the fold-wise best model\")\n",
    "        plot_precision_recall_curve(best_true_labels, best_predictions)\n",
    "\n",
    "        # Fold summary of best metrics\n",
    "        print(\n",
    "            f\"Fold {fold+1} — Best Epoch: {best_fold_metrics['epoch']}, \"\n",
    "            f\"AUC: {best_fold_metrics['auc_score']:.4f}, \"\n",
    "            f\"Train Loss: {best_fold_metrics['train_loss']:.4f}, Val Loss: {best_fold_metrics['val_loss']:.4f}, \"\n",
    "            f\"Train Acc: {best_fold_metrics['train_accuracy']:.4f}, Val Acc: {best_fold_metrics['val_accuracy']:.4f}, \"\n",
    "            f\"Precision: {best_fold_metrics['precision']:.4f}, Recall: {best_fold_metrics['recall']:.4f}, \"\n",
    "            f\"F1: {best_fold_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Update global best model across folds:\n",
    "        # Prefer lower val loss; tie-break on higher AUC\n",
    "        if (best_fold_metrics['val_loss'] < best_model_info.get('val_loss', float('inf'))) or \\\n",
    "           (best_fold_metrics['val_loss'] <= best_model_info.get('val_loss', float('inf')) and\n",
    "            best_fold_metrics['auc_score'] > best_model_info.get('auc', 0)):\n",
    "            best_auc = best_fold_metrics['auc_score']\n",
    "            best_model_info = {\n",
    "                'state_dict': best_model_wts,\n",
    "                'fold': fold,\n",
    "                'epoch': best_fold_metrics['epoch'],\n",
    "                'auc': best_auc,\n",
    "                'val_loss': best_fold_metrics['val_loss'],\n",
    "                'performance': (\n",
    "                    best_fold_metrics['train_accuracy'],\n",
    "                    best_fold_metrics['val_accuracy'],\n",
    "                    best_fold_metrics['precision'],\n",
    "                    best_fold_metrics['recall'],\n",
    "                    best_fold_metrics['f1']\n",
    "                ),\n",
    "                'train_losses': epoch_train_losses,\n",
    "                'val_losses': epoch_val_losses,\n",
    "                'train_accuracies': epoch_train_accuracies,\n",
    "                'val_accuracies': epoch_val_accuracies\n",
    "            }\n",
    "            # Store data for final overall plots\n",
    "            fold_train_losses = epoch_train_losses\n",
    "            fold_val_losses = epoch_val_losses\n",
    "            fold_train_accuracies = epoch_train_accuracies\n",
    "            fold_val_accuracies = epoch_val_accuracies\n",
    "            fold_true_labels = best_true_labels\n",
    "            fold_predictions = best_predictions\n",
    "\n",
    "    # Final cleanup before overall plots\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Plot curves for the overall best model across folds (all epochs)\n",
    "    print(\"Train-Val Loss Curve for all epochs — Overall Best Model\")\n",
    "    plot_training_validation_loss(fold_train_losses, fold_val_losses)\n",
    "\n",
    "    print(\"Train-Val Accuracy Curve for all epochs — Overall Best Model\")\n",
    "    plot_training_validation_accuracy(fold_train_accuracies, fold_val_accuracies)\n",
    "\n",
    "    # Plot curves truncated to the best epoch for the overall best model\n",
    "    print(\"Train-Val Loss curve up to best epoch — Overall Best Model\")\n",
    "    plot_training_validation_loss(fold_train_losses[:best_model_info['epoch']], fold_val_losses[:best_model_info['epoch']])\n",
    "\n",
    "    print(\"Train-Val Accuracy curve up to best epoch — Overall Best Model\")\n",
    "    plot_training_validation_accuracy(fold_train_accuracies[:best_model_info['epoch']], fold_val_accuracies[:best_model_info['epoch']])\n",
    "\n",
    "    # Plot PR and ROC for the overall best model\n",
    "    print(\"Precision-Recall curve — Overall Best Model\")\n",
    "    plot_precision_recall_curve(fold_true_labels, fold_predictions)\n",
    "\n",
    "    print(\"AUC-ROC curve — Overall Best Model\")\n",
    "    plot_auc_roc_curve(fold_true_labels, fold_predictions)\n",
    "\n",
    "    # Save & package best model: persist state dict and full artifacts (model + tokenizers)\n",
    "    torch.save(best_model_info['state_dict'], 'deberta_codebert.pt')\n",
    "    model.load_state_dict(torch.load('deberta_codebert.pt'))\n",
    "    save_model_and_tokenizer(model, tokenizer_deberta, tokenizer_codebert, 'sm_deberta_codebert/deberta_codebert')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Final summary of which fold/epoch produced the best model\n",
    "    print(f\"Best model from fold {best_model_info['fold']+1} with AUC: {best_auc:.4f} at epoch {best_model_info['epoch']} saved.\")\n",
    "    return best_model_info['performance']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d4a23",
   "metadata": {},
   "source": [
    "### Train/Val Split, Class Weights & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc23919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the full dataframe into training and test sets\n",
    "# - 10% of the data is reserved for testing\n",
    "# - Stratified split ensures class balance is preserved (based on 'TDR' column)\n",
    "# - Random seed fixed for reproducibility\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.1, stratify=df['TDR'], random_state=42\n",
    ")\n",
    "\n",
    "# Extract class labels from the training data\n",
    "labels = train_df['TDR'].tolist()\n",
    "\n",
    "# Compute class weights to handle imbalanced classification\n",
    "# 'balanced' mode assigns weights inversely proportional to class frequencies\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "# Convert computed weights into a PyTorch tensor and move to the correct device (GPU/CPU)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Save the held-out test subset for future evaluation or reproducibility\n",
    "test_df.to_csv('test_deberta_codebert.csv', index=False)\n",
    "\n",
    "# Build the custom dataset for training\n",
    "# - Text input comes from column 'TB'\n",
    "# - Code input comes from column 'Extracted Code'\n",
    "# - Labels are from column 'TDR'\n",
    "dataset = TextCodeDataset(\n",
    "    train_df['TB'].tolist(),\n",
    "    train_df['Extracted Code'].tolist(),\n",
    "    train_df['TDR'].tolist(),\n",
    "    tokenizer_deberta,\n",
    "    tokenizer_codebert,\n",
    "    MAX_LENGTH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dbf45",
   "metadata": {},
   "source": [
    "### Run Cross-Validation & Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a990072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation on the prepared dataset\n",
    "# This will train and evaluate the model across multiple folds,\n",
    "# returning the performance metrics (train acc, val acc, precision, recall, F1)\n",
    "fold_performance = cross_validate(dataset, class_weights_tensor)\n",
    "\n",
    "# Display the overall best model's performance metrics after cross-validation\n",
    "print(\n",
    "    f'Overall Performance: '\n",
    "    f'Train Acc: {fold_performance[0]:.4f}, '\n",
    "    f'Val Acc: {fold_performance[1]:.4f}, '\n",
    "    f'Precision: {fold_performance[2]:.4f}, '\n",
    "    f'Recall: {fold_performance[3]:.4f}, '\n",
    "    f'F1: {fold_performance[4]:.4f}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d6306",
   "metadata": {},
   "source": [
    "### Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new instance of the combined DeBERTa + CodeBERT model\n",
    "model = CombinedModel().to(device)\n",
    "\n",
    "# Load the previously saved model weights into the initialized model\n",
    "# 'map_location=device' ensures compatibility with the current hardware (CPU or GPU)\n",
    "model.load_state_dict(torch.load('deberta_codebert.pt', map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode (disables dropout, gradient updates, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully and set to evaluation mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a57d06",
   "metadata": {},
   "source": [
    "### Prepare the Test Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the held-out test split from the CSV file saved earlier\n",
    "test_df = pd.read_csv('test_deberta_codebert.csv')\n",
    "\n",
    "# Build a dataset for the test data\n",
    "# - 'TB' column contains the natural language text\n",
    "# - 'Extracted Code' column contains the source code snippets\n",
    "# - 'TDR' column provides the ground truth labels\n",
    "test_dataset = TextCodeDataset(\n",
    "    test_df['TB'].tolist(),\n",
    "    test_df['Extracted Code'].tolist(),\n",
    "    test_df['TDR'].tolist(),\n",
    "    tokenizer_deberta,\n",
    "    tokenizer_codebert,\n",
    "    MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Create a DataLoader for efficient batched inference \n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Confirm successful preparation of the test dataset\n",
    "print(f\"Test set prepared with {len(test_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a379c02",
   "metadata": {},
   "source": [
    "### Run Inference on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to collect ground-truth labels, predicted classes, and prediction scores\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_scores = []\n",
    "\n",
    "# Define softmax function to convert logits into probability distributions\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# Disable gradient computation for efficient inference\n",
    "with torch.no_grad():\n",
    "    # Iterate through the test DataLoader in batches\n",
    "    for batch in test_loader:\n",
    "        # Move all inputs (except labels) to the target device (CPU/GPU)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass through the model to obtain raw logits\n",
    "        logits = model(**inputs)\n",
    "\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        # Get predicted class indices (0 or 1)\n",
    "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "        # Extract probabilities for the positive class (index 1)\n",
    "        scores = probs[:, 1].cpu().numpy()\n",
    "\n",
    "        # Store labels, predictions, and positive-class probabilities\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds)\n",
    "        all_scores.extend(scores)\n",
    "\n",
    "# Confirm that inference has finished successfully\n",
    "print(\"Inference complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6b179",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute key classification metrics using true and predicted labels\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the binary classification task\n",
    "# 'zero_division=0' avoids division errors when there are no positive predictions\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average='binary', zero_division=0\n",
    ")\n",
    "\n",
    "# Compute ROC AUC using true labels and positive-class prediction scores\n",
    "roc_auc = roc_auc_score(all_labels, all_scores)\n",
    "\n",
    "# Compute Average Precision (area under the precision-recall curve)\n",
    "ap_score = average_precision_score(all_labels, all_scores)\n",
    "\n",
    "# Display the calculated performance metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Average Precision (PR AUC): {ap_score:.4f}\")\n",
    "\n",
    "# Print a detailed classification report showing per-class metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee59e4",
   "metadata": {},
   "source": [
    "### Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9182a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix comparing true vs. predicted labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Create a visualization object for the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix using a blue color map for better contrast\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Add a descriptive title for clarity\n",
    "plt.title(\"Confusion Matrix — Test Set\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb7843",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model prediction labels to the test DataFrame\n",
    "test_df['pred_label'] = all_preds\n",
    "\n",
    "# Add predicted positive-class probabilities (confidence scores) to the DataFrame\n",
    "test_df['pred_score'] = all_scores\n",
    "\n",
    "# Save the updated DataFrame containing predictions and scores to a CSV file\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "# Confirm successful export of prediction results\n",
    "print(\"Predictions saved to 'test_predictions.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
