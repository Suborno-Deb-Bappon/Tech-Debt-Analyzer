Id,Id Link,Title,Body,R_Body,TDR,Cleaned Body,TB,Debugging,Implementation,Conceptual Understanding,Best Practices,Code Review,Learning Resources,Ry TDR,Ry Reasons,Suborno (Comments regarding Ryder labelling),Agree with My Comments? (yes/No),S TDR,Final TDR,S Q Topic,Ry Q Topic,Ry Q Topic Reason,Final Q Topic,Ry_Debugging,Ry_Implementation,Ry_Conceptual Understanding,Ry_Best Practices,Ry_Code Review,Ry_Learning Resources,Ry Q Type Reason,Final Q Type,S TD Type,Ry TD Type,Ry TD Type Reason,Final TD Type,S TD Urgency,Ry TD Urgency,Ry TD Urgency Reason,Final TD Urgency
22401289,22401289,how can get high performance when foreach DataRow in a DataTable in my code?,"<p>I get a datatable (<strong>ora_dt</strong>) from an Oracle DB, now i need to add an column(<strong>colu_sql</strong>) in <strong>ora_dt</strong>,however i have to get <strong>colu_sql</strong>'s value from some Sqlserver DBs. </p>

<p>Here is my code:</p>

<pre><code>
public void ProcessDataTable(DataSet _ds)
{
    _ds.Tables[0].Columns.Add(new DataColumn(""Descpition"", typeof(string)));

    int countryId = -1;
    string des_ID = string.Empty;
    string geo = string.Empty;

    foreach (DataRow row in _ds.Tables[0].Rows)
    {
        if (row[""des_ID""] != DBNull.Value)
            des_ID = row[""des_ID""].ToString();

        if (!string.IsNullOrEmpty(des_ID))
        {
            if (countryId == 12 || countryId == 13)
                geo = ""NA"";
            else if ((countryId == 10 || countryId == 11))
                geo = ""LA"";
            else
                geo = ""EMEA"";
            row[""Descpition""] = GetDes(geo, des_ID);
        }
        else { row[""ExemptionDes""] = string.Empty; }
    }
}     
</code></pre>

<p>For every DataRow, inorder to get row[""Descpition""] value, i have to check its <strong>geo</strong> and <strong>des_id</strong>, and select them from another SqlserverDB.</p>

<p>If the row counts in DataTable is very large, then when i foreach the DataTable, i have to visit the sqlserver db many times, it makes performance bad, </p>

<p>Actually i cannot add new column description in Oracle.how can get high performance when foreach DataRow in a DataTable in my code?</p>

<p><code><pre>
private string GetDes(string geo, string des_ID)
{
    string description = string.Empty;
    string query = ""select description from geo_exemption  where des_ID= "" + des_ID;
    Database DbSQL = DbSQLFactory.CreateDatabase(geo);
    using (DataReader dr = DbSQL.ExecuteReader(sqlCmd))
    {
        while (dr.Read())
        {
            if (dr[""description""] != null)
                description = dr[""description""].ToString();
        }
        dr.Close();
    }
    return description;
}
</pre></code></p>
","i get a datatable ora_dt from an oracle db now i need to add an columncolu_sql in ora_dthowever i have to get colu_sqls value from some sqlserver dbs 
here is my code
for every datarow inorder to get rowdescpition value i have to check its geo and des_id and select them from another sqlserverdb
if the row counts in datatable is very large then when i foreach the datatable i have to visit the sqlserver db many times it makes performance bad 
actually i cannot add new column description in oraclehow can get high performance when foreach datarow in a datatable in my code

",0,"I get a datatable (ora_dt) from an Oracle DB, now i need to add an column(colu_sql) in ora_dt,however i have to get colu_sql's value from some Sqlserver DBs. 
Here is my code:

public void ProcessDataTable(DataSet _ds)
{
    _ds.Tables[0].Columns.Add(new DataColumn(""Descpition"", typeof(string)));

    int countryId = -1;
    string des_ID = string.Empty;
    string geo = string.Empty;

    foreach (DataRow row in _ds.Tables[0].Rows)
    {
        if (row[""des_ID""] != DBNull.Value)
            des_ID = row[""des_ID""].ToString();

        if (!string.IsNullOrEmpty(des_ID))
        {
            if (countryId == 12 || countryId == 13)
                geo = ""NA"";
            else if ((countryId == 10 || countryId == 11))
                geo = ""LA"";
            else
                geo = ""EMEA"";
            row[""Descpition""] = GetDes(geo, des_ID);
        }
        else { row[""ExemptionDes""] = string.Empty; }
    }
}     

For every DataRow, inorder to get row[""Descpition""] value, i have to check its geo and des_id, and select them from another SqlserverDB.
If the row counts in DataTable is very large, then when i foreach the DataTable, i have to visit the sqlserver db many times, it makes performance bad, 
Actually i cannot add new column description in Oracle.how can get high performance when foreach DataRow in a DataTable in my code?

private string GetDes(string geo, string des_ID)
{
    string description = string.Empty;
    string query = ""select description from geo_exemption  where des_ID= "" + des_ID;
    Database DbSQL = DbSQLFactory.CreateDatabase(geo);
    using (DataReader dr = DbSQL.ExecuteReader(sqlCmd))
    {
        while (dr.Read())
        {
            if (dr[""description""] != null)
                description = dr[""description""].ToString();
        }
        dr.Close();
    }
    return description;
}

","how can get high performance when foreach DataRow in a DataTable in my code?
I get a datatable (ora_dt) from an Oracle DB, now i need to add an column(colu_sql) in ora_dt,however i have to get colu_sql's value from some Sqlserver DBs. 
Here is my code:

public void ProcessDataTable(DataSet _ds)
{
    _ds.Tables[0].Columns.Add(new DataColumn(""Descpition"", typeof(string)));

    int countryId = -1;
    string des_ID = string.Empty;
    string geo = string.Empty;

    foreach (DataRow row in _ds.Tables[0].Rows)
    {
        if (row[""des_ID""] != DBNull.Value)
            des_ID = row[""des_ID""].ToString();

        if (!string.IsNullOrEmpty(des_ID))
        {
            if (countryId == 12 || countryId == 13)
                geo = ""NA"";
            else if ((countryId == 10 || countryId == 11))
                geo = ""LA"";
            else
                geo = ""EMEA"";
            row[""Descpition""] = GetDes(geo, des_ID);
        }
        else { row[""ExemptionDes""] = string.Empty; }
    }
}     

For every DataRow, inorder to get row[""Descpition""] value, i have to check its geo and des_id, and select them from another SqlserverDB.
If the row counts in DataTable is very large, then when i foreach the DataTable, i have to visit the sqlserver db many times, it makes performance bad, 
Actually i cannot add new column description in Oracle.how can get high performance when foreach DataRow in a DataTable in my code?

private string GetDes(string geo, string des_ID)
{
    string description = string.Empty;
    string query = ""select description from geo_exemption  where des_ID= "" + des_ID;
    Database DbSQL = DbSQLFactory.CreateDatabase(geo);
    using (DataReader dr = DbSQL.ExecuteReader(sqlCmd))
    {
        while (dr.Read())
        {
            if (dr[""description""] != null)
                description = dr[""description""].ToString();
        }
        dr.Close();
    }
    return description;
}

",0,0,0,1,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,0,1,0,0,,,Infrastructure,Infrastructure,,Infrastructure,Very Severe,Very Severe,,Very Severe
51444920,51444920,C# Find properties shared by two types of the same hierarchy,"<p>I need to copy the value of properties from one class to another that are descendants of the same base type. Source and target object may be on different levels of the same inheritance branch, meaning one is derived from the other, or be descendant of different branches, meaning they share a common base type.</p>

<pre><code>      A
  B1      B2
C1 C2      C3
</code></pre>

<p>From the structure above I may want to copy all properties from A to C1, C2 to C3, C3 to B1, etc. Basically any possible combination from the tree. Obviously I can only copy properties present in the source type, that <strong>must also</strong> be present in the target type.</p>

<p>Iterating the properties of the source type is easy as</p>

<pre><code>var sourceProperties = source.GetType().GetProperties();
</code></pre>

<p>However how do I check which property is declared on the target type? Simply checking by name is not enough, as they might have different types. Also in the past I made bad experiences with duplicate properties using <code>new</code>.</p>

<p>Unfortunately C# or .NET has no built-in method to check if a type has a certain <code>PropertyInfo</code> like <code>Type.HasProperty(PropertyInfo)</code>. The best I could come up with is to check if the property was declared by a shared base type.</p>

<pre><code>public static void CopyProperties(object source, object target)
{
    var targetType = target.GetType();
    var sharedProperties =source.GetType().GetProperties()
        .Where(p =&gt; p.DeclaringType.IsAssignableFrom(targetType));

    foreach (var property in sharedProperties)
    {
        var value = property.GetValue(source);
        if (property.CanWrite)
            property.SetValue(target, value);
    }
}
</code></pre>

<p><strong>Question:</strong> Is there a better solution?</p>
","i need to copy the value of properties from one class to another that are descendants of the same base type source and target object may be on different levels of the same inheritance branch meaning one is derived from the other or be descendant of different branches meaning they share a common base type
from the structure above i may want to copy all properties from a to c1 c2 to c3 c3 to b1 etc basically any possible combination from the tree obviously i can only copy properties present in the source type that must also be present in the target type
iterating the properties of the source type is easy as
however how do i check which property is declared on the target type simply checking by name is not enough as they might have different types also in the past i made bad experiences with duplicate properties using new
unfortunately c or net has no builtin method to check if a type has a certain propertyinfo like typehaspropertypropertyinfo the best i could come up with is to check if the property was declared by a shared base type
question is there a better solution
",1,"I need to copy the value of properties from one class to another that are descendants of the same base type. Source and target object may be on different levels of the same inheritance branch, meaning one is derived from the other, or be descendant of different branches, meaning they share a common base type.
      A
  B1      B2
C1 C2      C3

From the structure above I may want to copy all properties from A to C1, C2 to C3, C3 to B1, etc. Basically any possible combination from the tree. Obviously I can only copy properties present in the source type, that must also be present in the target type.
Iterating the properties of the source type is easy as
var sourceProperties = source.GetType().GetProperties();

However how do I check which property is declared on the target type? Simply checking by name is not enough, as they might have different types. Also in the past I made bad experiences with duplicate properties using new.
Unfortunately C# or .NET has no built-in method to check if a type has a certain PropertyInfo like Type.HasProperty(PropertyInfo). The best I could come up with is to check if the property was declared by a shared base type.
public static void CopyProperties(object source, object target)
{
    var targetType = target.GetType();
    var sharedProperties =source.GetType().GetProperties()
        .Where(p => p.DeclaringType.IsAssignableFrom(targetType));

    foreach (var property in sharedProperties)
    {
        var value = property.GetValue(source);
        if (property.CanWrite)
            property.SetValue(target, value);
    }
}

Question: Is there a better solution?
","C# Find properties shared by two types of the same hierarchy
I need to copy the value of properties from one class to another that are descendants of the same base type. Source and target object may be on different levels of the same inheritance branch, meaning one is derived from the other, or be descendant of different branches, meaning they share a common base type.
      A
  B1      B2
C1 C2      C3

From the structure above I may want to copy all properties from A to C1, C2 to C3, C3 to B1, etc. Basically any possible combination from the tree. Obviously I can only copy properties present in the source type, that must also be present in the target type.
Iterating the properties of the source type is easy as
var sourceProperties = source.GetType().GetProperties();

However how do I check which property is declared on the target type? Simply checking by name is not enough, as they might have different types. Also in the past I made bad experiences with duplicate properties using new.
Unfortunately C# or .NET has no built-in method to check if a type has a certain PropertyInfo like Type.HasProperty(PropertyInfo). The best I could come up with is to check if the property was declared by a shared base type.
public static void CopyProperties(object source, object target)
{
    var targetType = target.GetType();
    var sharedProperties =source.GetType().GetProperties()
        .Where(p => p.DeclaringType.IsAssignableFrom(targetType));

    foreach (var property in sharedProperties)
    {
        var value = property.GetValue(source);
        if (property.CanWrite)
            property.SetValue(target, value);
    }
}

Question: Is there a better solution?
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
40196002,40196002,Implement binary search using the `Collections.binarySearch` signature,"<p>Here is my attempt at implementing the binary search that must follow:</p>

<pre><code>static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key)
static &lt;T&gt; int binarySearch(List&lt;? extends T&gt; list, T key, Comparator&lt;? super T&gt; c)
</code></pre>

<p>However, I would like to avoid code duplication and delegate one of the implementations to the other (currently, the first to the second). To do that I need to get rid of the wildcard <code>?</code> and use a second generic type like so:</p>

<pre><code>public class BinarySearch {
    public static &lt;Q extends Comparable&lt;? super T&gt;, T&gt;
    int search(List&lt;Q&gt; xs, T x) {
        return search(xs, x, Q::compareTo);
    }

    public static &lt;T&gt;
    int search(List&lt;? extends T&gt; xs, T x, Comparator&lt;? super T&gt; cmp) {
        int l = 0, r = xs.size() - 1;

        while (l &lt;= r) {
            int mid = l + (r - l) / 2;

            if (cmp.compare(xs.get(mid), x) == 0)
                return mid;

            if (cmp.compare(xs.get(mid), x) &lt; 0)
                r = mid - 1;
            else
                l = mid + 1;
        }

        return xs.size();
    }
}
</code></pre>

<p>Unfortunately, this does not compile, failing with the error:</p>

<p><code>Non-static method cannot be referenced from a static context</code></p>

<p>How could I fix that?</p>

<p><em>PS: if you wonder why the signatures from <code>Collections</code> look the way they do, here is an explanation: <a href=""https://stackoverflow.com/questions/40195450/how-do-these-two-generic-signatures-for-collections-binarysearch-differ"">How do these two generic signatures for Collections.binarySearch differ?</a></em></p>

<p><em>PPS: there used to be (a now deleted) answer that you cannot pass <code>T::compareTo</code> in place where a <code>Comparator</code> is expected. Well, I believe that you can, here is my working implementation of QuickSort that does exactly that: <a href=""https://github.com/all3fox/algos-java/blob/481f2c71952bf2c7510cb93cc1af8e90016ccf3b/src/main/java/io/github/all3fox/sort/QuickSort.java"" rel=""nofollow noreferrer"">https://github.com/all3fox/algos-java/blob/481f2c71952bf2c7510cb93cc1af8e90016ccf3b/src/main/java/io/github/all3fox/sort/QuickSort.java</a></em></p>
","here is my attempt at implementing the binary search that must follow
however i would like to avoid code duplication and delegate one of the implementations to the other currently the first to the second to do that i need to get rid of the wildcard  and use a second generic type like so
unfortunately this does not compile failing with the error
nonstatic method cannot be referenced from a static context
how could i fix that
ps if you wonder why the signatures from collections look the way they do here is an explanation how do these two generic signatures for collectionsbinarysearch differ
pps there used to be a now deleted answer that you cannot pass tcompareto in place where a comparator is expected well i believe that you can here is my working implementation of quicksort that does exactly that httpsgithubcomall3foxalgosjavablob481f2c71952bf2c7510cb93cc1af8e90016ccf3bsrcmainjavaiogithuball3foxsortquicksortjava
",1,"Here is my attempt at implementing the binary search that must follow:
static <T> int binarySearch(List<? extends Comparable<? super T>> list, T key)
static <T> int binarySearch(List<? extends T> list, T key, Comparator<? super T> c)

However, I would like to avoid code duplication and delegate one of the implementations to the other (currently, the first to the second). To do that I need to get rid of the wildcard ? and use a second generic type like so:
public class BinarySearch {
    public static <Q extends Comparable<? super T>, T>
    int search(List<Q> xs, T x) {
        return search(xs, x, Q::compareTo);
    }

    public static <T>
    int search(List<? extends T> xs, T x, Comparator<? super T> cmp) {
        int l = 0, r = xs.size() - 1;

        while (l <= r) {
            int mid = l + (r - l) / 2;

            if (cmp.compare(xs.get(mid), x) == 0)
                return mid;

            if (cmp.compare(xs.get(mid), x) < 0)
                r = mid - 1;
            else
                l = mid + 1;
        }

        return xs.size();
    }
}

Unfortunately, this does not compile, failing with the error:
Non-static method cannot be referenced from a static context
How could I fix that?
PS: if you wonder why the signatures from Collections look the way they do, here is an explanation: How do these two generic signatures for Collections.binarySearch differ?
PPS: there used to be (a now deleted) answer that you cannot pass T::compareTo in place where a Comparator is expected. Well, I believe that you can, here is my working implementation of QuickSort that does exactly that: https://github.com/all3fox/algos-java/blob/481f2c71952bf2c7510cb93cc1af8e90016ccf3b/src/main/java/io/github/all3fox/sort/QuickSort.java
","Implement binary search using the `Collections.binarySearch` signature
Here is my attempt at implementing the binary search that must follow:
static <T> int binarySearch(List<? extends Comparable<? super T>> list, T key)
static <T> int binarySearch(List<? extends T> list, T key, Comparator<? super T> c)

However, I would like to avoid code duplication and delegate one of the implementations to the other (currently, the first to the second). To do that I need to get rid of the wildcard ? and use a second generic type like so:
public class BinarySearch {
    public static <Q extends Comparable<? super T>, T>
    int search(List<Q> xs, T x) {
        return search(xs, x, Q::compareTo);
    }

    public static <T>
    int search(List<? extends T> xs, T x, Comparator<? super T> cmp) {
        int l = 0, r = xs.size() - 1;

        while (l <= r) {
            int mid = l + (r - l) / 2;

            if (cmp.compare(xs.get(mid), x) == 0)
                return mid;

            if (cmp.compare(xs.get(mid), x) < 0)
                r = mid - 1;
            else
                l = mid + 1;
        }

        return xs.size();
    }
}

Unfortunately, this does not compile, failing with the error:
Non-static method cannot be referenced from a static context
How could I fix that?
PS: if you wonder why the signatures from Collections look the way they do, here is an explanation: How do these two generic signatures for Collections.binarySearch differ?
PPS: there used to be (a now deleted) answer that you cannot pass T::compareTo in place where a Comparator is expected. Well, I believe that you can, here is my working implementation of QuickSort that does exactly that: https://github.com/all3fox/algos-java/blob/481f2c71952bf2c7510cb93cc1af8e90016ccf3b/src/main/java/io/github/all3fox/sort/QuickSort.java
",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
76122890,76122890,Can I create an array of unique words from two arrays in JavaScript?,"<p>I have two arrays in JavaScript, and I want to create a new array that contains only the unique values from both arrays. How can I do this in JavaScript?</p>
<pre><code>const arr1 = ['apple', 'banana', 'cherry', 'date', 'elderberry'];
const arr2 = ['cherry', 'date', 'elderberry', 'fig', 'grape'];
</code></pre>
<p>I've tried using the <code>concat</code> method to combine the arrays and then using a <code>Set</code> to remove duplicates, but that doesn't work because the <code>Set</code> considers arrays to be different objects even if they have the same values.</p>
<p>I've also tried using a <code>for</code> loop to iterate through both arrays and an <code>if</code> statement to determine whether each word already exists in the new array, but that seems wasteful and doesn't function if the arrays include a large number of duplicate words.</p>
<p>Can someone please point me in the direction of a more reliable and efficient JavaScript method for generating an array of distinct words from two arrays? I'm grateful.</p>
","i have two arrays in javascript and i want to create a new array that contains only the unique values from both arrays how can i do this in javascript
ive tried using the concat method to combine the arrays and then using a set to remove duplicates but that doesnt work because the set considers arrays to be different objects even if they have the same values
ive also tried using a for loop to iterate through both arrays and an if statement to determine whether each word already exists in the new array but that seems wasteful and doesnt function if the arrays include a large number of duplicate words
can someone please point me in the direction of a more reliable and efficient javascript method for generating an array of distinct words from two arrays im grateful
",0,"I have two arrays in JavaScript, and I want to create a new array that contains only the unique values from both arrays. How can I do this in JavaScript?
const arr1 = ['apple', 'banana', 'cherry', 'date', 'elderberry'];
const arr2 = ['cherry', 'date', 'elderberry', 'fig', 'grape'];

I've tried using the concat method to combine the arrays and then using a Set to remove duplicates, but that doesn't work because the Set considers arrays to be different objects even if they have the same values.
I've also tried using a for loop to iterate through both arrays and an if statement to determine whether each word already exists in the new array, but that seems wasteful and doesn't function if the arrays include a large number of duplicate words.
Can someone please point me in the direction of a more reliable and efficient JavaScript method for generating an array of distinct words from two arrays? I'm grateful.
","Can I create an array of unique words from two arrays in JavaScript?
I have two arrays in JavaScript, and I want to create a new array that contains only the unique values from both arrays. How can I do this in JavaScript?
const arr1 = ['apple', 'banana', 'cherry', 'date', 'elderberry'];
const arr2 = ['cherry', 'date', 'elderberry', 'fig', 'grape'];

I've tried using the concat method to combine the arrays and then using a Set to remove duplicates, but that doesn't work because the Set considers arrays to be different objects even if they have the same values.
I've also tried using a for loop to iterate through both arrays and an if statement to determine whether each word already exists in the new array, but that seems wasteful and doesn't function if the arrays include a large number of duplicate words.
Can someone please point me in the direction of a more reliable and efficient JavaScript method for generating an array of distinct words from two arrays? I'm grateful.
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Documentation,Documentation,,Documentation,Severe,Severe,,Severe
17208490,17208490,String operations and memory management,"<p>I want to write a convenient wrapper to a C-style function <a href=""http://www.cplusplus.com/reference/ctime/strftime/"" rel=""nofollow"">strftime</a>. And I've come to some options to convert char-array to string and vise-versa. Here is my code:</p>

<pre><code>std::string Time::getAsFormattedString ( const std::string&amp; format , const size_t&amp; maxStringSize = 999 )
{

    char* timeArray = 0;
    std::string timeString;

    //  [OPTION_0]
    timeArray = reinterpret_cast &lt;char*&gt; (malloc(sizeof(char)*maxStringSize)));

    //  [OPTION_1]
    timeArray = const_cast &lt;char*&gt; (timeString.c_str());

    //  [OPTION_2]
    timeArray = &amp;(*(timeString.begin()));

    strftime(timeArray,maxStringSize,format.c_str(),&amp;this-&gt;time);
    timeString = timeArray;

    //  [OPTION_0]
    free(timeArray);

    return timeString;

}
</code></pre>

<p>â„–0 option looks safe since no exceptions can be thrown before memory freeing (Edit: <code>timeString = timeArray</code> can throw one, try-catch needed around that line)</p>

<p>â„–1 const-casting always looks like a hack</p>

<p>â„–2 seems to be the best by I do not know if there could be some issues with it</p>

<p>Can you please tell me, which one is the most safe, correct, optimal and maybe kind of best-practice.</p>

<p>Thank you.</p>
","i want to write a convenient wrapper to a cstyle function strftime and ive come to some options to convert chararray to string and viseversa here is my code
0 option looks safe since no exceptions can be thrown before memory freeing edit timestring  timearray can throw one trycatch needed around that line
1 constcasting always looks like a hack
2 seems to be the best by i do not know if there could be some issues with it
can you please tell me which one is the most safe correct optimal and maybe kind of bestpractice
thank you
",1,"I want to write a convenient wrapper to a C-style function strftime. And I've come to some options to convert char-array to string and vise-versa. Here is my code:
std::string Time::getAsFormattedString ( const std::string& format , const size_t& maxStringSize = 999 )
{

    char* timeArray = 0;
    std::string timeString;

    //  [OPTION_0]
    timeArray = reinterpret_cast <char*> (malloc(sizeof(char)*maxStringSize)));

    //  [OPTION_1]
    timeArray = const_cast <char*> (timeString.c_str());

    //  [OPTION_2]
    timeArray = &(*(timeString.begin()));

    strftime(timeArray,maxStringSize,format.c_str(),&this->time);
    timeString = timeArray;

    //  [OPTION_0]
    free(timeArray);

    return timeString;

}

â„–0 option looks safe since no exceptions can be thrown before memory freeing (Edit: timeString = timeArray can throw one, try-catch needed around that line)
â„–1 const-casting always looks like a hack
â„–2 seems to be the best by I do not know if there could be some issues with it
Can you please tell me, which one is the most safe, correct, optimal and maybe kind of best-practice.
Thank you.
","String operations and memory management
I want to write a convenient wrapper to a C-style function strftime. And I've come to some options to convert char-array to string and vise-versa. Here is my code:
std::string Time::getAsFormattedString ( const std::string& format , const size_t& maxStringSize = 999 )
{

    char* timeArray = 0;
    std::string timeString;

    //  [OPTION_0]
    timeArray = reinterpret_cast <char*> (malloc(sizeof(char)*maxStringSize)));

    //  [OPTION_1]
    timeArray = const_cast <char*> (timeString.c_str());

    //  [OPTION_2]
    timeArray = &(*(timeString.begin()));

    strftime(timeArray,maxStringSize,format.c_str(),&this->time);
    timeString = timeArray;

    //  [OPTION_0]
    free(timeArray);

    return timeString;

}

â„–0 option looks safe since no exceptions can be thrown before memory freeing (Edit: timeString = timeArray can throw one, try-catch needed around that line)
â„–1 const-casting always looks like a hack
â„–2 seems to be the best by I do not know if there could be some issues with it
Can you please tell me, which one is the most safe, correct, optimal and maybe kind of best-practice.
Thank you.
",0,1,0,1,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,0,1,0,0,,,Infrastructure,Infrastructure,,Infrastructure,None,None,,None
19347890,19347890,How to get boost::object_pool thread safe?,"<p>After asking <a href=""https://stackoverflow.com/questions/19333595/is-there-a-faster-c-heap-allocation-deallocation-mechanism-available-than-boos?noredirect=1#comment28645338_19333595"">Is there a faster heap allocation/deallocation mechanism available than boost::object_pool?</a> I got feedback that this object pool is NOT thread safe.</p>

<p>So I wrote an ObjectFactory wrapping boost::object_pool and adding mutex locks:</p>

<pre><code>#include &lt;memory&gt;
using std::shared_ptr;

#include &lt;boost/pool/object_pool.hpp&gt;
#include &lt;boost/thread/mutex.hpp&gt;

template &lt;typename T&gt;
class ObjectFactory 
{
private:
    struct SharedDeleter 
    {
        ObjectFactory&lt;T&gt;* m_pFact;

        SharedDeleter(ObjectFactory&lt;T&gt;* fact) : m_pFact(fact) {}

        inline void operator()(T* p) const 
        {      
          m_pFact-&gt;destroy(p);
        }
    };


    boost::object_pool&lt;T&gt;   m_Pool;
    boost::mutex            m_PoolMutex;

    SharedDeleter           m_Deleter;


public:
    ObjectFactory() : m_Deleter(this)
    {
    }

    template&lt;typename TType = T, typename... TArgs&gt; 
    inline TType* create(TArgs&amp;&amp;... mArgs)
    {
        boost::unique_lock&lt;boost::mutex&gt; scoped_lock(m_PoolMutex);
        return m_Pool.construct(mArgs...);
    }

    inline void destroy(T* mObj) 
    { 
        boost::unique_lock&lt;boost::mutex&gt; scoped_lock(m_PoolMutex);
        m_Pool.destroy(mObj); 
    }


    template&lt;typename TType = T, typename... TArgs&gt; 
    inline std::shared_ptr&lt;TType&gt; make_shared(TArgs&amp;&amp;... mArgs)
    {
        return std::shared_ptr&lt;TType&gt;(this-&gt;create(mArgs...), m_Deleter);
    }

};
</code></pre>

<p>Timing results without the mutex lock:</p>

<pre><code> With WithObjectFactory!: 
Start time: 1381682855810868000 nanoseconds since Jan 1, 1970
End time: 1381682863375427000 nanoseconds since Jan 1, 1970
Duration: 7.56456 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381682863375476000 nanoseconds since Jan 1, 1970
End time: 1381682879114065000 nanoseconds since Jan 1, 1970
Duration: 15.7386 seconds
</code></pre>

<p>Timing results WITH the mutex locks:</p>

<pre><code> With WithObjectFactory!: 
Start time: 1381683562246086000 nanoseconds since Jan 1, 1970
End time: 1381683574399319000 nanoseconds since Jan 1, 1970
Duration: 12.1532 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381683574399378000 nanoseconds since Jan 1, 1970
End time: 1381683595103438000 nanoseconds since Jan 1, 1970
Duration: 20.7041 seconds
</code></pre>

<p>What you see is that the mutex locking takes more than 20% of the time leaving, IMHO, boost::object_pool only usable for single thread applications.</p>

<p>My questions: 
Am I using the proper locking mechanism?
Does the destroy also needs a mutex lock?
Is there any bug I am not seeing in above code?</p>

<p>Thanks</p>

<p>EDIT:
Tested with std::map but is to slow. Found boost::thread_specific_ptr and it seems to work:</p>

<pre><code>#include &lt;memory&gt;
using std::shared_ptr;

#include &lt;boost/pool/object_pool.hpp&gt;
#include &lt;boost/thread.hpp&gt;

template &lt;typename T&gt;
class ObjectFactory 
{
private:
    struct SharedDeleter 
    {
        ObjectFactory&lt;T&gt;* m_pFact;

        SharedDeleter(ObjectFactory&lt;T&gt;* fact) : m_pFact(fact) {}

        inline void operator()(T* p) const 
        {      
          m_pFact-&gt;destroy(p);
        }
    };

    boost::thread_specific_ptr&lt;boost::object_pool&lt;T&gt;&gt;   m_tpPool;   
    SharedDeleter           m_Deleter;

public:
    ObjectFactory() : m_Deleter(this)
    {
        if ( !m_tpPool.get() )
            m_tpPool.reset(new boost::object_pool&lt;T&gt;());
    }

    template&lt;typename TType = T, typename... TArgs&gt; 
    inline TType* create(TArgs&amp;&amp;... mArgs)
    {
        return m_tpPool-&gt;construct(mArgs...);
    }

    inline void destroy(T* mObj) 
    { 
        m_tpPool-&gt;destroy(mObj);
    }

    template&lt;typename TType = T, typename... TArgs&gt; 
    inline std::shared_ptr&lt;TType&gt; make_shared(TArgs&amp;&amp;... mArgs)
    {
        return std::shared_ptr&lt;TType&gt;(this-&gt;create(mArgs...), m_Deleter);
    }
};
</code></pre>

<p>Results with above code:</p>

<pre><code> With WithObjectFactory!: 
Start time: 1381714923605177000 nanoseconds since Jan 1, 1970
End time: 1381714934202228000 nanoseconds since Jan 1, 1970
Duration: 10.5971 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381714934202285000 nanoseconds since Jan 1, 1970
End time: 1381714950900537000 nanoseconds since Jan 1, 1970
Duration: 16.6983 seconds
</code></pre>
","after asking is there a faster heap allocationdeallocation mechanism available than boostobject_pool i got feedback that this object pool is not thread safe
so i wrote an objectfactory wrapping boostobject_pool and adding mutex locks
timing results without the mutex lock
timing results with the mutex locks
what you see is that the mutex locking takes more than 20 of the time leaving imho boostobject_pool only usable for single thread applications
my questions 
am i using the proper locking mechanism
does the destroy also needs a mutex lock
is there any bug i am not seeing in above code
thanks
edit
tested with stdmap but is to slow found boostthread_specific_ptr and it seems to work
results with above code
",1,"After asking Is there a faster heap allocation/deallocation mechanism available than boost::object_pool? I got feedback that this object pool is NOT thread safe.
So I wrote an ObjectFactory wrapping boost::object_pool and adding mutex locks:
#include <memory>
using std::shared_ptr;

#include <boost/pool/object_pool.hpp>
#include <boost/thread/mutex.hpp>

template <typename T>
class ObjectFactory 
{
private:
    struct SharedDeleter 
    {
        ObjectFactory<T>* m_pFact;

        SharedDeleter(ObjectFactory<T>* fact) : m_pFact(fact) {}

        inline void operator()(T* p) const 
        {      
          m_pFact->destroy(p);
        }
    };


    boost::object_pool<T>   m_Pool;
    boost::mutex            m_PoolMutex;

    SharedDeleter           m_Deleter;


public:
    ObjectFactory() : m_Deleter(this)
    {
    }

    template<typename TType = T, typename... TArgs> 
    inline TType* create(TArgs&&... mArgs)
    {
        boost::unique_lock<boost::mutex> scoped_lock(m_PoolMutex);
        return m_Pool.construct(mArgs...);
    }

    inline void destroy(T* mObj) 
    { 
        boost::unique_lock<boost::mutex> scoped_lock(m_PoolMutex);
        m_Pool.destroy(mObj); 
    }


    template<typename TType = T, typename... TArgs> 
    inline std::shared_ptr<TType> make_shared(TArgs&&... mArgs)
    {
        return std::shared_ptr<TType>(this->create(mArgs...), m_Deleter);
    }

};

Timing results without the mutex lock:
 With WithObjectFactory!: 
Start time: 1381682855810868000 nanoseconds since Jan 1, 1970
End time: 1381682863375427000 nanoseconds since Jan 1, 1970
Duration: 7.56456 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381682863375476000 nanoseconds since Jan 1, 1970
End time: 1381682879114065000 nanoseconds since Jan 1, 1970
Duration: 15.7386 seconds

Timing results WITH the mutex locks:
 With WithObjectFactory!: 
Start time: 1381683562246086000 nanoseconds since Jan 1, 1970
End time: 1381683574399319000 nanoseconds since Jan 1, 1970
Duration: 12.1532 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381683574399378000 nanoseconds since Jan 1, 1970
End time: 1381683595103438000 nanoseconds since Jan 1, 1970
Duration: 20.7041 seconds

What you see is that the mutex locking takes more than 20% of the time leaving, IMHO, boost::object_pool only usable for single thread applications.
My questions: 
Am I using the proper locking mechanism?
Does the destroy also needs a mutex lock?
Is there any bug I am not seeing in above code?
Thanks
EDIT:
Tested with std::map but is to slow. Found boost::thread_specific_ptr and it seems to work:
#include <memory>
using std::shared_ptr;

#include <boost/pool/object_pool.hpp>
#include <boost/thread.hpp>

template <typename T>
class ObjectFactory 
{
private:
    struct SharedDeleter 
    {
        ObjectFactory<T>* m_pFact;

        SharedDeleter(ObjectFactory<T>* fact) : m_pFact(fact) {}

        inline void operator()(T* p) const 
        {      
          m_pFact->destroy(p);
        }
    };

    boost::thread_specific_ptr<boost::object_pool<T>>   m_tpPool;   
    SharedDeleter           m_Deleter;

public:
    ObjectFactory() : m_Deleter(this)
    {
        if ( !m_tpPool.get() )
            m_tpPool.reset(new boost::object_pool<T>());
    }

    template<typename TType = T, typename... TArgs> 
    inline TType* create(TArgs&&... mArgs)
    {
        return m_tpPool->construct(mArgs...);
    }

    inline void destroy(T* mObj) 
    { 
        m_tpPool->destroy(mObj);
    }

    template<typename TType = T, typename... TArgs> 
    inline std::shared_ptr<TType> make_shared(TArgs&&... mArgs)
    {
        return std::shared_ptr<TType>(this->create(mArgs...), m_Deleter);
    }
};

Results with above code:
 With WithObjectFactory!: 
Start time: 1381714923605177000 nanoseconds since Jan 1, 1970
End time: 1381714934202228000 nanoseconds since Jan 1, 1970
Duration: 10.5971 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381714934202285000 nanoseconds since Jan 1, 1970
End time: 1381714950900537000 nanoseconds since Jan 1, 1970
Duration: 16.6983 seconds

","How to get boost::object_pool thread safe?
After asking Is there a faster heap allocation/deallocation mechanism available than boost::object_pool? I got feedback that this object pool is NOT thread safe.
So I wrote an ObjectFactory wrapping boost::object_pool and adding mutex locks:
#include <memory>
using std::shared_ptr;

#include <boost/pool/object_pool.hpp>
#include <boost/thread/mutex.hpp>

template <typename T>
class ObjectFactory 
{
private:
    struct SharedDeleter 
    {
        ObjectFactory<T>* m_pFact;

        SharedDeleter(ObjectFactory<T>* fact) : m_pFact(fact) {}

        inline void operator()(T* p) const 
        {      
          m_pFact->destroy(p);
        }
    };


    boost::object_pool<T>   m_Pool;
    boost::mutex            m_PoolMutex;

    SharedDeleter           m_Deleter;


public:
    ObjectFactory() : m_Deleter(this)
    {
    }

    template<typename TType = T, typename... TArgs> 
    inline TType* create(TArgs&&... mArgs)
    {
        boost::unique_lock<boost::mutex> scoped_lock(m_PoolMutex);
        return m_Pool.construct(mArgs...);
    }

    inline void destroy(T* mObj) 
    { 
        boost::unique_lock<boost::mutex> scoped_lock(m_PoolMutex);
        m_Pool.destroy(mObj); 
    }


    template<typename TType = T, typename... TArgs> 
    inline std::shared_ptr<TType> make_shared(TArgs&&... mArgs)
    {
        return std::shared_ptr<TType>(this->create(mArgs...), m_Deleter);
    }

};

Timing results without the mutex lock:
 With WithObjectFactory!: 
Start time: 1381682855810868000 nanoseconds since Jan 1, 1970
End time: 1381682863375427000 nanoseconds since Jan 1, 1970
Duration: 7.56456 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381682863375476000 nanoseconds since Jan 1, 1970
End time: 1381682879114065000 nanoseconds since Jan 1, 1970
Duration: 15.7386 seconds

Timing results WITH the mutex locks:
 With WithObjectFactory!: 
Start time: 1381683562246086000 nanoseconds since Jan 1, 1970
End time: 1381683574399319000 nanoseconds since Jan 1, 1970
Duration: 12.1532 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381683574399378000 nanoseconds since Jan 1, 1970
End time: 1381683595103438000 nanoseconds since Jan 1, 1970
Duration: 20.7041 seconds

What you see is that the mutex locking takes more than 20% of the time leaving, IMHO, boost::object_pool only usable for single thread applications.
My questions: 
Am I using the proper locking mechanism?
Does the destroy also needs a mutex lock?
Is there any bug I am not seeing in above code?
Thanks
EDIT:
Tested with std::map but is to slow. Found boost::thread_specific_ptr and it seems to work:
#include <memory>
using std::shared_ptr;

#include <boost/pool/object_pool.hpp>
#include <boost/thread.hpp>

template <typename T>
class ObjectFactory 
{
private:
    struct SharedDeleter 
    {
        ObjectFactory<T>* m_pFact;

        SharedDeleter(ObjectFactory<T>* fact) : m_pFact(fact) {}

        inline void operator()(T* p) const 
        {      
          m_pFact->destroy(p);
        }
    };

    boost::thread_specific_ptr<boost::object_pool<T>>   m_tpPool;   
    SharedDeleter           m_Deleter;

public:
    ObjectFactory() : m_Deleter(this)
    {
        if ( !m_tpPool.get() )
            m_tpPool.reset(new boost::object_pool<T>());
    }

    template<typename TType = T, typename... TArgs> 
    inline TType* create(TArgs&&... mArgs)
    {
        return m_tpPool->construct(mArgs...);
    }

    inline void destroy(T* mObj) 
    { 
        m_tpPool->destroy(mObj);
    }

    template<typename TType = T, typename... TArgs> 
    inline std::shared_ptr<TType> make_shared(TArgs&&... mArgs)
    {
        return std::shared_ptr<TType>(this->create(mArgs...), m_Deleter);
    }
};

Results with above code:
 With WithObjectFactory!: 
Start time: 1381714923605177000 nanoseconds since Jan 1, 1970
End time: 1381714934202228000 nanoseconds since Jan 1, 1970
Duration: 10.5971 seconds

 With WithObjectFactory and std::shared_ptr!: 
Start time: 1381714934202285000 nanoseconds since Jan 1, 1970
End time: 1381714950900537000 nanoseconds since Jan 1, 1970
Duration: 16.6983 seconds

",0,1,0,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,0,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
52236932,52236932,How can I cleanly use an existing connection or create/dispose of a new one?,"<p>I have some instances where sometimes I want to execute code within the same connection (for using temp tables, etc), but most of the time I want to open and close the connection as soon as possible.</p>

<pre><code>public int BulkInsert&lt;T&gt;(IDataReader dataReader, Dictionary&lt;string, string&gt; columnMappings = null, int timeoutInSeconds = 120)
{
    if (_sqlConnection != null)
    {
        return BulkInsert&lt;T&gt;(dataReader, _sqlConnection, columnMappings, timeoutInSeconds);
    }

    using (var tempConnection = new SqlConnection(_connectionString))
    {
        return BulkInsert&lt;T&gt;(dataReader, tempConnection, columnMappings, timeoutInSeconds);
    }
}
</code></pre>

<p>How can I make this code more clean and not two separate calls?</p>

<p>My attempt:</p>

<pre><code>public int BulkInsert&lt;T&gt;(IDataReader dataReader, Dictionary&lt;string, string&gt; columnMappings = null, int timeoutInSeconds = 120)
{
    var rv = 0;
    var conn = _sqlConnection ?? new SqlConnection(_connectionString);
    try {
        rv = BulkInsert&lt;T&gt;(dataReader, conn, columnMappings, timeoutInSeconds);
    } finally {
        if (conn != _sqlConnection)
        {
            conn.Dispose();
        }
    }

    return rv;
}
</code></pre>

<p>but I am not really happy with it. </p>

<p>P.S. I wasnt sure if this belonged to stackoverflow or programming but I figured because the use of <code>using</code> that it was pretty c# specific and more of a refactor than just an opinion of style</p>
","i have some instances where sometimes i want to execute code within the same connection for using temp tables etc but most of the time i want to open and close the connection as soon as possible
how can i make this code more clean and not two separate calls
my attempt
but i am not really happy with it 
ps i wasnt sure if this belonged to stackoverflow or programming but i figured because the use of using that it was pretty c specific and more of a refactor than just an opinion of style
",1,"I have some instances where sometimes I want to execute code within the same connection (for using temp tables, etc), but most of the time I want to open and close the connection as soon as possible.
public int BulkInsert<T>(IDataReader dataReader, Dictionary<string, string> columnMappings = null, int timeoutInSeconds = 120)
{
    if (_sqlConnection != null)
    {
        return BulkInsert<T>(dataReader, _sqlConnection, columnMappings, timeoutInSeconds);
    }

    using (var tempConnection = new SqlConnection(_connectionString))
    {
        return BulkInsert<T>(dataReader, tempConnection, columnMappings, timeoutInSeconds);
    }
}

How can I make this code more clean and not two separate calls?
My attempt:
public int BulkInsert<T>(IDataReader dataReader, Dictionary<string, string> columnMappings = null, int timeoutInSeconds = 120)
{
    var rv = 0;
    var conn = _sqlConnection ?? new SqlConnection(_connectionString);
    try {
        rv = BulkInsert<T>(dataReader, conn, columnMappings, timeoutInSeconds);
    } finally {
        if (conn != _sqlConnection)
        {
            conn.Dispose();
        }
    }

    return rv;
}

but I am not really happy with it. 
P.S. I wasnt sure if this belonged to stackoverflow or programming but I figured because the use of using that it was pretty c# specific and more of a refactor than just an opinion of style
","How can I cleanly use an existing connection or create/dispose of a new one?
I have some instances where sometimes I want to execute code within the same connection (for using temp tables, etc), but most of the time I want to open and close the connection as soon as possible.
public int BulkInsert<T>(IDataReader dataReader, Dictionary<string, string> columnMappings = null, int timeoutInSeconds = 120)
{
    if (_sqlConnection != null)
    {
        return BulkInsert<T>(dataReader, _sqlConnection, columnMappings, timeoutInSeconds);
    }

    using (var tempConnection = new SqlConnection(_connectionString))
    {
        return BulkInsert<T>(dataReader, tempConnection, columnMappings, timeoutInSeconds);
    }
}

How can I make this code more clean and not two separate calls?
My attempt:
public int BulkInsert<T>(IDataReader dataReader, Dictionary<string, string> columnMappings = null, int timeoutInSeconds = 120)
{
    var rv = 0;
    var conn = _sqlConnection ?? new SqlConnection(_connectionString);
    try {
        rv = BulkInsert<T>(dataReader, conn, columnMappings, timeoutInSeconds);
    } finally {
        if (conn != _sqlConnection)
        {
            conn.Dispose();
        }
    }

    return rv;
}

but I am not really happy with it. 
P.S. I wasnt sure if this belonged to stackoverflow or programming but I figured because the use of using that it was pretty c# specific and more of a refactor than just an opinion of style
",0,1,0,1,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,0,1,0,0,,,Architecture,Code,"The code functionality was fine so there weren’t really any issues in the realm of reliability or flexibility, user was more so just looking for a cleaner approach",Architecture,Mild,Mild,,Mild
33093860,33093860,"Using nested vectors vs a flatten vector wrapper, strange behaviour","<p><strong>The problem</strong></p>

<p>For a long time I had the impression that using a nested <code>std::vector&lt;std::vector...&gt;</code> for simulating an N-dimensional array is in general bad, since the memory is not guarantee to be contiguous, and one may have cache misses. I thought it's better to use a flat vector and map from multiple dimensions to 1D and vice versa. So, I decided to test it (code listed at the end). It is pretty straightforward, I timed reading/writing to a nested 3D vector vs my own 3D wrapper of an 1D vector. I compiled the code with both <code>g++</code> and <code>clang++</code>, with <code>-O3</code> optimization turned on. For each run I changed the dimensions, so I can get a pretty good idea about the behaviour. To my surprise, these are the results I obtained on my machine MacBook Pro (Retina, 13-inch, Late 2012), 2.5GHz i5, 8GB RAM, OS X 10.10.5:</p>

<p><strong>g++ 5.2</strong></p>

<pre><code>dimensions       nested   flat
X   Y   Z        (ms)     (ms) 

100 100 100  -&gt;  16       24
150 150 150  -&gt;  58       98
200 200 200  -&gt;  136     308
250 250 250  -&gt;  264     746
300 300 300  -&gt;  440    1537
</code></pre>

<p><strong>clang++ (LLVM 7.0.0)</strong></p>

<pre><code>dimensions       nested   flat
X   Y   Z        (ms)     (ms) 

100 100 100  -&gt;  16       18
150 150 150  -&gt;  53       61
200 200 200  -&gt;  135     137
250 250 250  -&gt;  255     271
300 300 300  -&gt;  423     477
</code></pre>

<p><br></p>

<p>As you can see, the ""flatten"" wrapper is never beating the nested version. Moreover, g++'s libstdc++ implementation performs quite badly compared to libc++ implementation, for example for <code>300 x 300 x 300</code> the flatten version is almost 4 times slower than the nested version. libc++ seems to have equal performance. </p>

<p><strong>My questions:</strong></p>

<ol>
<li>Why isn't the flatten version faster? Shouldn't it be? Am I missing something in the testing code?</li>
<li>Moreover, why does g++'s libstdc++ performs so badly when using flatten vectors? Again, shouldn't it perform better?</li>
</ol>

<p><strong>The code I used:</strong></p>

<pre><code>#include &lt;chrono&gt;
#include &lt;cstddef&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;
#include &lt;random&gt;
#include &lt;vector&gt;

// Thin wrapper around flatten vector
template&lt;typename T&gt;
class Array3D
{
    std::size_t _X, _Y, _Z;
    std::vector&lt;T&gt; _vec;
public:
    Array3D(std::size_t X, std::size_t Y, std::size_t Z):
        _X(X), _Y(Y), _Z(Z), _vec(_X * _Y * _Z) {}
    T&amp; operator()(std::size_t x, std::size_t y, std::size_t z)
    {
        return _vec[z * (_X * _Y) + y * _X + x];
    }
    const T&amp; operator()(std::size_t x, std::size_t y, std::size_t z) const
    {
        return _vec[z * (_X * _Y) + y * _X + x];
    }
};

int main(int argc, char** argv)
{
    std::random_device rd{};
    std::mt19937 rng{rd()};
    std::uniform_real_distribution&lt;double&gt; urd(-1, 1);

    const std::size_t X = std::stol(argv[1]);
    const std::size_t Y = std::stol(argv[2]);
    const std::size_t Z = std::stol(argv[3]);


    // Standard library nested vector
    std::vector&lt;std::vector&lt;std::vector&lt;double&gt;&gt;&gt;
        vec3D(X, std::vector&lt;std::vector&lt;double&gt;&gt;(Y, std::vector&lt;double&gt;(Z)));

    // 3D wrapper around a 1D flat vector
    Array3D&lt;double&gt; vec1D(X, Y, Z);

    // TIMING nested vectors
    std::cout &lt;&lt; ""Timing nested vectors...\n"";
    auto start = std::chrono::steady_clock::now();
    volatile double tmp1 = 0;
    for (std::size_t x = 0 ; x &lt; X; ++x)
    {
        for (std::size_t y = 0 ; y &lt; Y; ++y)
        {
            for (std::size_t z = 0 ; z &lt; Z; ++z)
            {
                vec3D[x][y][z] = urd(rng);
                tmp1 += vec3D[x][y][z];
            }
        }
    }
    std::cout &lt;&lt; ""\tSum: "" &lt;&lt; tmp1 &lt;&lt; std::endl; // we make sure the loops are not optimized out
    auto end = std::chrono::steady_clock::now();
    std::cout &lt;&lt; ""Took: "";
    auto ms = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(end - start).count();
    std::cout &lt;&lt; ms &lt;&lt; "" milliseconds\n"";

    // TIMING flatten vector
    std::cout &lt;&lt; ""Timing flatten vector...\n"";
    start = std::chrono::steady_clock::now();
    volatile double tmp2 = 0;
    for (std::size_t x = 0 ; x &lt; X; ++x)
    {
        for (std::size_t y = 0 ; y &lt; Y; ++y)
        {
            for (std::size_t z = 0 ; z &lt; Z; ++z)
            {
                vec1D(x, y, z) = urd(rng);
                tmp2 += vec1D(x, y, z);
            }
        }
    }
    std::cout &lt;&lt; ""\tSum: "" &lt;&lt; tmp2 &lt;&lt; std::endl; // we make sure the loops are not optimized out
    end = std::chrono::steady_clock::now();
    std::cout &lt;&lt; ""Took: "";
    ms = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(end - start).count();
    std::cout &lt;&lt; ms &lt;&lt; "" milliseconds\n"";
}
</code></pre>

<p><strong>EDIT</strong></p>

<p>Changing the <code>Array3D&lt;T&gt;::operator()</code> return to</p>

<pre><code>return _vec[(x * _Y + y) * _Z + z];
</code></pre>

<p>as per <a href=""https://stackoverflow.com/a/33093906/3093378"">@1201ProgramAlarm's suggestion</a> does indeed get rid of the ""weird"" behaviour of g++, in the sense that the flat and nested versions take now roughly the same time. However it's still intriguing. I thought the nested one will be much worse due to cache issues. <strong>May I just be lucky and have all the memory contiguously allocated?</strong> </p>
","the problem
for a long time i had the impression that using a nested stdvectorstdvector for simulating an ndimensional array is in general bad since the memory is not guarantee to be contiguous and one may have cache misses i thought its better to use a flat vector and map from multiple dimensions to 1d and vice versa so i decided to test it code listed at the end it is pretty straightforward i timed readingwriting to a nested 3d vector vs my own 3d wrapper of an 1d vector i compiled the code with both g and clang with o3 optimization turned on for each run i changed the dimensions so i can get a pretty good idea about the behaviour to my surprise these are the results i obtained on my machine macbook pro retina 13inch late 2012 25ghz i5 8gb ram os x 10105
g 52
clang llvm 700

as you can see the flatten wrapper is never beating the nested version moreover gs libstdc implementation performs quite badly compared to libc implementation for example for 300 x 300 x 300 the flatten version is almost 4 times slower than the nested version libc seems to have equal performance 
my questions

why isnt the flatten version faster shouldnt it be am i missing something in the testing code
moreover why does gs libstdc performs so badly when using flatten vectors again shouldnt it perform better

the code i used
edit
changing the array3dtoperator return to
as per 1201programalarms suggestion does indeed get rid of the weird behaviour of g in the sense that the flat and nested versions take now roughly the same time however its still intriguing i thought the nested one will be much worse due to cache issues may i just be lucky and have all the memory contiguously allocated 
",1,"The problem
For a long time I had the impression that using a nested std::vector<std::vector...> for simulating an N-dimensional array is in general bad, since the memory is not guarantee to be contiguous, and one may have cache misses. I thought it's better to use a flat vector and map from multiple dimensions to 1D and vice versa. So, I decided to test it (code listed at the end). It is pretty straightforward, I timed reading/writing to a nested 3D vector vs my own 3D wrapper of an 1D vector. I compiled the code with both g++ and clang++, with -O3 optimization turned on. For each run I changed the dimensions, so I can get a pretty good idea about the behaviour. To my surprise, these are the results I obtained on my machine MacBook Pro (Retina, 13-inch, Late 2012), 2.5GHz i5, 8GB RAM, OS X 10.10.5:
g++ 5.2
dimensions       nested   flat
X   Y   Z        (ms)     (ms) 

100 100 100  ->  16       24
150 150 150  ->  58       98
200 200 200  ->  136     308
250 250 250  ->  264     746
300 300 300  ->  440    1537

clang++ (LLVM 7.0.0)
dimensions       nested   flat
X   Y   Z        (ms)     (ms) 

100 100 100  ->  16       18
150 150 150  ->  53       61
200 200 200  ->  135     137
250 250 250  ->  255     271
300 300 300  ->  423     477


As you can see, the ""flatten"" wrapper is never beating the nested version. Moreover, g++'s libstdc++ implementation performs quite badly compared to libc++ implementation, for example for 300 x 300 x 300 the flatten version is almost 4 times slower than the nested version. libc++ seems to have equal performance. 
My questions:

Why isn't the flatten version faster? Shouldn't it be? Am I missing something in the testing code?
Moreover, why does g++'s libstdc++ performs so badly when using flatten vectors? Again, shouldn't it perform better?

The code I used:
#include <chrono>
#include <cstddef>
#include <iostream>
#include <memory>
#include <random>
#include <vector>

// Thin wrapper around flatten vector
template<typename T>
class Array3D
{
    std::size_t _X, _Y, _Z;
    std::vector<T> _vec;
public:
    Array3D(std::size_t X, std::size_t Y, std::size_t Z):
        _X(X), _Y(Y), _Z(Z), _vec(_X * _Y * _Z) {}
    T& operator()(std::size_t x, std::size_t y, std::size_t z)
    {
        return _vec[z * (_X * _Y) + y * _X + x];
    }
    const T& operator()(std::size_t x, std::size_t y, std::size_t z) const
    {
        return _vec[z * (_X * _Y) + y * _X + x];
    }
};

int main(int argc, char** argv)
{
    std::random_device rd{};
    std::mt19937 rng{rd()};
    std::uniform_real_distribution<double> urd(-1, 1);

    const std::size_t X = std::stol(argv[1]);
    const std::size_t Y = std::stol(argv[2]);
    const std::size_t Z = std::stol(argv[3]);


    // Standard library nested vector
    std::vector<std::vector<std::vector<double>>>
        vec3D(X, std::vector<std::vector<double>>(Y, std::vector<double>(Z)));

    // 3D wrapper around a 1D flat vector
    Array3D<double> vec1D(X, Y, Z);

    // TIMING nested vectors
    std::cout << ""Timing nested vectors...\n"";
    auto start = std::chrono::steady_clock::now();
    volatile double tmp1 = 0;
    for (std::size_t x = 0 ; x < X; ++x)
    {
        for (std::size_t y = 0 ; y < Y; ++y)
        {
            for (std::size_t z = 0 ; z < Z; ++z)
            {
                vec3D[x][y][z] = urd(rng);
                tmp1 += vec3D[x][y][z];
            }
        }
    }
    std::cout << ""\tSum: "" << tmp1 << std::endl; // we make sure the loops are not optimized out
    auto end = std::chrono::steady_clock::now();
    std::cout << ""Took: "";
    auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
    std::cout << ms << "" milliseconds\n"";

    // TIMING flatten vector
    std::cout << ""Timing flatten vector...\n"";
    start = std::chrono::steady_clock::now();
    volatile double tmp2 = 0;
    for (std::size_t x = 0 ; x < X; ++x)
    {
        for (std::size_t y = 0 ; y < Y; ++y)
        {
            for (std::size_t z = 0 ; z < Z; ++z)
            {
                vec1D(x, y, z) = urd(rng);
                tmp2 += vec1D(x, y, z);
            }
        }
    }
    std::cout << ""\tSum: "" << tmp2 << std::endl; // we make sure the loops are not optimized out
    end = std::chrono::steady_clock::now();
    std::cout << ""Took: "";
    ms = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
    std::cout << ms << "" milliseconds\n"";
}

EDIT
Changing the Array3D<T>::operator() return to
return _vec[(x * _Y + y) * _Z + z];

as per @1201ProgramAlarm's suggestion does indeed get rid of the ""weird"" behaviour of g++, in the sense that the flat and nested versions take now roughly the same time. However it's still intriguing. I thought the nested one will be much worse due to cache issues. May I just be lucky and have all the memory contiguously allocated? 
","Using nested vectors vs a flatten vector wrapper, strange behaviour
The problem
For a long time I had the impression that using a nested std::vector<std::vector...> for simulating an N-dimensional array is in general bad, since the memory is not guarantee to be contiguous, and one may have cache misses. I thought it's better to use a flat vector and map from multiple dimensions to 1D and vice versa. So, I decided to test it (code listed at the end). It is pretty straightforward, I timed reading/writing to a nested 3D vector vs my own 3D wrapper of an 1D vector. I compiled the code with both g++ and clang++, with -O3 optimization turned on. For each run I changed the dimensions, so I can get a pretty good idea about the behaviour. To my surprise, these are the results I obtained on my machine MacBook Pro (Retina, 13-inch, Late 2012), 2.5GHz i5, 8GB RAM, OS X 10.10.5:
g++ 5.2
dimensions       nested   flat
X   Y   Z        (ms)     (ms) 

100 100 100  ->  16       24
150 150 150  ->  58       98
200 200 200  ->  136     308
250 250 250  ->  264     746
300 300 300  ->  440    1537

clang++ (LLVM 7.0.0)
dimensions       nested   flat
X   Y   Z        (ms)     (ms) 

100 100 100  ->  16       18
150 150 150  ->  53       61
200 200 200  ->  135     137
250 250 250  ->  255     271
300 300 300  ->  423     477


As you can see, the ""flatten"" wrapper is never beating the nested version. Moreover, g++'s libstdc++ implementation performs quite badly compared to libc++ implementation, for example for 300 x 300 x 300 the flatten version is almost 4 times slower than the nested version. libc++ seems to have equal performance. 
My questions:

Why isn't the flatten version faster? Shouldn't it be? Am I missing something in the testing code?
Moreover, why does g++'s libstdc++ performs so badly when using flatten vectors? Again, shouldn't it perform better?

The code I used:
#include <chrono>
#include <cstddef>
#include <iostream>
#include <memory>
#include <random>
#include <vector>

// Thin wrapper around flatten vector
template<typename T>
class Array3D
{
    std::size_t _X, _Y, _Z;
    std::vector<T> _vec;
public:
    Array3D(std::size_t X, std::size_t Y, std::size_t Z):
        _X(X), _Y(Y), _Z(Z), _vec(_X * _Y * _Z) {}
    T& operator()(std::size_t x, std::size_t y, std::size_t z)
    {
        return _vec[z * (_X * _Y) + y * _X + x];
    }
    const T& operator()(std::size_t x, std::size_t y, std::size_t z) const
    {
        return _vec[z * (_X * _Y) + y * _X + x];
    }
};

int main(int argc, char** argv)
{
    std::random_device rd{};
    std::mt19937 rng{rd()};
    std::uniform_real_distribution<double> urd(-1, 1);

    const std::size_t X = std::stol(argv[1]);
    const std::size_t Y = std::stol(argv[2]);
    const std::size_t Z = std::stol(argv[3]);


    // Standard library nested vector
    std::vector<std::vector<std::vector<double>>>
        vec3D(X, std::vector<std::vector<double>>(Y, std::vector<double>(Z)));

    // 3D wrapper around a 1D flat vector
    Array3D<double> vec1D(X, Y, Z);

    // TIMING nested vectors
    std::cout << ""Timing nested vectors...\n"";
    auto start = std::chrono::steady_clock::now();
    volatile double tmp1 = 0;
    for (std::size_t x = 0 ; x < X; ++x)
    {
        for (std::size_t y = 0 ; y < Y; ++y)
        {
            for (std::size_t z = 0 ; z < Z; ++z)
            {
                vec3D[x][y][z] = urd(rng);
                tmp1 += vec3D[x][y][z];
            }
        }
    }
    std::cout << ""\tSum: "" << tmp1 << std::endl; // we make sure the loops are not optimized out
    auto end = std::chrono::steady_clock::now();
    std::cout << ""Took: "";
    auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
    std::cout << ms << "" milliseconds\n"";

    // TIMING flatten vector
    std::cout << ""Timing flatten vector...\n"";
    start = std::chrono::steady_clock::now();
    volatile double tmp2 = 0;
    for (std::size_t x = 0 ; x < X; ++x)
    {
        for (std::size_t y = 0 ; y < Y; ++y)
        {
            for (std::size_t z = 0 ; z < Z; ++z)
            {
                vec1D(x, y, z) = urd(rng);
                tmp2 += vec1D(x, y, z);
            }
        }
    }
    std::cout << ""\tSum: "" << tmp2 << std::endl; // we make sure the loops are not optimized out
    end = std::chrono::steady_clock::now();
    std::cout << ""Took: "";
    ms = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
    std::cout << ms << "" milliseconds\n"";
}

EDIT
Changing the Array3D<T>::operator() return to
return _vec[(x * _Y + y) * _Z + z];

as per @1201ProgramAlarm's suggestion does indeed get rid of the ""weird"" behaviour of g++, in the sense that the flat and nested versions take now roughly the same time. However it's still intriguing. I thought the nested one will be much worse due to cache issues. May I just be lucky and have all the memory contiguously allocated? 
",0,1,1,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
69766080,69766080,How to correctly implement maximum weight independent set of positive tree using BFS,"<p>can someone help me implement the maximum weight independent set for a TREE (not a graph)?
The tree is represented by an adjacency matrix, and we have an array for the weights of the vertices.
BFS output: // 0: distances from start vertex
// 1: BFS-order
// 2: parent-IDs
I tried this code, it doesn't work on all test cases and it says most of the time that the weight is too small.
Can someone help me find the errors?</p>
<pre><code>import java.io.*;
import java.util.*;

public class Lab5
{

    /**
     *  Problem: Find a maximum weight independent set using dynammic programming.
     */
    
    
    private static int[] problem(Tree t, int[] weights)
    {
        // Implement me!
        
        //base cases
        if (t.noOfVertices==0) {
            return new int[] {};
        }
        if (t.noOfVertices==1) {
            return new int[] {weights[0]};
        }
        
        //we will implement this using bfs, we will use 0 as the root
        int[][] bfs = t.bfs(0);
        
        //finding leaves
        int leaf[] = new int [t.noOfVertices];
        
        //now we can implement our algorithm
        //M is the maximum weight of the tree if it contains i, and M1 is the maximum weight of the tree if it doesn't contain i
        int M[]=new int[t.noOfVertices];
        int M1[]=new int[t.noOfVertices];
        
        //treating elements that aren't leaves
        int nodeDiscovered[] = new int[t.noOfVertices];
        
        
        for (int i = 0; i&lt;t.noOfVertices; i++) {
            if (t.edges[i].length==1) {
                leaf[i]=1;
                M[i]=weights[i];
                nodeDiscovered[i]=1;
                M1[i]=0;
            }
            else {
                leaf[i]=0;  
                nodeDiscovered[i]=0;
                }
        }
    
        for (int i = 1; i&lt;t.noOfVertices; i++) {
            if (leaf[i]==1) {
                int node = bfs[2][i];
                if (nodeDiscovered[node]!=0) {
                    continue;
                }
                while (node&gt;-1) {
                    int parent = bfs[2][node];
                    ArrayList&lt;Integer&gt; sibs = new ArrayList&lt;Integer&gt;();
                    if (parent!=-1) {
                        for (int j = 0; j&lt;t.edges[parent].length; j++) {
                            if (t.edges[parent][j]!=bfs[2][parent]) {
                                sibs.add(t.edges[parent][j]);
                            }
                        }
                    }
                    else {
                        sibs.add(node);
                    }
                    for (int sib : sibs) {
                        if (nodeDiscovered[sib]!=0) {
                            continue;
                        }
                        M[sib]=weights[sib];
                        for (int k : t.edges[sib]) {
                            if(bfs[0][sib]==bfs[0][k]-1) {
                                M[sib]=M[sib]+M1[k];
                                M1[sib]+=(M[k]&gt;M1[k])?M[k]:M1[k];
                            }
                        }
                        nodeDiscovered[sib]=1;
                    }
                    
                    node = bfs[2][node];
                }
                
            }
        }
        //putting the answers in an arraylist
        ArrayList&lt;Integer&gt; set = new ArrayList&lt;Integer&gt;();
        if (M[0]&gt;M1[0]) {
            set.add(0);
        }
        for (int i = 1; i&lt;t.noOfVertices; i++) {
            if (!set.contains(bfs[2][i]) &amp;&amp; M[i]&gt;=M1[i] ) {
                set.add(i);
            }
        }
        System.out.println(set);
        //putting the elements of the arraylist into an array of int
        int[] set1 = new int[set.size()];
        for (int i = 0; i&lt;set.size(); i++) {
            set1[i]=set.get(i);
        }
        return set1;
        
    }

    // ---------------------------------------------------------------------
    // Do not change any of the code below!
    // Do not change any of the code below!

    /**
     *  Determines if a given set of vertices is an independent set for the given tree.
     */
    private static boolean isIndSet(Tree t, int[] set)
    {
        if (set == null) return false;

        boolean[] covered = new boolean[t.noOfVertices];

        for (int i = 0; i &lt; set.length; i++)
        {
            int vId = set[i];
            int[] neighs = t.edges[vId];

            if (covered[vId]) return false;
            covered[vId] = true;

            for (int j = 0; j &lt; neighs.length; j++)
            {
                int nId = neighs[j];
                covered[nId] = true;
            }
        }

        return true;
    }

    private static final int LabNo = 5;
    private static final String course = &quot;CS 427&quot;;
    private static final String quarter = &quot;Fall 2021&quot;;
    private static final Random rng = new Random(190817);

    private static boolean testProblem(int[][] testCase)
    {
        int[] parents = testCase[0];
        int[] weights = testCase[1];

        Tree t = Tree.fromParents(parents);

        int[] solution = maxIsWeight(t, weights);
        int isWeight = solution[0];
        int isSize = solution[1];

        int[] answer = problem(t, weights.clone());

        if (!isIndSet(t, answer))
        {
            System.out.println(&quot;Not an independent set.&quot;);
            return false;
        }

        int ansWeight = 0;
        for (int i = 0; i &lt; answer.length; i++)
        {
            ansWeight += weights[answer[i]];
        }

        if (ansWeight &lt; isWeight)
        {
            System.out.println(&quot;Weight too small.&quot;);
            return false;
        }

        if (answer.length &lt; isSize)
        {
            System.out.println(&quot;Set too small.&quot;);
            return false;
        }

        return true;
    }

    private static int[] maxIsWeight(Tree t, int[] weigh)
    {
        int n = t.noOfVertices;

        int[][] dfs = t.dfs(0);
        int[] post = dfs[2];

        int[] w = new int[n];
        for (int i = 0; i &lt; n; i++)
        {
            w[i] = weigh[i] * n + 1;
        }

        boolean[] isCandidate = new boolean[n];

        for (int i = 0; i &lt; n; i++)
        {
            int vId = post[i];
            if (w[vId] &lt;= 0) continue;

            isCandidate[vId] = true;

            int[] neighs = t.edges[vId];
            for (int j = 0; j &lt; neighs.length; j++)
            {
                int uId = neighs[j];
                w[uId] = Math.max(w[uId] - w[vId], 0);
            }
        }

        int isWeight = 0;
        int isSize = 0;

        for (int i = n - 1; i &gt;= 0; i--)
        {
            int vId = post[i];
            if (!isCandidate[vId]) continue;

            isWeight += weigh[vId];
            isSize++;

            int[] neighs = t.edges[vId];
            for (int j = 0; j &lt; neighs.length; j++)
            {
                int uId = neighs[j];
                isCandidate[uId] = false;
            }
        }

        return new int[] { isWeight, isSize };
    }

    public static void main(String args[])
    {
        System.out.println(course + &quot; -- &quot; + quarter + &quot; -- Lab &quot; + LabNo);

        int noOfTests = 300;
        boolean passedAll = true;

        System.out.println(&quot;-- -- -- -- --&quot;);
        System.out.println(noOfTests + &quot; random test cases.&quot;);

        for (int i = 1; i &lt;= noOfTests; i++)
        {
            boolean passed = false;
            boolean exce = false;

            try
            {
                int[][] testCase = createProblem(i);
                passed = testProblem(testCase);
            }
            catch (Exception ex)
            {
                passed = false;
                exce = true;
                ex.printStackTrace();
            }

            if (!passed)
            {
                System.out.println(&quot;Test &quot; + i + &quot; failed!&quot; + (exce ? &quot; (Exception)&quot; : &quot;&quot;));
                passedAll = false;
               //break;
            }
        }

        if (passedAll)
        {
            System.out.println(&quot;All test passed.&quot;);
        }

    }

    private static int[][] createProblem(int testNo)
    {
        int size = rng.nextInt(Math.min(testNo, 5000)) + 5;

        // -- Generate tree. ---

        int[] parents = new int[size];
        parents[0] = -1;

        for (int i = 1; i &lt; parents.length; i++)
        {
            parents[i] = rng.nextInt(i);
        }

        // -- Generate weights. ---

        int[] weights = new int[size];

        for (int i = 0; i &lt; weights.length; i++)
        {
            weights[i] = rng.nextInt(256);
        }

        return new int[][] { parents, weights };
    }

}
</code></pre>
<p>I attached an image that contains the algorithm that I used.</p>
<p><img src=""https://i.stack.imgur.com/jbek6.jpg"" alt="""" /></p>
","can someone help me implement the maximum weight independent set for a tree not a graph
the tree is represented by an adjacency matrix and we have an array for the weights of the vertices
bfs output  0 distances from start vertex
 1 bfsorder
 2 parentids
i tried this code it doesnt work on all test cases and it says most of the time that the weight is too small
can someone help me find the errors
i attached an image that contains the algorithm that i used

",1,"can someone help me implement the maximum weight independent set for a TREE (not a graph)?
The tree is represented by an adjacency matrix, and we have an array for the weights of the vertices.
BFS output: // 0: distances from start vertex
// 1: BFS-order
// 2: parent-IDs
I tried this code, it doesn't work on all test cases and it says most of the time that the weight is too small.
Can someone help me find the errors?
import java.io.*;
import java.util.*;

public class Lab5
{

    /**
     *  Problem: Find a maximum weight independent set using dynammic programming.
     */
    
    
    private static int[] problem(Tree t, int[] weights)
    {
        // Implement me!
        
        //base cases
        if (t.noOfVertices==0) {
            return new int[] {};
        }
        if (t.noOfVertices==1) {
            return new int[] {weights[0]};
        }
        
        //we will implement this using bfs, we will use 0 as the root
        int[][] bfs = t.bfs(0);
        
        //finding leaves
        int leaf[] = new int [t.noOfVertices];
        
        //now we can implement our algorithm
        //M is the maximum weight of the tree if it contains i, and M1 is the maximum weight of the tree if it doesn't contain i
        int M[]=new int[t.noOfVertices];
        int M1[]=new int[t.noOfVertices];
        
        //treating elements that aren't leaves
        int nodeDiscovered[] = new int[t.noOfVertices];
        
        
        for (int i = 0; i<t.noOfVertices; i++) {
            if (t.edges[i].length==1) {
                leaf[i]=1;
                M[i]=weights[i];
                nodeDiscovered[i]=1;
                M1[i]=0;
            }
            else {
                leaf[i]=0;  
                nodeDiscovered[i]=0;
                }
        }
    
        for (int i = 1; i<t.noOfVertices; i++) {
            if (leaf[i]==1) {
                int node = bfs[2][i];
                if (nodeDiscovered[node]!=0) {
                    continue;
                }
                while (node>-1) {
                    int parent = bfs[2][node];
                    ArrayList<Integer> sibs = new ArrayList<Integer>();
                    if (parent!=-1) {
                        for (int j = 0; j<t.edges[parent].length; j++) {
                            if (t.edges[parent][j]!=bfs[2][parent]) {
                                sibs.add(t.edges[parent][j]);
                            }
                        }
                    }
                    else {
                        sibs.add(node);
                    }
                    for (int sib : sibs) {
                        if (nodeDiscovered[sib]!=0) {
                            continue;
                        }
                        M[sib]=weights[sib];
                        for (int k : t.edges[sib]) {
                            if(bfs[0][sib]==bfs[0][k]-1) {
                                M[sib]=M[sib]+M1[k];
                                M1[sib]+=(M[k]>M1[k])?M[k]:M1[k];
                            }
                        }
                        nodeDiscovered[sib]=1;
                    }
                    
                    node = bfs[2][node];
                }
                
            }
        }
        //putting the answers in an arraylist
        ArrayList<Integer> set = new ArrayList<Integer>();
        if (M[0]>M1[0]) {
            set.add(0);
        }
        for (int i = 1; i<t.noOfVertices; i++) {
            if (!set.contains(bfs[2][i]) && M[i]>=M1[i] ) {
                set.add(i);
            }
        }
        System.out.println(set);
        //putting the elements of the arraylist into an array of int
        int[] set1 = new int[set.size()];
        for (int i = 0; i<set.size(); i++) {
            set1[i]=set.get(i);
        }
        return set1;
        
    }

    // ---------------------------------------------------------------------
    // Do not change any of the code below!
    // Do not change any of the code below!

    /**
     *  Determines if a given set of vertices is an independent set for the given tree.
     */
    private static boolean isIndSet(Tree t, int[] set)
    {
        if (set == null) return false;

        boolean[] covered = new boolean[t.noOfVertices];

        for (int i = 0; i < set.length; i++)
        {
            int vId = set[i];
            int[] neighs = t.edges[vId];

            if (covered[vId]) return false;
            covered[vId] = true;

            for (int j = 0; j < neighs.length; j++)
            {
                int nId = neighs[j];
                covered[nId] = true;
            }
        }

        return true;
    }

    private static final int LabNo = 5;
    private static final String course = ""CS 427"";
    private static final String quarter = ""Fall 2021"";
    private static final Random rng = new Random(190817);

    private static boolean testProblem(int[][] testCase)
    {
        int[] parents = testCase[0];
        int[] weights = testCase[1];

        Tree t = Tree.fromParents(parents);

        int[] solution = maxIsWeight(t, weights);
        int isWeight = solution[0];
        int isSize = solution[1];

        int[] answer = problem(t, weights.clone());

        if (!isIndSet(t, answer))
        {
            System.out.println(""Not an independent set."");
            return false;
        }

        int ansWeight = 0;
        for (int i = 0; i < answer.length; i++)
        {
            ansWeight += weights[answer[i]];
        }

        if (ansWeight < isWeight)
        {
            System.out.println(""Weight too small."");
            return false;
        }

        if (answer.length < isSize)
        {
            System.out.println(""Set too small."");
            return false;
        }

        return true;
    }

    private static int[] maxIsWeight(Tree t, int[] weigh)
    {
        int n = t.noOfVertices;

        int[][] dfs = t.dfs(0);
        int[] post = dfs[2];

        int[] w = new int[n];
        for (int i = 0; i < n; i++)
        {
            w[i] = weigh[i] * n + 1;
        }

        boolean[] isCandidate = new boolean[n];

        for (int i = 0; i < n; i++)
        {
            int vId = post[i];
            if (w[vId] <= 0) continue;

            isCandidate[vId] = true;

            int[] neighs = t.edges[vId];
            for (int j = 0; j < neighs.length; j++)
            {
                int uId = neighs[j];
                w[uId] = Math.max(w[uId] - w[vId], 0);
            }
        }

        int isWeight = 0;
        int isSize = 0;

        for (int i = n - 1; i >= 0; i--)
        {
            int vId = post[i];
            if (!isCandidate[vId]) continue;

            isWeight += weigh[vId];
            isSize++;

            int[] neighs = t.edges[vId];
            for (int j = 0; j < neighs.length; j++)
            {
                int uId = neighs[j];
                isCandidate[uId] = false;
            }
        }

        return new int[] { isWeight, isSize };
    }

    public static void main(String args[])
    {
        System.out.println(course + "" -- "" + quarter + "" -- Lab "" + LabNo);

        int noOfTests = 300;
        boolean passedAll = true;

        System.out.println(""-- -- -- -- --"");
        System.out.println(noOfTests + "" random test cases."");

        for (int i = 1; i <= noOfTests; i++)
        {
            boolean passed = false;
            boolean exce = false;

            try
            {
                int[][] testCase = createProblem(i);
                passed = testProblem(testCase);
            }
            catch (Exception ex)
            {
                passed = false;
                exce = true;
                ex.printStackTrace();
            }

            if (!passed)
            {
                System.out.println(""Test "" + i + "" failed!"" + (exce ? "" (Exception)"" : """"));
                passedAll = false;
               //break;
            }
        }

        if (passedAll)
        {
            System.out.println(""All test passed."");
        }

    }

    private static int[][] createProblem(int testNo)
    {
        int size = rng.nextInt(Math.min(testNo, 5000)) + 5;

        // -- Generate tree. ---

        int[] parents = new int[size];
        parents[0] = -1;

        for (int i = 1; i < parents.length; i++)
        {
            parents[i] = rng.nextInt(i);
        }

        // -- Generate weights. ---

        int[] weights = new int[size];

        for (int i = 0; i < weights.length; i++)
        {
            weights[i] = rng.nextInt(256);
        }

        return new int[][] { parents, weights };
    }

}

I attached an image that contains the algorithm that I used.

","How to correctly implement maximum weight independent set of positive tree using BFS
can someone help me implement the maximum weight independent set for a TREE (not a graph)?
The tree is represented by an adjacency matrix, and we have an array for the weights of the vertices.
BFS output: // 0: distances from start vertex
// 1: BFS-order
// 2: parent-IDs
I tried this code, it doesn't work on all test cases and it says most of the time that the weight is too small.
Can someone help me find the errors?
import java.io.*;
import java.util.*;

public class Lab5
{

    /**
     *  Problem: Find a maximum weight independent set using dynammic programming.
     */
    
    
    private static int[] problem(Tree t, int[] weights)
    {
        // Implement me!
        
        //base cases
        if (t.noOfVertices==0) {
            return new int[] {};
        }
        if (t.noOfVertices==1) {
            return new int[] {weights[0]};
        }
        
        //we will implement this using bfs, we will use 0 as the root
        int[][] bfs = t.bfs(0);
        
        //finding leaves
        int leaf[] = new int [t.noOfVertices];
        
        //now we can implement our algorithm
        //M is the maximum weight of the tree if it contains i, and M1 is the maximum weight of the tree if it doesn't contain i
        int M[]=new int[t.noOfVertices];
        int M1[]=new int[t.noOfVertices];
        
        //treating elements that aren't leaves
        int nodeDiscovered[] = new int[t.noOfVertices];
        
        
        for (int i = 0; i<t.noOfVertices; i++) {
            if (t.edges[i].length==1) {
                leaf[i]=1;
                M[i]=weights[i];
                nodeDiscovered[i]=1;
                M1[i]=0;
            }
            else {
                leaf[i]=0;  
                nodeDiscovered[i]=0;
                }
        }
    
        for (int i = 1; i<t.noOfVertices; i++) {
            if (leaf[i]==1) {
                int node = bfs[2][i];
                if (nodeDiscovered[node]!=0) {
                    continue;
                }
                while (node>-1) {
                    int parent = bfs[2][node];
                    ArrayList<Integer> sibs = new ArrayList<Integer>();
                    if (parent!=-1) {
                        for (int j = 0; j<t.edges[parent].length; j++) {
                            if (t.edges[parent][j]!=bfs[2][parent]) {
                                sibs.add(t.edges[parent][j]);
                            }
                        }
                    }
                    else {
                        sibs.add(node);
                    }
                    for (int sib : sibs) {
                        if (nodeDiscovered[sib]!=0) {
                            continue;
                        }
                        M[sib]=weights[sib];
                        for (int k : t.edges[sib]) {
                            if(bfs[0][sib]==bfs[0][k]-1) {
                                M[sib]=M[sib]+M1[k];
                                M1[sib]+=(M[k]>M1[k])?M[k]:M1[k];
                            }
                        }
                        nodeDiscovered[sib]=1;
                    }
                    
                    node = bfs[2][node];
                }
                
            }
        }
        //putting the answers in an arraylist
        ArrayList<Integer> set = new ArrayList<Integer>();
        if (M[0]>M1[0]) {
            set.add(0);
        }
        for (int i = 1; i<t.noOfVertices; i++) {
            if (!set.contains(bfs[2][i]) && M[i]>=M1[i] ) {
                set.add(i);
            }
        }
        System.out.println(set);
        //putting the elements of the arraylist into an array of int
        int[] set1 = new int[set.size()];
        for (int i = 0; i<set.size(); i++) {
            set1[i]=set.get(i);
        }
        return set1;
        
    }

    // ---------------------------------------------------------------------
    // Do not change any of the code below!
    // Do not change any of the code below!

    /**
     *  Determines if a given set of vertices is an independent set for the given tree.
     */
    private static boolean isIndSet(Tree t, int[] set)
    {
        if (set == null) return false;

        boolean[] covered = new boolean[t.noOfVertices];

        for (int i = 0; i < set.length; i++)
        {
            int vId = set[i];
            int[] neighs = t.edges[vId];

            if (covered[vId]) return false;
            covered[vId] = true;

            for (int j = 0; j < neighs.length; j++)
            {
                int nId = neighs[j];
                covered[nId] = true;
            }
        }

        return true;
    }

    private static final int LabNo = 5;
    private static final String course = ""CS 427"";
    private static final String quarter = ""Fall 2021"";
    private static final Random rng = new Random(190817);

    private static boolean testProblem(int[][] testCase)
    {
        int[] parents = testCase[0];
        int[] weights = testCase[1];

        Tree t = Tree.fromParents(parents);

        int[] solution = maxIsWeight(t, weights);
        int isWeight = solution[0];
        int isSize = solution[1];

        int[] answer = problem(t, weights.clone());

        if (!isIndSet(t, answer))
        {
            System.out.println(""Not an independent set."");
            return false;
        }

        int ansWeight = 0;
        for (int i = 0; i < answer.length; i++)
        {
            ansWeight += weights[answer[i]];
        }

        if (ansWeight < isWeight)
        {
            System.out.println(""Weight too small."");
            return false;
        }

        if (answer.length < isSize)
        {
            System.out.println(""Set too small."");
            return false;
        }

        return true;
    }

    private static int[] maxIsWeight(Tree t, int[] weigh)
    {
        int n = t.noOfVertices;

        int[][] dfs = t.dfs(0);
        int[] post = dfs[2];

        int[] w = new int[n];
        for (int i = 0; i < n; i++)
        {
            w[i] = weigh[i] * n + 1;
        }

        boolean[] isCandidate = new boolean[n];

        for (int i = 0; i < n; i++)
        {
            int vId = post[i];
            if (w[vId] <= 0) continue;

            isCandidate[vId] = true;

            int[] neighs = t.edges[vId];
            for (int j = 0; j < neighs.length; j++)
            {
                int uId = neighs[j];
                w[uId] = Math.max(w[uId] - w[vId], 0);
            }
        }

        int isWeight = 0;
        int isSize = 0;

        for (int i = n - 1; i >= 0; i--)
        {
            int vId = post[i];
            if (!isCandidate[vId]) continue;

            isWeight += weigh[vId];
            isSize++;

            int[] neighs = t.edges[vId];
            for (int j = 0; j < neighs.length; j++)
            {
                int uId = neighs[j];
                isCandidate[uId] = false;
            }
        }

        return new int[] { isWeight, isSize };
    }

    public static void main(String args[])
    {
        System.out.println(course + "" -- "" + quarter + "" -- Lab "" + LabNo);

        int noOfTests = 300;
        boolean passedAll = true;

        System.out.println(""-- -- -- -- --"");
        System.out.println(noOfTests + "" random test cases."");

        for (int i = 1; i <= noOfTests; i++)
        {
            boolean passed = false;
            boolean exce = false;

            try
            {
                int[][] testCase = createProblem(i);
                passed = testProblem(testCase);
            }
            catch (Exception ex)
            {
                passed = false;
                exce = true;
                ex.printStackTrace();
            }

            if (!passed)
            {
                System.out.println(""Test "" + i + "" failed!"" + (exce ? "" (Exception)"" : """"));
                passedAll = false;
               //break;
            }
        }

        if (passedAll)
        {
            System.out.println(""All test passed."");
        }

    }

    private static int[][] createProblem(int testNo)
    {
        int size = rng.nextInt(Math.min(testNo, 5000)) + 5;

        // -- Generate tree. ---

        int[] parents = new int[size];
        parents[0] = -1;

        for (int i = 1; i < parents.length; i++)
        {
            parents[i] = rng.nextInt(i);
        }

        // -- Generate weights. ---

        int[] weights = new int[size];

        for (int i = 0; i < weights.length; i++)
        {
            weights[i] = rng.nextInt(256);
        }

        return new int[][] { parents, weights };
    }

}

I attached an image that contains the algorithm that I used.

",0,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
47101515,47101515,creating a large pdf matrix efficiently,"<p>I have a dataset of 60,000 examples of the form: </p>

<pre><code>      mu1     mu2    std1    std2
0   -0.745   0.729  0.0127  0.0149
1   -0.711   0.332  0.1240  0.0433
...
</code></pre>

<p>They are essentially parameters of 2-dimensional normal distributions. What I want to do is create a (NxN) matrix P such that P_ij = Normal( mu_i | mean=mu_j, cov=diagonal(std_j)), where mu_i is (mu1, mu2) for data 'i'. </p>

<p>I can do this with the following code for example:</p>

<pre><code>from scipy import stats
import numpy as np

mu_all = data[['mu1', 'mu2']]
std_all = data[['std1', 'std2']]
P = []
for i in range(len(data)):
    mu_i = mu_all[i,:]
    std_i = std_all[i,:]
    prob_i = stats.multivariate_normal.pdf(mu_all, mean=mu_i, cov=np.diag(std_i))
    P.append(prob_i)

P = np.array(P).T
</code></pre>

<p>But this is too expensive (my machine freezes). How can I do this more efficiently? My guess is that scipy cannot handle computing pdf of 60000 at once. Is there an alternative? </p>
","i have a dataset of 60000 examples of the form 
they are essentially parameters of 2dimensional normal distributions what i want to do is create a nxn matrix p such that p_ij  normal mu_i  meanmu_j covdiagonalstd_j where mu_i is mu1 mu2 for data i 
i can do this with the following code for example
but this is too expensive my machine freezes how can i do this more efficiently my guess is that scipy cannot handle computing pdf of 60000 at once is there an alternative 
",0,"I have a dataset of 60,000 examples of the form: 
      mu1     mu2    std1    std2
0   -0.745   0.729  0.0127  0.0149
1   -0.711   0.332  0.1240  0.0433
...

They are essentially parameters of 2-dimensional normal distributions. What I want to do is create a (NxN) matrix P such that P_ij = Normal( mu_i | mean=mu_j, cov=diagonal(std_j)), where mu_i is (mu1, mu2) for data 'i'. 
I can do this with the following code for example:
from scipy import stats
import numpy as np

mu_all = data[['mu1', 'mu2']]
std_all = data[['std1', 'std2']]
P = []
for i in range(len(data)):
    mu_i = mu_all[i,:]
    std_i = std_all[i,:]
    prob_i = stats.multivariate_normal.pdf(mu_all, mean=mu_i, cov=np.diag(std_i))
    P.append(prob_i)

P = np.array(P).T

But this is too expensive (my machine freezes). How can I do this more efficiently? My guess is that scipy cannot handle computing pdf of 60000 at once. Is there an alternative? 
","creating a large pdf matrix efficiently
I have a dataset of 60,000 examples of the form: 
      mu1     mu2    std1    std2
0   -0.745   0.729  0.0127  0.0149
1   -0.711   0.332  0.1240  0.0433
...

They are essentially parameters of 2-dimensional normal distributions. What I want to do is create a (NxN) matrix P such that P_ij = Normal( mu_i | mean=mu_j, cov=diagonal(std_j)), where mu_i is (mu1, mu2) for data 'i'. 
I can do this with the following code for example:
from scipy import stats
import numpy as np

mu_all = data[['mu1', 'mu2']]
std_all = data[['std1', 'std2']]
P = []
for i in range(len(data)):
    mu_i = mu_all[i,:]
    std_i = std_all[i,:]
    prob_i = stats.multivariate_normal.pdf(mu_all, mean=mu_i, cov=np.diag(std_i))
    P.append(prob_i)

P = np.array(P).T

But this is too expensive (my machine freezes). How can I do this more efficiently? My guess is that scipy cannot handle computing pdf of 60000 at once. Is there an alternative? 
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Very Severe,Very Severe,,Very Severe
60213490,60213490,Is there a C snippet that computes overflow-safe addition efficiently without using compiler builtins?,"<p>Here is a C function that adds an <code>int</code> to another, failing if overflow would happen:</p>

<pre><code>int safe_add(int *value, int delta) {
        if (*value &gt;= 0) {
                if (delta &gt; INT_MAX - *value) {
                        return -1;
                }
        } else {
                if (delta &lt; INT_MIN - *value) {
                        return -1;
                }
        }

        *value += delta;
        return 0;
}
</code></pre>

<p>Unfortunately it is <a href=""https://gcc.godbolt.org/z/v7AgAE"" rel=""noreferrer"">not optimized well</a> by GCC or Clang:</p>

<pre><code>safe_add(int*, int):
        movl    (%rdi), %eax
        testl   %eax, %eax
        js      .L2
        movl    $2147483647, %edx
        subl    %eax, %edx
        cmpl    %esi, %edx
        jl      .L6
.L4:
        addl    %esi, %eax
        movl    %eax, (%rdi)
        xorl    %eax, %eax
        ret
.L2:
        movl    $-2147483648, %edx
        subl    %eax, %edx
        cmpl    %esi, %edx
        jle     .L4
.L6:
        movl    $-1, %eax
        ret
</code></pre>

<p>This version with <code>__builtin_add_overflow()</code></p>

<pre><code>int safe_add(int *value, int delta) {
        int result;
        if (__builtin_add_overflow(*value, delta, &amp;result)) {
                return -1;
        } else {
                *value = result;
                return 0;
        }
}
</code></pre>

<p>is <a href=""https://gcc.godbolt.org/z/5z8m8o"" rel=""noreferrer"">optimized better</a>:</p>

<pre><code>safe_add(int*, int):
        xorl    %eax, %eax
        addl    (%rdi), %esi
        seto    %al
        jo      .L5
        movl    %esi, (%rdi)
        ret
.L5:
        movl    $-1, %eax
        ret
</code></pre>

<p>but I'm curious if there's a way without using builtins that will get pattern-matched by GCC or Clang.</p>
","here is a c function that adds an int to another failing if overflow would happen
unfortunately it is not optimized well by gcc or clang
this version with __builtin_add_overflow
is optimized better
but im curious if theres a way without using builtins that will get patternmatched by gcc or clang
",0,"Here is a C function that adds an int to another, failing if overflow would happen:
int safe_add(int *value, int delta) {
        if (*value >= 0) {
                if (delta > INT_MAX - *value) {
                        return -1;
                }
        } else {
                if (delta < INT_MIN - *value) {
                        return -1;
                }
        }

        *value += delta;
        return 0;
}

Unfortunately it is not optimized well by GCC or Clang:
safe_add(int*, int):
        movl    (%rdi), %eax
        testl   %eax, %eax
        js      .L2
        movl    $2147483647, %edx
        subl    %eax, %edx
        cmpl    %esi, %edx
        jl      .L6
.L4:
        addl    %esi, %eax
        movl    %eax, (%rdi)
        xorl    %eax, %eax
        ret
.L2:
        movl    $-2147483648, %edx
        subl    %eax, %edx
        cmpl    %esi, %edx
        jle     .L4
.L6:
        movl    $-1, %eax
        ret

This version with __builtin_add_overflow()
int safe_add(int *value, int delta) {
        int result;
        if (__builtin_add_overflow(*value, delta, &result)) {
                return -1;
        } else {
                *value = result;
                return 0;
        }
}

is optimized better:
safe_add(int*, int):
        xorl    %eax, %eax
        addl    (%rdi), %esi
        seto    %al
        jo      .L5
        movl    %esi, (%rdi)
        ret
.L5:
        movl    $-1, %eax
        ret

but I'm curious if there's a way without using builtins that will get pattern-matched by GCC or Clang.
","Is there a C snippet that computes overflow-safe addition efficiently without using compiler builtins?
Here is a C function that adds an int to another, failing if overflow would happen:
int safe_add(int *value, int delta) {
        if (*value >= 0) {
                if (delta > INT_MAX - *value) {
                        return -1;
                }
        } else {
                if (delta < INT_MIN - *value) {
                        return -1;
                }
        }

        *value += delta;
        return 0;
}

Unfortunately it is not optimized well by GCC or Clang:
safe_add(int*, int):
        movl    (%rdi), %eax
        testl   %eax, %eax
        js      .L2
        movl    $2147483647, %edx
        subl    %eax, %edx
        cmpl    %esi, %edx
        jl      .L6
.L4:
        addl    %esi, %eax
        movl    %eax, (%rdi)
        xorl    %eax, %eax
        ret
.L2:
        movl    $-2147483648, %edx
        subl    %eax, %edx
        cmpl    %esi, %edx
        jle     .L4
.L6:
        movl    $-1, %eax
        ret

This version with __builtin_add_overflow()
int safe_add(int *value, int delta) {
        int result;
        if (__builtin_add_overflow(*value, delta, &result)) {
                return -1;
        } else {
                *value = result;
                return 0;
        }
}

is optimized better:
safe_add(int*, int):
        xorl    %eax, %eax
        addl    (%rdi), %esi
        seto    %al
        jo      .L5
        movl    %esi, (%rdi)
        ret
.L5:
        movl    $-1, %eax
        ret

but I'm curious if there's a way without using builtins that will get pattern-matched by GCC or Clang.
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Compiler and Compilation Strategies,Compiler and Build Optimization,,Compiler and Compilation Strategies,0,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
17331828,17331828,js memory organization - closures versus objects?,"<p>When I develop in JS, I'm frequently faced with the choice between capturing variables with an inner function (closure) and using an object.</p>

<p>Inner function:</p>

<pre><code>var x = ...;
var f = function() {
    // use x here
}
</code></pre>

<p>Object:</p>

<pre><code>obj.x = ...;
obj.f = function() {
    // use x here
}
</code></pre>

<p>The inner function approach ""feels"" more natural, but I was wondering - are there performance implications to worry about doing it one way or another, and what is the most idiomatic way to do this kind of thing?</p>
","when i develop in js im frequently faced with the choice between capturing variables with an inner function closure and using an object
inner function
object
the inner function approach feels more natural but i was wondering  are there performance implications to worry about doing it one way or another and what is the most idiomatic way to do this kind of thing
",1,"When I develop in JS, I'm frequently faced with the choice between capturing variables with an inner function (closure) and using an object.
Inner function:
var x = ...;
var f = function() {
    // use x here
}

Object:
obj.x = ...;
obj.f = function() {
    // use x here
}

The inner function approach ""feels"" more natural, but I was wondering - are there performance implications to worry about doing it one way or another, and what is the most idiomatic way to do this kind of thing?
","js memory organization - closures versus objects?
When I develop in JS, I'm frequently faced with the choice between capturing variables with an inner function (closure) and using an object.
Inner function:
var x = ...;
var f = function() {
    // use x here
}

Object:
obj.x = ...;
obj.f = function() {
    // use x here
}

The inner function approach ""feels"" more natural, but I was wondering - are there performance implications to worry about doing it one way or another, and what is the most idiomatic way to do this kind of thing?
",0,0,1,1,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,1,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
74590865,74590865,Will there be dangling references to map values if it gets resized?,"<p>Would reference to values in <code>std::map</code> or <code>std::unordered_map</code> be valid/maintained if the map's being inserted new elements or removed from?
For example, is the below code safe? Or would there be dangling references from any edge cases or map implementation variations.</p>
<pre><code>std::map&lt;string,SpecialType&gt; myMap({{&quot;Test&quot;,{}},{&quot;something&quot;,{}},{&quot;Test3&quot;,{}}});
auto pointer = &amp;myMap[&quot;Test&quot;];
myMap.erase(&quot;something&quot;);
pointer-&gt;DoSomething();
myMap[&quot;newItem&quot;] = SpecialType();
pointer-&gt;DoSomething();```
</code></pre>
","would reference to values in stdmap or stdunordered_map be validmaintained if the maps being inserted new elements or removed from
for example is the below code safe or would there be dangling references from any edge cases or map implementation variations
",1,"Would reference to values in std::map or std::unordered_map be valid/maintained if the map's being inserted new elements or removed from?
For example, is the below code safe? Or would there be dangling references from any edge cases or map implementation variations.
std::map<string,SpecialType> myMap({{""Test"",{}},{""something"",{}},{""Test3"",{}}});
auto pointer = &myMap[""Test""];
myMap.erase(""something"");
pointer->DoSomething();
myMap[""newItem""] = SpecialType();
pointer->DoSomething();```

","Will there be dangling references to map values if it gets resized?
Would reference to values in std::map or std::unordered_map be valid/maintained if the map's being inserted new elements or removed from?
For example, is the below code safe? Or would there be dangling references from any edge cases or map implementation variations.
std::map<string,SpecialType> myMap({{""Test"",{}},{""something"",{}},{""Test3"",{}}});
auto pointer = &myMap[""Test""];
myMap.erase(""something"");
pointer->DoSomething();
myMap[""newItem""] = SpecialType();
pointer->DoSomething();```

",0,1,1,0,0,0,0,- Is not a question with a code problem,"Label: 1

Reasoning: This question expresses technical debt because it concerns the maintainability and potential pitfalls of using references to values in std::map or std::unordered_map. The asker is unsure about the validity and safety of their code when inserting new elements or removing existing ones from the map, which could lead to dangling references or unexpected behavior.

The code snippet provided is a specific example of this uncertainty, and the asker is seeking clarification on whether their approach is safe and robust. The questions about edge cases and map implementation variations imply that the asker is aware of potential issues and is attempting to mitigate them.

The technical debt in this question arises from the uncertainty and potential fragility of the code, which could lead to bugs, crashes, or unexpected behavior if not addressed. The asker is essentially asking for guidance on how to write more robust and future-proof code, which is a key aspect of technical debt.",Yes,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,1,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
39969798,39969798,More memory efficient way of making a dictionary?,"<p>VERY sorry for the vagueness, but I don't actually know what part of what I'm doing is inefficient.</p>
<p>I've made a program that takes a list of positive integers (example*):</p>
<pre><code>[1, 1, 3, 5, 16, 2, 4, 6, 6, 8, 9, 24, 200,]
</code></pre>
<p>*the real lists can be up to 2000 in length and the elements between 0 and 100,000 exclusive</p>
<p>And creates a dictionary where each number tupled with its index (like so: <code>(number, index)</code>) is a key and the value for each key is a list of every number (and that number's index) in the input that it goes evenly into.</p>
<p>So the entry for the 3 would be: <code>(3, 2): [(16, 4), (6, 7), (6, 8), (9, 10), (24, 11)]</code></p>
<p>My code is this:</p>
<pre><code>num_dict = {}
sorted_list = sorted(beginning_list)

for a2, a in enumerate(sorted_list):
    num_dict[(a, a2)] = []

for x2, x in enumerate(sorted_list):
    for y2, y in enumerate(sorted_list[x2 + 1:]):
        if y % x == 0:
            pair = (y, y2 + x2 + 1)
            num_dict[(x, x2)].append(pair)
</code></pre>
<p>But, when I run this script, I hit a <code>MemoryError</code>.</p>
<p>I understand that this means that I am running out of memory but in the situation I'm in, adding more ram or updating to a 64-bit version of python is not an option.</p>
<p>I am certain that the problem is not coming from the list sorting or the first for loop. It has to be the second for loop. I just included the other lines for context.</p>
<p>The full output for the list above would be (sorry for the unsortedness, that's just how dictionaries do):</p>
<pre><code>(200, 12): []
(6, 7): [(24, 11)]
(16, 10): []
(6, 6): [(6, 7), (24, 11)]
(5, 5): [(200, 12)]
(4, 4): [(8, 8), (16, 10), (24, 11), (200, 12)]
(9, 9): []
(8, 8): [(16, 10), (24, 11), (200, 12)]
(2, 2): [(4, 4), (6, 6), (6, 7), (8, 8), (16, 10), (24, 11), (200, 12)]
(24, 11): []
(1, 0): [(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(1, 1): [(2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(3, 3): [(6, 6), (6, 7), (9, 9), (24, 11)]
</code></pre>
<p>Is there a better way of going about this?</p>
<h3>EDIT:</h3>
<p>This dictionary will then be fed into this:</p>
<pre><code>ans_set = set()
for x in num_dict:
    for y in num_dict[x]:
        for z in num_dict[y]:
            ans_set.add((x[0], y[0], z[0]))
return len(ans_set)
</code></pre>
<p>to find all unique possible triplets in which the 3rd value can be evenly divided by the 2nd value which can be evenly divided by the 1st.</p>
<p>If you think you know of a better way of doing the entire thing, I'm open to redoing the whole of it.</p>
<h1>Final Edit</h1>
<p>I've found the best way to find the number of triples by reevaluating what I needed it to do. This method doesn't actually find the triples, it just counts them.</p>
<pre><code>def foo(l):
    llen = len(l)
    total = 0
    cache = {}
    for i in range(llen):
        cache[i] = 0
    for x in range(llen):
        for y in range(x + 1, llen):
            if l[y] % l[x] == 0:
                cache[y] += 1
                total += cache[x]
    return total
</code></pre>
<p>And here's a version of the function that explains the thought process as it goes (not good for huge lists though because of spam prints):</p>
<pre><code>def bar(l):
    list_length = len(l)
    total_triples = 0
    cache = {}
    for i in range(list_length):
        cache[i] = 0
    for x in range(list_length):
        print(&quot;\n\nfor index[{}]: {}&quot;.format(x, l[x]))
        for y in range(x + 1, list_length):
            print(&quot;\n\ttry index[{}]: {}&quot;.format(y, l[y]))
            if l[y] % l[x] == 0:
                print(&quot;\n\t\t{} can be evenly diveded by {}&quot;.format(l[y], l[x]))
                cache[y] += 1
                total_triples += cache[x]
                print(&quot;\t\tcache[{0}] is now {1}&quot;.format(y, cache[y]))
                print(&quot;\t\tcount is now {}&quot;.format(total_triples))
                print(&quot;\t\t(+{} from cache[{}])&quot;.format(cache[x], x))
            else:
                print(&quot;\n\t\tfalse&quot;)
    print(&quot;\ntotal number of triples:&quot;, total_triples)
</code></pre>
","very sorry for the vagueness but i dont actually know what part of what im doing is inefficient
ive made a program that takes a list of positive integers example
the real lists can be up to 2000 in length and the elements between 0 and 100000 exclusive
and creates a dictionary where each number tupled with its index like so number index is a key and the value for each key is a list of every number and that numbers index in the input that it goes evenly into
so the entry for the 3 would be 3 2 16 4 6 7 6 8 9 10 24 11
my code is this
but when i run this script i hit a memoryerror
i understand that this means that i am running out of memory but in the situation im in adding more ram or updating to a 64bit version of python is not an option
i am certain that the problem is not coming from the list sorting or the first for loop it has to be the second for loop i just included the other lines for context
the full output for the list above would be sorry for the unsortedness thats just how dictionaries do
is there a better way of going about this
edit
this dictionary will then be fed into this
to find all unique possible triplets in which the 3rd value can be evenly divided by the 2nd value which can be evenly divided by the 1st
if you think you know of a better way of doing the entire thing im open to redoing the whole of it
final edit
ive found the best way to find the number of triples by reevaluating what i needed it to do this method doesnt actually find the triples it just counts them
and heres a version of the function that explains the thought process as it goes not good for huge lists though because of spam prints
",1,"VERY sorry for the vagueness, but I don't actually know what part of what I'm doing is inefficient.
I've made a program that takes a list of positive integers (example*):
[1, 1, 3, 5, 16, 2, 4, 6, 6, 8, 9, 24, 200,]

*the real lists can be up to 2000 in length and the elements between 0 and 100,000 exclusive
And creates a dictionary where each number tupled with its index (like so: (number, index)) is a key and the value for each key is a list of every number (and that number's index) in the input that it goes evenly into.
So the entry for the 3 would be: (3, 2): [(16, 4), (6, 7), (6, 8), (9, 10), (24, 11)]
My code is this:
num_dict = {}
sorted_list = sorted(beginning_list)

for a2, a in enumerate(sorted_list):
    num_dict[(a, a2)] = []

for x2, x in enumerate(sorted_list):
    for y2, y in enumerate(sorted_list[x2 + 1:]):
        if y % x == 0:
            pair = (y, y2 + x2 + 1)
            num_dict[(x, x2)].append(pair)

But, when I run this script, I hit a MemoryError.
I understand that this means that I am running out of memory but in the situation I'm in, adding more ram or updating to a 64-bit version of python is not an option.
I am certain that the problem is not coming from the list sorting or the first for loop. It has to be the second for loop. I just included the other lines for context.
The full output for the list above would be (sorry for the unsortedness, that's just how dictionaries do):
(200, 12): []
(6, 7): [(24, 11)]
(16, 10): []
(6, 6): [(6, 7), (24, 11)]
(5, 5): [(200, 12)]
(4, 4): [(8, 8), (16, 10), (24, 11), (200, 12)]
(9, 9): []
(8, 8): [(16, 10), (24, 11), (200, 12)]
(2, 2): [(4, 4), (6, 6), (6, 7), (8, 8), (16, 10), (24, 11), (200, 12)]
(24, 11): []
(1, 0): [(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(1, 1): [(2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(3, 3): [(6, 6), (6, 7), (9, 9), (24, 11)]

Is there a better way of going about this?
EDIT:
This dictionary will then be fed into this:
ans_set = set()
for x in num_dict:
    for y in num_dict[x]:
        for z in num_dict[y]:
            ans_set.add((x[0], y[0], z[0]))
return len(ans_set)

to find all unique possible triplets in which the 3rd value can be evenly divided by the 2nd value which can be evenly divided by the 1st.
If you think you know of a better way of doing the entire thing, I'm open to redoing the whole of it.
Final Edit
I've found the best way to find the number of triples by reevaluating what I needed it to do. This method doesn't actually find the triples, it just counts them.
def foo(l):
    llen = len(l)
    total = 0
    cache = {}
    for i in range(llen):
        cache[i] = 0
    for x in range(llen):
        for y in range(x + 1, llen):
            if l[y] % l[x] == 0:
                cache[y] += 1
                total += cache[x]
    return total

And here's a version of the function that explains the thought process as it goes (not good for huge lists though because of spam prints):
def bar(l):
    list_length = len(l)
    total_triples = 0
    cache = {}
    for i in range(list_length):
        cache[i] = 0
    for x in range(list_length):
        print(""\n\nfor index[{}]: {}"".format(x, l[x]))
        for y in range(x + 1, list_length):
            print(""\n\ttry index[{}]: {}"".format(y, l[y]))
            if l[y] % l[x] == 0:
                print(""\n\t\t{} can be evenly diveded by {}"".format(l[y], l[x]))
                cache[y] += 1
                total_triples += cache[x]
                print(""\t\tcache[{0}] is now {1}"".format(y, cache[y]))
                print(""\t\tcount is now {}"".format(total_triples))
                print(""\t\t(+{} from cache[{}])"".format(cache[x], x))
            else:
                print(""\n\t\tfalse"")
    print(""\ntotal number of triples:"", total_triples)

","More memory efficient way of making a dictionary?
VERY sorry for the vagueness, but I don't actually know what part of what I'm doing is inefficient.
I've made a program that takes a list of positive integers (example*):
[1, 1, 3, 5, 16, 2, 4, 6, 6, 8, 9, 24, 200,]

*the real lists can be up to 2000 in length and the elements between 0 and 100,000 exclusive
And creates a dictionary where each number tupled with its index (like so: (number, index)) is a key and the value for each key is a list of every number (and that number's index) in the input that it goes evenly into.
So the entry for the 3 would be: (3, 2): [(16, 4), (6, 7), (6, 8), (9, 10), (24, 11)]
My code is this:
num_dict = {}
sorted_list = sorted(beginning_list)

for a2, a in enumerate(sorted_list):
    num_dict[(a, a2)] = []

for x2, x in enumerate(sorted_list):
    for y2, y in enumerate(sorted_list[x2 + 1:]):
        if y % x == 0:
            pair = (y, y2 + x2 + 1)
            num_dict[(x, x2)].append(pair)

But, when I run this script, I hit a MemoryError.
I understand that this means that I am running out of memory but in the situation I'm in, adding more ram or updating to a 64-bit version of python is not an option.
I am certain that the problem is not coming from the list sorting or the first for loop. It has to be the second for loop. I just included the other lines for context.
The full output for the list above would be (sorry for the unsortedness, that's just how dictionaries do):
(200, 12): []
(6, 7): [(24, 11)]
(16, 10): []
(6, 6): [(6, 7), (24, 11)]
(5, 5): [(200, 12)]
(4, 4): [(8, 8), (16, 10), (24, 11), (200, 12)]
(9, 9): []
(8, 8): [(16, 10), (24, 11), (200, 12)]
(2, 2): [(4, 4), (6, 6), (6, 7), (8, 8), (16, 10), (24, 11), (200, 12)]
(24, 11): []
(1, 0): [(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(1, 1): [(2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(3, 3): [(6, 6), (6, 7), (9, 9), (24, 11)]

Is there a better way of going about this?
EDIT:
This dictionary will then be fed into this:
ans_set = set()
for x in num_dict:
    for y in num_dict[x]:
        for z in num_dict[y]:
            ans_set.add((x[0], y[0], z[0]))
return len(ans_set)

to find all unique possible triplets in which the 3rd value can be evenly divided by the 2nd value which can be evenly divided by the 1st.
If you think you know of a better way of doing the entire thing, I'm open to redoing the whole of it.
Final Edit
I've found the best way to find the number of triples by reevaluating what I needed it to do. This method doesn't actually find the triples, it just counts them.
def foo(l):
    llen = len(l)
    total = 0
    cache = {}
    for i in range(llen):
        cache[i] = 0
    for x in range(llen):
        for y in range(x + 1, llen):
            if l[y] % l[x] == 0:
                cache[y] += 1
                total += cache[x]
    return total

And here's a version of the function that explains the thought process as it goes (not good for huge lists though because of spam prints):
def bar(l):
    list_length = len(l)
    total_triples = 0
    cache = {}
    for i in range(list_length):
        cache[i] = 0
    for x in range(list_length):
        print(""\n\nfor index[{}]: {}"".format(x, l[x]))
        for y in range(x + 1, list_length):
            print(""\n\ttry index[{}]: {}"".format(y, l[y]))
            if l[y] % l[x] == 0:
                print(""\n\t\t{} can be evenly diveded by {}"".format(l[y], l[x]))
                cache[y] += 1
                total_triples += cache[x]
                print(""\t\tcache[{0}] is now {1}"".format(y, cache[y]))
                print(""\t\tcount is now {}"".format(total_triples))
                print(""\t\t(+{} from cache[{}])"".format(cache[x], x))
            else:
                print(""\n\t\tfalse"")
    print(""\ntotal number of triples:"", total_triples)

",1,1,0,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,1,0,0,0,0,,,Architecture,Design,The underlying algorithm design is poor and not scalable,Design,Moderate,Moderate,,Moderate
14946281,14946281,Alignment of data members and member functions for performance,"<p>Is it true aligning data members of a struct/class no longer yields the benefits it used to, especially on nehalem because of hardware improvements? If so, is it still the case that alignment will always make better performance, just very small noticeable improvements compared with on past CPUs?</p>

<p>Does alignment of member variables extend to member functions? I believe I once read (it could be on the wikibooks ""C++ performance"") that there are rules for ""packing"" member functions into various ""units"" (i.e. source files) for optimum loading into the instruction cache? (If I have got my terminology wrong here please correct me).</p>
","is it true aligning data members of a structclass no longer yields the benefits it used to especially on nehalem because of hardware improvements if so is it still the case that alignment will always make better performance just very small noticeable improvements compared with on past cpus
does alignment of member variables extend to member functions i believe i once read it could be on the wikibooks c performance that there are rules for packing member functions into various units ie source files for optimum loading into the instruction cache if i have got my terminology wrong here please correct me
",1,"Is it true aligning data members of a struct/class no longer yields the benefits it used to, especially on nehalem because of hardware improvements? If so, is it still the case that alignment will always make better performance, just very small noticeable improvements compared with on past CPUs?
Does alignment of member variables extend to member functions? I believe I once read (it could be on the wikibooks ""C++ performance"") that there are rules for ""packing"" member functions into various ""units"" (i.e. source files) for optimum loading into the instruction cache? (If I have got my terminology wrong here please correct me).
","Alignment of data members and member functions for performance
Is it true aligning data members of a struct/class no longer yields the benefits it used to, especially on nehalem because of hardware improvements? If so, is it still the case that alignment will always make better performance, just very small noticeable improvements compared with on past CPUs?
Does alignment of member variables extend to member functions? I believe I once read (it could be on the wikibooks ""C++ performance"") that there are rules for ""packing"" member functions into various ""units"" (i.e. source files) for optimum loading into the instruction cache? (If I have got my terminology wrong here please correct me).
",0,0,1,1,0,0,0,- Is a theoretical question,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,1,0,0,,,Architecture,Architecture,,Architecture,None,None,,None
21763617,21763617,Why is getting a member faster than calling hasOwnProperty?,"<p>I'm writing a JS object that needs to perform really basic key-value caching on string:function pairs. The class runs on the client and caches partially-compiled templates for rendering part of the page, so it may have anywhere from 20-200 items. </p>

<p>Before actually writing the class, I figured it would be a good idea to see what the fastest cache retrieval method was. The options that came to mind were:</p>

<p><strong>1. Basic property access:</strong></p>

<pre><code>if (x[k] !== undefined) {
    v = x[k];
}
</code></pre>

<p><strong>2. Key Check (Own):</strong></p>

<pre><code>if (x.hasOwnProperty(k)) {
    v = x[k];
}
</code></pre>

<p><strong>3. Key Check (General):</strong></p>

<pre><code>if (k in x) {
    v = x[k];
}
</code></pre>

<p>I assumed that 3 would be fastest (checks to see if the property exists but doesn't retrieve it or worry about where it exists) and 1 would be slowest (actually gets the property, even if it doesn't do anything).</p>

<p><a href=""http://jsperf.com/double-lookup"">Putting all of these into jsPerf</a> yielded some very strange results. In both Chrome (and Chromium) and IE, #1 is about twice as fast. In Firefox, #3 has a minor edge, but performance is similar between all three. It didn't matter if I was running in a VM or not, and didn't change a lot between versions.</p>

<p>I'm having trouble explaining these results. It might be that #1 notices that nothing will happen to the data and so just checks for the key internally, but why is it faster than #3? Why does #3 not get the same optimization?</p>

<p>What is causing these results? Is there some JIT optimization I might be hitting that skews the data? </p>

<p>More importantly, why is this so drastically different between browsers, with all options being roughly equal in FF?</p>
","im writing a js object that needs to perform really basic keyvalue caching on stringfunction pairs the class runs on the client and caches partiallycompiled templates for rendering part of the page so it may have anywhere from 20200 items 
before actually writing the class i figured it would be a good idea to see what the fastest cache retrieval method was the options that came to mind were
1 basic property access
2 key check own
3 key check general
i assumed that 3 would be fastest checks to see if the property exists but doesnt retrieve it or worry about where it exists and 1 would be slowest actually gets the property even if it doesnt do anything
putting all of these into jsperf yielded some very strange results in both chrome and chromium and ie 1 is about twice as fast in firefox 3 has a minor edge but performance is similar between all three it didnt matter if i was running in a vm or not and didnt change a lot between versions
im having trouble explaining these results it might be that 1 notices that nothing will happen to the data and so just checks for the key internally but why is it faster than 3 why does 3 not get the same optimization
what is causing these results is there some jit optimization i might be hitting that skews the data 
more importantly why is this so drastically different between browsers with all options being roughly equal in ff
",1,"I'm writing a JS object that needs to perform really basic key-value caching on string:function pairs. The class runs on the client and caches partially-compiled templates for rendering part of the page, so it may have anywhere from 20-200 items. 
Before actually writing the class, I figured it would be a good idea to see what the fastest cache retrieval method was. The options that came to mind were:
1. Basic property access:
if (x[k] !== undefined) {
    v = x[k];
}

2. Key Check (Own):
if (x.hasOwnProperty(k)) {
    v = x[k];
}

3. Key Check (General):
if (k in x) {
    v = x[k];
}

I assumed that 3 would be fastest (checks to see if the property exists but doesn't retrieve it or worry about where it exists) and 1 would be slowest (actually gets the property, even if it doesn't do anything).
Putting all of these into jsPerf yielded some very strange results. In both Chrome (and Chromium) and IE, #1 is about twice as fast. In Firefox, #3 has a minor edge, but performance is similar between all three. It didn't matter if I was running in a VM or not, and didn't change a lot between versions.
I'm having trouble explaining these results. It might be that #1 notices that nothing will happen to the data and so just checks for the key internally, but why is it faster than #3? Why does #3 not get the same optimization?
What is causing these results? Is there some JIT optimization I might be hitting that skews the data? 
More importantly, why is this so drastically different between browsers, with all options being roughly equal in FF?
","Why is getting a member faster than calling hasOwnProperty?
I'm writing a JS object that needs to perform really basic key-value caching on string:function pairs. The class runs on the client and caches partially-compiled templates for rendering part of the page, so it may have anywhere from 20-200 items. 
Before actually writing the class, I figured it would be a good idea to see what the fastest cache retrieval method was. The options that came to mind were:
1. Basic property access:
if (x[k] !== undefined) {
    v = x[k];
}

2. Key Check (Own):
if (x.hasOwnProperty(k)) {
    v = x[k];
}

3. Key Check (General):
if (k in x) {
    v = x[k];
}

I assumed that 3 would be fastest (checks to see if the property exists but doesn't retrieve it or worry about where it exists) and 1 would be slowest (actually gets the property, even if it doesn't do anything).
Putting all of these into jsPerf yielded some very strange results. In both Chrome (and Chromium) and IE, #1 is about twice as fast. In Firefox, #3 has a minor edge, but performance is similar between all three. It didn't matter if I was running in a VM or not, and didn't change a lot between versions.
I'm having trouble explaining these results. It might be that #1 notices that nothing will happen to the data and so just checks for the key internally, but why is it faster than #3? Why does #3 not get the same optimization?
What is causing these results? Is there some JIT optimization I might be hitting that skews the data? 
More importantly, why is this so drastically different between browsers, with all options being roughly equal in FF?
",0,0,1,0,0,0,0,- Is a question comparing approaches,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
18132816,18132816,C++ destructor runtime error: failed to munmap,"<p>I've defined a class called ClusterSet that just has one field, called <code>clusters</code>: </p>

<pre><code>class ClusterSet {
   std::map&lt;std::string, std::map&lt;std::string, float&gt;* &gt; clusters;

   public:
      typedef std::map&lt;std::string, std::map&lt;std::string, float&gt; *&gt;::iterator iterator;
      typedef std::map&lt;std::string, std::map&lt;std::string, float&gt; *&gt;::const_iterator const_iterator;

      iterator begin() { return clusters.begin(); }
      const_iterator begin() const { return clusters.begin(); }
      iterator end() { return clusters.end(); }
      const_iterator end() const { return clusters.end(); }

      void create_cluster(std::string representative);
      void add_member(std::string representative, std::string member, float similarity);
      int write_to_file(std::string outputfile);
      int size();
      ~ClusterSet();
};
</code></pre>

<p>In my <code>create_cluster</code> method, I use <code>new</code> to allocate memory for the inner map, and store this pointer in <code>clusters</code>.  I defined a destructor so that I can deallocate all this memory:</p>

<pre><code>ClusterSet::~ClusterSet() {
  ClusterSet::iterator clust_it;
  for (clust_it = clusters.begin(); clust_it != clusters.end(); ++clust_it) {
      std::cout &lt;&lt; ""Deleting members for "" &lt;&lt; clust_it-&gt;first &lt;&lt; std::endl;
      delete clust_it-&gt;second;
  }
}
</code></pre>

<p>When my destructor is called, it seems to deallocate all the inner maps correctly (it prints out ""Deleting members for..."" for each one).  However, once that's done I get a runtime error that says ""failed to ""munmap"" 1068 bytes: Invalid argument"".  What's causing this?</p>

<p>I have briefly looked at the ""rule of three"" but I don't understand why I would need a copy constructor or an assignment operator, or how that might solve my problem.  I would never need to use either directly.</p>
","ive defined a class called clusterset that just has one field called clusters 
in my create_cluster method i use new to allocate memory for the inner map and store this pointer in clusters  i defined a destructor so that i can deallocate all this memory
when my destructor is called it seems to deallocate all the inner maps correctly it prints out deleting members for for each one  however once thats done i get a runtime error that says failed to munmap 1068 bytes invalid argument  whats causing this
i have briefly looked at the rule of three but i dont understand why i would need a copy constructor or an assignment operator or how that might solve my problem  i would never need to use either directly
",1,"I've defined a class called ClusterSet that just has one field, called clusters: 
class ClusterSet {
   std::map<std::string, std::map<std::string, float>* > clusters;

   public:
      typedef std::map<std::string, std::map<std::string, float> *>::iterator iterator;
      typedef std::map<std::string, std::map<std::string, float> *>::const_iterator const_iterator;

      iterator begin() { return clusters.begin(); }
      const_iterator begin() const { return clusters.begin(); }
      iterator end() { return clusters.end(); }
      const_iterator end() const { return clusters.end(); }

      void create_cluster(std::string representative);
      void add_member(std::string representative, std::string member, float similarity);
      int write_to_file(std::string outputfile);
      int size();
      ~ClusterSet();
};

In my create_cluster method, I use new to allocate memory for the inner map, and store this pointer in clusters.  I defined a destructor so that I can deallocate all this memory:
ClusterSet::~ClusterSet() {
  ClusterSet::iterator clust_it;
  for (clust_it = clusters.begin(); clust_it != clusters.end(); ++clust_it) {
      std::cout << ""Deleting members for "" << clust_it->first << std::endl;
      delete clust_it->second;
  }
}

When my destructor is called, it seems to deallocate all the inner maps correctly (it prints out ""Deleting members for..."" for each one).  However, once that's done I get a runtime error that says ""failed to ""munmap"" 1068 bytes: Invalid argument"".  What's causing this?
I have briefly looked at the ""rule of three"" but I don't understand why I would need a copy constructor or an assignment operator, or how that might solve my problem.  I would never need to use either directly.
","C++ destructor runtime error: failed to munmap
I've defined a class called ClusterSet that just has one field, called clusters: 
class ClusterSet {
   std::map<std::string, std::map<std::string, float>* > clusters;

   public:
      typedef std::map<std::string, std::map<std::string, float> *>::iterator iterator;
      typedef std::map<std::string, std::map<std::string, float> *>::const_iterator const_iterator;

      iterator begin() { return clusters.begin(); }
      const_iterator begin() const { return clusters.begin(); }
      iterator end() { return clusters.end(); }
      const_iterator end() const { return clusters.end(); }

      void create_cluster(std::string representative);
      void add_member(std::string representative, std::string member, float similarity);
      int write_to_file(std::string outputfile);
      int size();
      ~ClusterSet();
};

In my create_cluster method, I use new to allocate memory for the inner map, and store this pointer in clusters.  I defined a destructor so that I can deallocate all this memory:
ClusterSet::~ClusterSet() {
  ClusterSet::iterator clust_it;
  for (clust_it = clusters.begin(); clust_it != clusters.end(); ++clust_it) {
      std::cout << ""Deleting members for "" << clust_it->first << std::endl;
      delete clust_it->second;
  }
}

When my destructor is called, it seems to deallocate all the inner maps correctly (it prints out ""Deleting members for..."" for each one).  However, once that's done I get a runtime error that says ""failed to ""munmap"" 1068 bytes: Invalid argument"".  What's causing this?
I have briefly looked at the ""rule of three"" but I don't understand why I would need a copy constructor or an assignment operator, or how that might solve my problem.  I would never need to use either directly.
",1,0,1,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,0,1,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
5949514,5949514,Memory allocation problem when creating a class C++,"<p>Still learning C++, and still having specific errors :). I have an object of the following composition:</p>

<pre><code>class document {
private:
    char *denumire;
    char *tema;
    char *autorul;
    int num_pag;
    data last_edit;
public:
    document();
    document (const char *s1,const char *s2,const char *s3, int i1, data d1);
    document (const document&amp;);
    document (const char *s1);
    ~document ();
    void printdoc(void);
    void chden(char *s);
    void chtema(char *s);
    void chaut(char *s);
    void chnumpag(int n);
    void chdata(data d);
};
</code></pre>

<p>The problem is that when I try to initialize it with the following constructor, I get segmentation fault:</p>

<pre><code>document::document (char *s1, char *s2, char *s3, int i1, data d1) {
    denumire=new char[strlen(s1)+1];
    strcpy(denumire,s1);
    tema=new char[strlen(s2)+1];
    strcpy(tema,s2);
    autorul=new char[strlen(s3)+1];
    strcpy(autorul,s3);
    num_pag=i1;
    last_edit.an=d1.an;
    last_edit.luna=d1.luna;
    last_edit.zi=d1.zi;
    cout &lt;&lt;""Setarea documentului finisata\n"";
}
</code></pre>

<p>As much as I understand, all the variables are assigned correctly, because the message ""Setarea documentului finisata"" appears, and after that the segfault appears. All the code compiles fine, without any warnings. 
Also, I tried to search for something on Google, but couldn't find situations similar to mine. What could be the cause of such strange behaviour?</p>

<p>PS: The implementation of the copy constructor:</p>

<pre><code>document::document (const document&amp; a) :
    denumire(new char [strlen(a.denumire)+1]),
    tema(new char[strlen(a.tema)+1]),
    autorul(new char[strlen(a.autorul)+1]),
    num_pag(a.num_pag),
    last_edit(a.last_edit)
{
    strcpy(denumire,a.denumire);
    strcpy(tema,a.tema);
    strcpy(autorul,a.autorul);
}
</code></pre>

<p>I took it from a teacher's examples. Also, I initialise the variables in the following way:</p>

<pre><code>document c(s1.c_str(),s2.c_str(),s3.c_str(),i1,d1)
</code></pre>

<p>because the teacher requires us that the object contains dinamically created strings :)</p>
","still learning c and still having specific errors  i have an object of the following composition
the problem is that when i try to initialize it with the following constructor i get segmentation fault
as much as i understand all the variables are assigned correctly because the message setarea documentului finisata appears and after that the segfault appears all the code compiles fine without any warnings 
also i tried to search for something on google but couldnt find situations similar to mine what could be the cause of such strange behaviour
ps the implementation of the copy constructor
i took it from a teachers examples also i initialise the variables in the following way
because the teacher requires us that the object contains dinamically created strings 
",1,"Still learning C++, and still having specific errors :). I have an object of the following composition:
class document {
private:
    char *denumire;
    char *tema;
    char *autorul;
    int num_pag;
    data last_edit;
public:
    document();
    document (const char *s1,const char *s2,const char *s3, int i1, data d1);
    document (const document&);
    document (const char *s1);
    ~document ();
    void printdoc(void);
    void chden(char *s);
    void chtema(char *s);
    void chaut(char *s);
    void chnumpag(int n);
    void chdata(data d);
};

The problem is that when I try to initialize it with the following constructor, I get segmentation fault:
document::document (char *s1, char *s2, char *s3, int i1, data d1) {
    denumire=new char[strlen(s1)+1];
    strcpy(denumire,s1);
    tema=new char[strlen(s2)+1];
    strcpy(tema,s2);
    autorul=new char[strlen(s3)+1];
    strcpy(autorul,s3);
    num_pag=i1;
    last_edit.an=d1.an;
    last_edit.luna=d1.luna;
    last_edit.zi=d1.zi;
    cout <<""Setarea documentului finisata\n"";
}

As much as I understand, all the variables are assigned correctly, because the message ""Setarea documentului finisata"" appears, and after that the segfault appears. All the code compiles fine, without any warnings. 
Also, I tried to search for something on Google, but couldn't find situations similar to mine. What could be the cause of such strange behaviour?
PS: The implementation of the copy constructor:
document::document (const document& a) :
    denumire(new char [strlen(a.denumire)+1]),
    tema(new char[strlen(a.tema)+1]),
    autorul(new char[strlen(a.autorul)+1]),
    num_pag(a.num_pag),
    last_edit(a.last_edit)
{
    strcpy(denumire,a.denumire);
    strcpy(tema,a.tema);
    strcpy(autorul,a.autorul);
}

I took it from a teacher's examples. Also, I initialise the variables in the following way:
document c(s1.c_str(),s2.c_str(),s3.c_str(),i1,d1)

because the teacher requires us that the object contains dinamically created strings :)
","Memory allocation problem when creating a class C++
Still learning C++, and still having specific errors :). I have an object of the following composition:
class document {
private:
    char *denumire;
    char *tema;
    char *autorul;
    int num_pag;
    data last_edit;
public:
    document();
    document (const char *s1,const char *s2,const char *s3, int i1, data d1);
    document (const document&);
    document (const char *s1);
    ~document ();
    void printdoc(void);
    void chden(char *s);
    void chtema(char *s);
    void chaut(char *s);
    void chnumpag(int n);
    void chdata(data d);
};

The problem is that when I try to initialize it with the following constructor, I get segmentation fault:
document::document (char *s1, char *s2, char *s3, int i1, data d1) {
    denumire=new char[strlen(s1)+1];
    strcpy(denumire,s1);
    tema=new char[strlen(s2)+1];
    strcpy(tema,s2);
    autorul=new char[strlen(s3)+1];
    strcpy(autorul,s3);
    num_pag=i1;
    last_edit.an=d1.an;
    last_edit.luna=d1.luna;
    last_edit.zi=d1.zi;
    cout <<""Setarea documentului finisata\n"";
}

As much as I understand, all the variables are assigned correctly, because the message ""Setarea documentului finisata"" appears, and after that the segfault appears. All the code compiles fine, without any warnings. 
Also, I tried to search for something on Google, but couldn't find situations similar to mine. What could be the cause of such strange behaviour?
PS: The implementation of the copy constructor:
document::document (const document& a) :
    denumire(new char [strlen(a.denumire)+1]),
    tema(new char[strlen(a.tema)+1]),
    autorul(new char[strlen(a.autorul)+1]),
    num_pag(a.num_pag),
    last_edit(a.last_edit)
{
    strcpy(denumire,a.denumire);
    strcpy(tema,a.tema);
    strcpy(autorul,a.autorul);
}

I took it from a teacher's examples. Also, I initialise the variables in the following way:
document c(s1.c_str(),s2.c_str(),s3.c_str(),i1,d1)

because the teacher requires us that the object contains dinamically created strings :)
",1,1,0,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
34145720,34145720,JavaScript postfix a boolean / one-shot flag for throttling an event,"<p>I want to use a flag once, then convert it to false no matter what, after the expression returns the truthiness:</p>
<pre><code>var isProceding = true;
someObject.addEventListener(&quot;someEvent&quot;, doOnce); // event fires many times

function doOnce () {
    if (isProceding) {
        isProceding = false; // i want to join this line with the previous
        // do stuff
        someObject.removeEventListener(&quot;someEvent&quot;, doOnce);
    }
}
</code></pre>
<hr />
<blockquote>
<p><em><strong>from <a href=""http://jsperf.com/coercing-booleans"" rel=""nofollow noreferrer"">jsperf</a></strong></em></p>
<ol>
<li><p><code>if (!isDone++) {</code> <strong>343,020,200</strong> single coercion post fix</p>
</li>
<li><p><code>if (isProceding &amp;&amp; isProceding--) {</code> <strong>342,466,581 short circuit post fix</strong></p>
</li>
<li><p><code>if (isProceding) { isProceding = false;</code> <strong>338,360,292</strong> standard</p>
</li>
<li><p><code>if (0 &lt; isProceding--) {</code> <strong>278,447,221 comparison coercion post fix</strong></p>
</li>
<li><p><code>if (isProceding --&gt; false) {</code> <strong>9,983,236 double coercion postfix</strong></p>
</li>
</ol>
</blockquote>
","i want to use a flag once then convert it to false no matter what after the expression returns the truthiness


from jsperf

if isdone  343020200 single coercion post fix

if isproceding  isproceding  342466581 short circuit post fix

if isproceding  isproceding  false 338360292 standard

if 0  isproceding  278447221 comparison coercion post fix

if isproceding  false  9983236 double coercion postfix



",1,"I want to use a flag once, then convert it to false no matter what, after the expression returns the truthiness:
var isProceding = true;
someObject.addEventListener(""someEvent"", doOnce); // event fires many times

function doOnce () {
    if (isProceding) {
        isProceding = false; // i want to join this line with the previous
        // do stuff
        someObject.removeEventListener(""someEvent"", doOnce);
    }
}



from jsperf

if (!isDone++) { 343,020,200 single coercion post fix

if (isProceding && isProceding--) { 342,466,581 short circuit post fix

if (isProceding) { isProceding = false; 338,360,292 standard

if (0 < isProceding--) { 278,447,221 comparison coercion post fix

if (isProceding --> false) { 9,983,236 double coercion postfix



","JavaScript postfix a boolean / one-shot flag for throttling an event
I want to use a flag once, then convert it to false no matter what, after the expression returns the truthiness:
var isProceding = true;
someObject.addEventListener(""someEvent"", doOnce); // event fires many times

function doOnce () {
    if (isProceding) {
        isProceding = false; // i want to join this line with the previous
        // do stuff
        someObject.removeEventListener(""someEvent"", doOnce);
    }
}



from jsperf

if (!isDone++) { 343,020,200 single coercion post fix

if (isProceding && isProceding--) { 342,466,581 short circuit post fix

if (isProceding) { isProceding = false; 338,360,292 standard

if (0 < isProceding--) { 278,447,221 comparison coercion post fix

if (isProceding --> false) { 9,983,236 double coercion postfix



",1,1,0,0,0,0,0,- User just doesn't know how to approach a problem,"Label: 1

Reasoning:

This question expresses technical debt because the code is trying to achieve a specific functionality (i.e., setting a flag to false after an event is triggered) and the user is exploring different approaches to optimize the code. The fact that the user is benchmarking different implementations using jsperf and providing performance metrics indicates that the current implementation is not satisfactory and requires improvement.

The technical debt in this question is inherent, as the user is aware of the inefficiencies in the code and is actively seeking ways to optimize it. The use of phrases like ""I want to join this line with the previous"" and the exploration of alternative implementations (e.g., using post-fix increment, short-circuit evaluation, and coercion) suggest that the user is concerned about improving the performance and readability of the code.

Additionally, the user's concern with optimizing the code and reducing the execution time indicates that the current implementation is not meeting the required performance standards, which is a classic sign of technical debt.

Overall, the question demonstrates a clear awareness of the need for improvement and a desire to refactor the code to achieve better performance, making it an expression of technical debt.",Yes,1,1,Runtime Efficiency,Algorithm Optimization,"User wants a more succinct algorithm, doesn’t necessarily discuss execution speed",Runtime Efficiency,1,1,0,0,0,0,,,Code,Code,,Code,None,None,,None
70854754,70854754,Recording every function call of a program in Java,"<p>I want to log every function call (timestamp and and function name) of Java a program (including threads/processes)
In Python I can simply have a profiler that records function entries in the program as follows:</p>
<pre><code>def tracefunc(frame, event, arg, indent=[0]):
      if event == &quot;call&quot;:
          indent[0] += 2
          print(&quot;-&quot; * indent[0] + &quot;&gt; call function&quot;, frame.f_code.co_name)
      elif event == &quot;return&quot;:
          print(&quot;&lt;&quot; + &quot;-&quot; * indent[0], &quot;exit function&quot;, frame.f_code.co_name)
          indent[0] -= 2
      return tracefunc
</code></pre>
<p>Inside my code I use it as:</p>
<pre><code>import sys
import tracer

sys.setprofile(tracer.tracefunc)
</code></pre>
<p>I was wondering if there a similar thing in Java. Or do I need to use something else? Note that I want to log a big program such as Cassandra</p>
","i want to log every function call timestamp and and function name of java a program including threadsprocesses
in python i can simply have a profiler that records function entries in the program as follows
inside my code i use it as
i was wondering if there a similar thing in java or do i need to use something else note that i want to log a big program such as cassandra
",1,"I want to log every function call (timestamp and and function name) of Java a program (including threads/processes)
In Python I can simply have a profiler that records function entries in the program as follows:
def tracefunc(frame, event, arg, indent=[0]):
      if event == ""call"":
          indent[0] += 2
          print(""-"" * indent[0] + ""> call function"", frame.f_code.co_name)
      elif event == ""return"":
          print(""<"" + ""-"" * indent[0], ""exit function"", frame.f_code.co_name)
          indent[0] -= 2
      return tracefunc

Inside my code I use it as:
import sys
import tracer

sys.setprofile(tracer.tracefunc)

I was wondering if there a similar thing in Java. Or do I need to use something else? Note that I want to log a big program such as Cassandra
","Recording every function call of a program in Java
I want to log every function call (timestamp and and function name) of Java a program (including threads/processes)
In Python I can simply have a profiler that records function entries in the program as follows:
def tracefunc(frame, event, arg, indent=[0]):
      if event == ""call"":
          indent[0] += 2
          print(""-"" * indent[0] + ""> call function"", frame.f_code.co_name)
      elif event == ""return"":
          print(""<"" + ""-"" * indent[0], ""exit function"", frame.f_code.co_name)
          indent[0] -= 2
      return tracefunc

Inside my code I use it as:
import sys
import tracer

sys.setprofile(tracer.tracefunc)

I was wondering if there a similar thing in Java. Or do I need to use something else? Note that I want to log a big program such as Cassandra
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Infrastructure,Infrastructure,,Infrastructure,Mild,Mild,,Mild
72546002,72546002,Display HashMap content in tree view,"<p>I need your help because I don't find the solution in Java for my problem.</p>
<p>I stored in object <code>LinkedHashMap&lt;String, String&gt;</code> this content:</p>
<pre><code>TAG1.TAG2.TAG11 : value1
TAG1.TAG2.TAG12 : value2
TAG1.TAG2.TAG3.TAG131 : value3
TAG1.TAG2.TAG3.TAG132 : value4
TAG1.TAG2.TAG3.TAG133 : value5
TAG1.TAG2.TAG3.TAG134 : value6
TAG1.TAG4.TAG5.TAG21 : value7
TAG1.TAG4.TAG5.TAG22 : value8
TAG1.TAG4.TAG5.TAG23 : value9
TAG6 : value10
</code></pre>
<p>I need to display if a tag has 2 or more children, the list of child.
Here is the expected result:</p>
<pre><code>TAG1.TAG2
    TAG11 : value1
    TAG12 : value2
TAG1.TAG2.TAG3
    TAG131 : value3
    TAG132 : value4
    TAG133 : value5
    TAG134 : value6
TAG1.TAG4.TAG5
    TAG21 : value7
    TAG22 : value8
    TAG23 : value9
TAG6 : value10
</code></pre>
<hr />
<p><strong>EDIT 14/06/2022 :</strong></p>
<p>In fact, my original analyse is bad because initialy I have a XML file :</p>
<pre><code>&lt;TAG1&gt;
 &lt;TAG2&gt;
  &lt;TAG11&gt;value1&lt;/TAG11&gt;
  &lt;TAG12&gt;value2&lt;/TAG12&gt;
  &lt;TAG3&gt;
   &lt;TAG131&gt;value3&lt;/TAG131&gt;
   &lt;TAG132&gt;value4&lt;/TAG132&gt;
   &lt;TAG133&gt;value5&lt;/TAG133&gt;
   &lt;TAG134&gt;value6&lt;/TAG134&gt;
  &lt;/TAG3&gt;
 &lt;/TAG2&gt;
 &lt;TAG4&gt;
  &lt;TAG5&gt;
   &lt;TAG21&gt;value7&lt;/TAG21&gt;
   &lt;TAG22&gt;value8&lt;/TAG22&gt;
   &lt;TAG23&gt;value9&lt;/TAG23&gt;
  &lt;/TAG5&gt;
 &lt;/TAG4&gt;
&lt;/TAG1&gt;
&lt;TAG6&gt;value10&lt;/TAG6&gt;
</code></pre>
<p>And I created a map to store it :</p>
<pre><code>TAG1.TAG2.TAG11 : value1
TAG1.TAG2.TAG12 : value2
TAG1.TAG2.TAG3.TAG131 : value3
TAG1.TAG2.TAG3.TAG132 : value4
TAG1.TAG2.TAG3.TAG133 : value5
TAG1.TAG2.TAG3.TAG134 : value6
TAG1.TAG4.TAG5.TAG21 : value7
TAG1.TAG4.TAG5.TAG22 : value8
TAG1.TAG4.TAG5.TAG23 : value9
TAG6 : value10
</code></pre>
<p>But, today I have a this case :</p>
<pre><code>&lt;TAG1&gt;
 &lt;TAG2&gt;
  &lt;TAG11&gt;value1&lt;/TAG11&gt;
  &lt;TAG12&gt;value2&lt;/TAG12&gt;
  &lt;TAG3&gt;
   &lt;TAG131&gt;value3&lt;/TAG131&gt;
   &lt;TAG132&gt;value4&lt;/TAG132&gt;
   &lt;TAG133&gt;value5&lt;/TAG133&gt;
   &lt;TAG134&gt;value6&lt;/TAG134&gt;
  &lt;/TAG3&gt;
  &lt;TAG3&gt;
   &lt;TAG131&gt;value11&lt;/TAG131&gt;
   &lt;TAG132&gt;value12&lt;/TAG132&gt;
   &lt;TAG133&gt;value13&lt;/TAG133&gt;
   &lt;TAG134&gt;value14&lt;/TAG134&gt;
  &lt;/TAG3&gt;
 &lt;/TAG2&gt;
 &lt;TAG4&gt;
  &lt;TAG5&gt;
   &lt;TAG21&gt;value7&lt;/TAG21&gt;
   &lt;TAG22&gt;value8&lt;/TAG22&gt;
   &lt;TAG23&gt;value9&lt;/TAG23&gt;
  &lt;/TAG5&gt;
 &lt;/TAG4&gt;
&lt;/TAG1&gt;
&lt;TAG6&gt;value10&lt;/TAG6&gt;
</code></pre>
<p>But the Map object does not allow to store many keys (in the example many TAG3). Have you got an idea how I can resolve this problem ?</p>
<hr />
<p><strong>EDIT 15/06/2022 :</strong></p>
<p>In fact the expected result needs to keep the original XML structure.
Here the result of last sample :</p>
<pre><code>TAG1.TAG2
    TAG11 : value1
    TAG12 : value2
TAG1.TAG2.TAG3
    TAG131 : value3
    TAG132 : value4
    TAG133 : value5
    TAG134 : value6
TAG1.TAG2.TAG3
    TAG131 : value11
    TAG132 : value12
    TAG133 : value13
    TAG134 : value14
TAG1.TAG4.TAG5
    TAG21 : value7
    TAG22 : value8
    TAG23 : value9
TAG6 : value10
</code></pre>
<p>It's to display xml more human reader.</p>
<hr />
<p><strong>EDIT 04/07/2022 :</strong></p>
<p>I detect a problem of inconsistent with &quot;new TreeMap&lt;&gt;(Comparator.comparingInt(MyTag::getAppearanceOrder)&quot;. Indeed, some MyTag object are the same AppearanceOrder, so there is a problem of inconsistent ordering. Some value in Map are so removed.
To resolve I used :</p>
<pre><code>map.entrySet().stream().sorted(Map.Entry.comparingByKey(. . .))
</code></pre>
<p>And I store the result in Map with collect().</p>
<p>Below the working code :</p>
<pre><code>public class Main {
    public static void main(String[] args) throws ParserConfigurationException, IOException, SAXException {
        //Accessing the xml file
        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
        DocumentBuilder builder = factory.newDocumentBuilder();

        Document document = builder.parse(new File(&quot;data.xml&quot;));

        document.getDocumentElement().normalize();
        Element root = document.getDocumentElement();

        //Retrieving a List of records where each record contains: the original chain of tags, the numbered chain of tags and the value
        String tagSep = &quot;.&quot;;
        List&lt;Record&gt; listRecords = new ArrayList&lt;&gt;();
        visitXMLFile(listRecords, root.getChildNodes(), tagSep, &quot;&quot;, &quot;&quot;, new HashMap&lt;&gt;());

        //Queue sorted by the numbered tag's length in descending order (from the longest to the shortest)
        PriorityQueue&lt;Record&gt; queue = new PriorityQueue&lt;&gt;(Comparator.comparing(Record::getTagNumberedLen).reversed());
        queue.addAll(listRecords);

        //Using a set to have unique numbered tags (no duplicates) to group by in the resulting map
        Set&lt;MyTag&gt; setMyTags = new HashSet&lt;&gt;();

        //Checking for each numbered tag if its largest substring is equal to any other numbered tag's beginning:
        //  - if it does, then the substring is collected as a key to group by within the final map
        //
        //  - if it doesn't, then another substring is generated from the previous substring until a matching value is found.
        //          If no value is found, then the numbered tag is collected entirely as a key for the resulting map.
        while (!queue.isEmpty()) {
            Record rec = queue.poll();

            //This loop keeps creating substrings of the current numbered tag until:
            //  - the substring matches another numbered tag's beginning
            //  - or no more substrings can be generated
            int lastIndexTagNum = rec.getTagNumbered().lastIndexOf(tagSep);
            int lastIndexTag = rec.getTag().lastIndexOf(tagSep);
            while (lastIndexTagNum &gt; 0) {

                //Checking if the substring matches the beginning of any numbered tag except the current one
                String subStrTagNum = rec.getTagNumbered().substring(0, lastIndexTagNum);
                if (listRecords.stream().anyMatch(r -&gt; !r.getTagNumbered().equals(rec.getTagNumbered()) &amp;&amp; r.getTagNumbered().startsWith(subStrTagNum + tagSep))) {

                    String subStrTag = rec.getTag().substring(0, lastIndexTag);
                    int appearanceOrder = listRecords.stream().filter(r -&gt; r.getTagNumbered().startsWith(subStrTagNum + tagSep)).map(r -&gt; r.getAppearanceOrder()).min(Comparator.naturalOrder()).orElse(0);

                    //If a match is found then the current substring is added to the set and the substring iteration is interrupted
                    setMyTags.add(new MyTag(subStrTag, subStrTagNum + tagSep, appearanceOrder));
                    break;
                }

                //Creating a new substring from the previous substring if no match has been found
                lastIndexTagNum = rec.getTagNumbered().substring(0, lastIndexTagNum).lastIndexOf(tagSep);
                lastIndexTag = rec.getTag().substring(0, lastIndexTag).lastIndexOf(tagSep);
            }

            //If no substrings of the current numbered tag matches the beginning of any other numbered tag,
            //then the current numbered tag is collected as a key for the resulting map
            if (lastIndexTagNum &lt; 0) {
                int appearanceOrder = listRecords.stream().filter(r -&gt; r.getTagNumbered().startsWith(rec.getTagNumbered())).map(r -&gt; r.getAppearanceOrder()).min(Comparator.naturalOrder()).orElse(0);
                setMyTags.add(new MyTag(rec.getTag(), rec.getTagNumbered(), appearanceOrder));
            }
        }

        //Creating a temporary resulting map (not sorted as the input)
        Map&lt;MyTag, List&lt;String&gt;&gt; mapTemp = listRecords.stream()
                .collect(Collectors.toMap(
                                rec -&gt; {
                                    //Looking for the longest numbered tag which matches the beginning of the current record's numbered tag.
                                    //The reason why we need the longest match (i.e. the most accurate) is because some elements
                                    //may share the same parents but be on different levels, for example the values 3, 4, 5 and 6
                                    //have a key whose beginning matches both &quot;TAG1.TAG2&quot; and &quot;TAG1.TAG2.TAG3&quot;, but only the longest
                                    //match is actually the right one.
                                    return setMyTags.stream().filter(mt -&gt; rec.getTagNumbered().startsWith(mt.getTagNumbered())).max(Comparator.comparingInt(MyTag::getTagNumberedLen)).orElseThrow(() -&gt; new RuntimeException(&quot;No key found&quot;));
                                },
                                rec -&gt; {
                                    //Retrieving, like above, the numbered tag that will be used to map the current value
                                    MyTag myTag = setMyTags.stream().filter(mt -&gt; rec.getTagNumbered().startsWith(mt.getTagNumbered())).max(Comparator.comparingInt(MyTag::getTagNumberedLen)).orElseThrow(() -&gt; new RuntimeException(&quot;No key found&quot;));

                                    //If the new numbered tag and the record's numbered tag are equal then a List with the current value is returned
                                    if (myTag.getTagNumbered().equals(rec.getTagNumbered())) {
                                        return new ArrayList&lt;&gt;(List.of(rec.getValue()));
                                    } else {    //If the new numbered tag is a substring of the record's numbered tag then the rest of the current (non-numbered) tag is added to the value
                                        return new ArrayList&lt;&gt;(List.of(rec.getTag().substring(myTag.getTag().length() + 1) + &quot; : &quot; + rec.getValue()));
                                    }
                                },
                                //Handling colliding cases by merging the lists together
                                (list1, list2) -&gt; {
                                    list1.addAll(list2);
                                    return list1;
                                }
                        )
                );

        //Creating a TreeMap whose ordering is based on the insertion order of the input
        Map&lt;MyTag, List&lt;String&gt;&gt; mapRes =
                mapTemp.entrySet().stream()
                        .sorted(Map.Entry.comparingByKey(Comparator.comparingInt(MyTag::getAppearanceOrder)))
                        .collect(Collectors.toMap(
                                Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -&gt; e1, LinkedHashMap::new));

        //Printing the resulting map
        for (Map.Entry&lt;MyTag, List&lt;String&gt;&gt; entry : mapRes.entrySet()) {
            System.out.println(entry.getKey());
            for (String value : entry.getValue()) {
                System.out.println(&quot;\t&quot; + value);
            }
        }
    }

    private static void visitXMLFile(List&lt;Record&gt; listInput, NodeList nodeList, String tagSep, String tag, String tagNumbered, Map&lt;String, Integer&gt; mapTagOccurrence) {
        for (int i = 0; i &lt; nodeList.getLength(); i++) {
            Node node = nodeList.item(i);
            if (node.hasChildNodes()) {
                String newTag = tag.isEmpty() ? node.getNodeName() : tag + tagSep + node.getNodeName();

                //Setting or incrementing the number of appearances of a tag chain
                //(sometimes a same chain of tags can be repeated, ex: TAG1.TAG2.TAG3)
                if (!mapTagOccurrence.containsKey(newTag)) {
                    mapTagOccurrence.put(newTag, 1);
                } else {
                    mapTagOccurrence.computeIfPresent(newTag, (key, val) -&gt; val + 1);
                }

                //Creating a numbered version of the tag where its number of appearances is added at the end.
                //This is done to uniquely identify different groups of tag chain when these are repeated (ex: TAG1.TAG2.TAG3)
                String newTagNum = tagNumbered.isEmpty() ? node.getNodeName() + mapTagOccurrence.get(newTag) : tagNumbered + tagSep + node.getNodeName() + mapTagOccurrence.get(newTag);

                visitXMLFile(listInput, node.getChildNodes(), tagSep, newTag, newTagNum, mapTagOccurrence);
            } else {
                if (!node.getTextContent().trim().equals(&quot;&quot;)) {
                    int appearanceOrder = listInput.size() + 1;
                    listInput.add(new Record(tag, tagNumbered, node.getTextContent().trim(), appearanceOrder));
                }
            }
        }
    }
}

class MyTag {

    //Tag chain for the user
    private String tag;

    //Unique tag chain for identification
    private String tagNumbered;

    private int appearanceOrder;

    public MyTag(String tag, String tagNumbered, int appearanceOrder) {
        this.tag = tag;
        this.tagNumbered = tagNumbered;
        this.appearanceOrder = appearanceOrder;
    }

    public String getTag() {
        return tag;
    }

    public String getTagNumbered() {
        return tagNumbered;
    }

    public int getTagNumberedLen() {
        return tagNumbered == null ? 0 : tagNumbered.length();
    }

    public int getAppearanceOrder() {
        return appearanceOrder;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        MyTag tagPair = (MyTag) o;
        return Objects.equals(tagNumbered, tagPair.tagNumbered);
    }

    @Override
    public int hashCode() {
        return Objects.hash(tagNumbered);
    }

    @Override
    public String toString() {
        return tag;
    }
}

class Record {

    //Tag chain for the user
    private String tag;

    //Unique tag chain for identification
    private String tagNumbered;

    private String value;

    private int appearanceOrder;

    public Record(String tag, String tagNumbered, String value, int appearanceOrder) {
        this.tag = tag;
        this.tagNumbered = tagNumbered;
        this.value = value;
        this.appearanceOrder = appearanceOrder;
    }

    public String getTag() {
        return tag;
    }

    public String getTagNumbered() {
        return tagNumbered;
    }

    public int getTagNumberedLen() {
        return tagNumbered == null ? 0 : tagNumbered.length();
    }

    public String getValue() {
        return value;
    }

    public int getAppearanceOrder() {
        return appearanceOrder;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        Record record = (Record) o;
        return Objects.equals(tagNumbered, record.tagNumbered);
    }

    @Override
    public int hashCode() {
        return Objects.hash(tagNumbered);
    }

    @Override
    public String toString() {
        return tag + &quot; - &quot; + tagNumbered + &quot; - &quot; + value;
    }
}
</code></pre>
","i need your help because i dont find the solution in java for my problem
i stored in object linkedhashmapstring string this content
i need to display if a tag has 2 or more children the list of child
here is the expected result

edit 14062022 
in fact my original analyse is bad because initialy i have a xml file 
and i created a map to store it 
but today i have a this case 
but the map object does not allow to store many keys in the example many tag3 have you got an idea how i can resolve this problem 

edit 15062022 
in fact the expected result needs to keep the original xml structure
here the result of last sample 
its to display xml more human reader

edit 04072022 
i detect a problem of inconsistent with new treemapcomparatorcomparingintmytaggetappearanceorder indeed some mytag object are the same appearanceorder so there is a problem of inconsistent ordering some value in map are so removed
to resolve i used 
and i store the result in map with collect
below the working code 
",1,"I need your help because I don't find the solution in Java for my problem.
I stored in object LinkedHashMap<String, String> this content:
TAG1.TAG2.TAG11 : value1
TAG1.TAG2.TAG12 : value2
TAG1.TAG2.TAG3.TAG131 : value3
TAG1.TAG2.TAG3.TAG132 : value4
TAG1.TAG2.TAG3.TAG133 : value5
TAG1.TAG2.TAG3.TAG134 : value6
TAG1.TAG4.TAG5.TAG21 : value7
TAG1.TAG4.TAG5.TAG22 : value8
TAG1.TAG4.TAG5.TAG23 : value9
TAG6 : value10

I need to display if a tag has 2 or more children, the list of child.
Here is the expected result:
TAG1.TAG2
    TAG11 : value1
    TAG12 : value2
TAG1.TAG2.TAG3
    TAG131 : value3
    TAG132 : value4
    TAG133 : value5
    TAG134 : value6
TAG1.TAG4.TAG5
    TAG21 : value7
    TAG22 : value8
    TAG23 : value9
TAG6 : value10


EDIT 14/06/2022 :
In fact, my original analyse is bad because initialy I have a XML file :
<TAG1>
 <TAG2>
  <TAG11>value1</TAG11>
  <TAG12>value2</TAG12>
  <TAG3>
   <TAG131>value3</TAG131>
   <TAG132>value4</TAG132>
   <TAG133>value5</TAG133>
   <TAG134>value6</TAG134>
  </TAG3>
 </TAG2>
 <TAG4>
  <TAG5>
   <TAG21>value7</TAG21>
   <TAG22>value8</TAG22>
   <TAG23>value9</TAG23>
  </TAG5>
 </TAG4>
</TAG1>
<TAG6>value10</TAG6>

And I created a map to store it :
TAG1.TAG2.TAG11 : value1
TAG1.TAG2.TAG12 : value2
TAG1.TAG2.TAG3.TAG131 : value3
TAG1.TAG2.TAG3.TAG132 : value4
TAG1.TAG2.TAG3.TAG133 : value5
TAG1.TAG2.TAG3.TAG134 : value6
TAG1.TAG4.TAG5.TAG21 : value7
TAG1.TAG4.TAG5.TAG22 : value8
TAG1.TAG4.TAG5.TAG23 : value9
TAG6 : value10

But, today I have a this case :
<TAG1>
 <TAG2>
  <TAG11>value1</TAG11>
  <TAG12>value2</TAG12>
  <TAG3>
   <TAG131>value3</TAG131>
   <TAG132>value4</TAG132>
   <TAG133>value5</TAG133>
   <TAG134>value6</TAG134>
  </TAG3>
  <TAG3>
   <TAG131>value11</TAG131>
   <TAG132>value12</TAG132>
   <TAG133>value13</TAG133>
   <TAG134>value14</TAG134>
  </TAG3>
 </TAG2>
 <TAG4>
  <TAG5>
   <TAG21>value7</TAG21>
   <TAG22>value8</TAG22>
   <TAG23>value9</TAG23>
  </TAG5>
 </TAG4>
</TAG1>
<TAG6>value10</TAG6>

But the Map object does not allow to store many keys (in the example many TAG3). Have you got an idea how I can resolve this problem ?

EDIT 15/06/2022 :
In fact the expected result needs to keep the original XML structure.
Here the result of last sample :
TAG1.TAG2
    TAG11 : value1
    TAG12 : value2
TAG1.TAG2.TAG3
    TAG131 : value3
    TAG132 : value4
    TAG133 : value5
    TAG134 : value6
TAG1.TAG2.TAG3
    TAG131 : value11
    TAG132 : value12
    TAG133 : value13
    TAG134 : value14
TAG1.TAG4.TAG5
    TAG21 : value7
    TAG22 : value8
    TAG23 : value9
TAG6 : value10

It's to display xml more human reader.

EDIT 04/07/2022 :
I detect a problem of inconsistent with ""new TreeMap<>(Comparator.comparingInt(MyTag::getAppearanceOrder)"". Indeed, some MyTag object are the same AppearanceOrder, so there is a problem of inconsistent ordering. Some value in Map are so removed.
To resolve I used :
map.entrySet().stream().sorted(Map.Entry.comparingByKey(. . .))

And I store the result in Map with collect().
Below the working code :
public class Main {
    public static void main(String[] args) throws ParserConfigurationException, IOException, SAXException {
        //Accessing the xml file
        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
        DocumentBuilder builder = factory.newDocumentBuilder();

        Document document = builder.parse(new File(""data.xml""));

        document.getDocumentElement().normalize();
        Element root = document.getDocumentElement();

        //Retrieving a List of records where each record contains: the original chain of tags, the numbered chain of tags and the value
        String tagSep = ""."";
        List<Record> listRecords = new ArrayList<>();
        visitXMLFile(listRecords, root.getChildNodes(), tagSep, """", """", new HashMap<>());

        //Queue sorted by the numbered tag's length in descending order (from the longest to the shortest)
        PriorityQueue<Record> queue = new PriorityQueue<>(Comparator.comparing(Record::getTagNumberedLen).reversed());
        queue.addAll(listRecords);

        //Using a set to have unique numbered tags (no duplicates) to group by in the resulting map
        Set<MyTag> setMyTags = new HashSet<>();

        //Checking for each numbered tag if its largest substring is equal to any other numbered tag's beginning:
        //  - if it does, then the substring is collected as a key to group by within the final map
        //
        //  - if it doesn't, then another substring is generated from the previous substring until a matching value is found.
        //          If no value is found, then the numbered tag is collected entirely as a key for the resulting map.
        while (!queue.isEmpty()) {
            Record rec = queue.poll();

            //This loop keeps creating substrings of the current numbered tag until:
            //  - the substring matches another numbered tag's beginning
            //  - or no more substrings can be generated
            int lastIndexTagNum = rec.getTagNumbered().lastIndexOf(tagSep);
            int lastIndexTag = rec.getTag().lastIndexOf(tagSep);
            while (lastIndexTagNum > 0) {

                //Checking if the substring matches the beginning of any numbered tag except the current one
                String subStrTagNum = rec.getTagNumbered().substring(0, lastIndexTagNum);
                if (listRecords.stream().anyMatch(r -> !r.getTagNumbered().equals(rec.getTagNumbered()) && r.getTagNumbered().startsWith(subStrTagNum + tagSep))) {

                    String subStrTag = rec.getTag().substring(0, lastIndexTag);
                    int appearanceOrder = listRecords.stream().filter(r -> r.getTagNumbered().startsWith(subStrTagNum + tagSep)).map(r -> r.getAppearanceOrder()).min(Comparator.naturalOrder()).orElse(0);

                    //If a match is found then the current substring is added to the set and the substring iteration is interrupted
                    setMyTags.add(new MyTag(subStrTag, subStrTagNum + tagSep, appearanceOrder));
                    break;
                }

                //Creating a new substring from the previous substring if no match has been found
                lastIndexTagNum = rec.getTagNumbered().substring(0, lastIndexTagNum).lastIndexOf(tagSep);
                lastIndexTag = rec.getTag().substring(0, lastIndexTag).lastIndexOf(tagSep);
            }

            //If no substrings of the current numbered tag matches the beginning of any other numbered tag,
            //then the current numbered tag is collected as a key for the resulting map
            if (lastIndexTagNum < 0) {
                int appearanceOrder = listRecords.stream().filter(r -> r.getTagNumbered().startsWith(rec.getTagNumbered())).map(r -> r.getAppearanceOrder()).min(Comparator.naturalOrder()).orElse(0);
                setMyTags.add(new MyTag(rec.getTag(), rec.getTagNumbered(), appearanceOrder));
            }
        }

        //Creating a temporary resulting map (not sorted as the input)
        Map<MyTag, List<String>> mapTemp = listRecords.stream()
                .collect(Collectors.toMap(
                                rec -> {
                                    //Looking for the longest numbered tag which matches the beginning of the current record's numbered tag.
                                    //The reason why we need the longest match (i.e. the most accurate) is because some elements
                                    //may share the same parents but be on different levels, for example the values 3, 4, 5 and 6
                                    //have a key whose beginning matches both ""TAG1.TAG2"" and ""TAG1.TAG2.TAG3"", but only the longest
                                    //match is actually the right one.
                                    return setMyTags.stream().filter(mt -> rec.getTagNumbered().startsWith(mt.getTagNumbered())).max(Comparator.comparingInt(MyTag::getTagNumberedLen)).orElseThrow(() -> new RuntimeException(""No key found""));
                                },
                                rec -> {
                                    //Retrieving, like above, the numbered tag that will be used to map the current value
                                    MyTag myTag = setMyTags.stream().filter(mt -> rec.getTagNumbered().startsWith(mt.getTagNumbered())).max(Comparator.comparingInt(MyTag::getTagNumberedLen)).orElseThrow(() -> new RuntimeException(""No key found""));

                                    //If the new numbered tag and the record's numbered tag are equal then a List with the current value is returned
                                    if (myTag.getTagNumbered().equals(rec.getTagNumbered())) {
                                        return new ArrayList<>(List.of(rec.getValue()));
                                    } else {    //If the new numbered tag is a substring of the record's numbered tag then the rest of the current (non-numbered) tag is added to the value
                                        return new ArrayList<>(List.of(rec.getTag().substring(myTag.getTag().length() + 1) + "" : "" + rec.getValue()));
                                    }
                                },
                                //Handling colliding cases by merging the lists together
                                (list1, list2) -> {
                                    list1.addAll(list2);
                                    return list1;
                                }
                        )
                );

        //Creating a TreeMap whose ordering is based on the insertion order of the input
        Map<MyTag, List<String>> mapRes =
                mapTemp.entrySet().stream()
                        .sorted(Map.Entry.comparingByKey(Comparator.comparingInt(MyTag::getAppearanceOrder)))
                        .collect(Collectors.toMap(
                                Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));

        //Printing the resulting map
        for (Map.Entry<MyTag, List<String>> entry : mapRes.entrySet()) {
            System.out.println(entry.getKey());
            for (String value : entry.getValue()) {
                System.out.println(""\t"" + value);
            }
        }
    }

    private static void visitXMLFile(List<Record> listInput, NodeList nodeList, String tagSep, String tag, String tagNumbered, Map<String, Integer> mapTagOccurrence) {
        for (int i = 0; i < nodeList.getLength(); i++) {
            Node node = nodeList.item(i);
            if (node.hasChildNodes()) {
                String newTag = tag.isEmpty() ? node.getNodeName() : tag + tagSep + node.getNodeName();

                //Setting or incrementing the number of appearances of a tag chain
                //(sometimes a same chain of tags can be repeated, ex: TAG1.TAG2.TAG3)
                if (!mapTagOccurrence.containsKey(newTag)) {
                    mapTagOccurrence.put(newTag, 1);
                } else {
                    mapTagOccurrence.computeIfPresent(newTag, (key, val) -> val + 1);
                }

                //Creating a numbered version of the tag where its number of appearances is added at the end.
                //This is done to uniquely identify different groups of tag chain when these are repeated (ex: TAG1.TAG2.TAG3)
                String newTagNum = tagNumbered.isEmpty() ? node.getNodeName() + mapTagOccurrence.get(newTag) : tagNumbered + tagSep + node.getNodeName() + mapTagOccurrence.get(newTag);

                visitXMLFile(listInput, node.getChildNodes(), tagSep, newTag, newTagNum, mapTagOccurrence);
            } else {
                if (!node.getTextContent().trim().equals("""")) {
                    int appearanceOrder = listInput.size() + 1;
                    listInput.add(new Record(tag, tagNumbered, node.getTextContent().trim(), appearanceOrder));
                }
            }
        }
    }
}

class MyTag {

    //Tag chain for the user
    private String tag;

    //Unique tag chain for identification
    private String tagNumbered;

    private int appearanceOrder;

    public MyTag(String tag, String tagNumbered, int appearanceOrder) {
        this.tag = tag;
        this.tagNumbered = tagNumbered;
        this.appearanceOrder = appearanceOrder;
    }

    public String getTag() {
        return tag;
    }

    public String getTagNumbered() {
        return tagNumbered;
    }

    public int getTagNumberedLen() {
        return tagNumbered == null ? 0 : tagNumbered.length();
    }

    public int getAppearanceOrder() {
        return appearanceOrder;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        MyTag tagPair = (MyTag) o;
        return Objects.equals(tagNumbered, tagPair.tagNumbered);
    }

    @Override
    public int hashCode() {
        return Objects.hash(tagNumbered);
    }

    @Override
    public String toString() {
        return tag;
    }
}

class Record {

    //Tag chain for the user
    private String tag;

    //Unique tag chain for identification
    private String tagNumbered;

    private String value;

    private int appearanceOrder;

    public Record(String tag, String tagNumbered, String value, int appearanceOrder) {
        this.tag = tag;
        this.tagNumbered = tagNumbered;
        this.value = value;
        this.appearanceOrder = appearanceOrder;
    }

    public String getTag() {
        return tag;
    }

    public String getTagNumbered() {
        return tagNumbered;
    }

    public int getTagNumberedLen() {
        return tagNumbered == null ? 0 : tagNumbered.length();
    }

    public String getValue() {
        return value;
    }

    public int getAppearanceOrder() {
        return appearanceOrder;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        Record record = (Record) o;
        return Objects.equals(tagNumbered, record.tagNumbered);
    }

    @Override
    public int hashCode() {
        return Objects.hash(tagNumbered);
    }

    @Override
    public String toString() {
        return tag + "" - "" + tagNumbered + "" - "" + value;
    }
}

","Display HashMap content in tree view
I need your help because I don't find the solution in Java for my problem.
I stored in object LinkedHashMap<String, String> this content:
TAG1.TAG2.TAG11 : value1
TAG1.TAG2.TAG12 : value2
TAG1.TAG2.TAG3.TAG131 : value3
TAG1.TAG2.TAG3.TAG132 : value4
TAG1.TAG2.TAG3.TAG133 : value5
TAG1.TAG2.TAG3.TAG134 : value6
TAG1.TAG4.TAG5.TAG21 : value7
TAG1.TAG4.TAG5.TAG22 : value8
TAG1.TAG4.TAG5.TAG23 : value9
TAG6 : value10

I need to display if a tag has 2 or more children, the list of child.
Here is the expected result:
TAG1.TAG2
    TAG11 : value1
    TAG12 : value2
TAG1.TAG2.TAG3
    TAG131 : value3
    TAG132 : value4
    TAG133 : value5
    TAG134 : value6
TAG1.TAG4.TAG5
    TAG21 : value7
    TAG22 : value8
    TAG23 : value9
TAG6 : value10


EDIT 14/06/2022 :
In fact, my original analyse is bad because initialy I have a XML file :
<TAG1>
 <TAG2>
  <TAG11>value1</TAG11>
  <TAG12>value2</TAG12>
  <TAG3>
   <TAG131>value3</TAG131>
   <TAG132>value4</TAG132>
   <TAG133>value5</TAG133>
   <TAG134>value6</TAG134>
  </TAG3>
 </TAG2>
 <TAG4>
  <TAG5>
   <TAG21>value7</TAG21>
   <TAG22>value8</TAG22>
   <TAG23>value9</TAG23>
  </TAG5>
 </TAG4>
</TAG1>
<TAG6>value10</TAG6>

And I created a map to store it :
TAG1.TAG2.TAG11 : value1
TAG1.TAG2.TAG12 : value2
TAG1.TAG2.TAG3.TAG131 : value3
TAG1.TAG2.TAG3.TAG132 : value4
TAG1.TAG2.TAG3.TAG133 : value5
TAG1.TAG2.TAG3.TAG134 : value6
TAG1.TAG4.TAG5.TAG21 : value7
TAG1.TAG4.TAG5.TAG22 : value8
TAG1.TAG4.TAG5.TAG23 : value9
TAG6 : value10

But, today I have a this case :
<TAG1>
 <TAG2>
  <TAG11>value1</TAG11>
  <TAG12>value2</TAG12>
  <TAG3>
   <TAG131>value3</TAG131>
   <TAG132>value4</TAG132>
   <TAG133>value5</TAG133>
   <TAG134>value6</TAG134>
  </TAG3>
  <TAG3>
   <TAG131>value11</TAG131>
   <TAG132>value12</TAG132>
   <TAG133>value13</TAG133>
   <TAG134>value14</TAG134>
  </TAG3>
 </TAG2>
 <TAG4>
  <TAG5>
   <TAG21>value7</TAG21>
   <TAG22>value8</TAG22>
   <TAG23>value9</TAG23>
  </TAG5>
 </TAG4>
</TAG1>
<TAG6>value10</TAG6>

But the Map object does not allow to store many keys (in the example many TAG3). Have you got an idea how I can resolve this problem ?

EDIT 15/06/2022 :
In fact the expected result needs to keep the original XML structure.
Here the result of last sample :
TAG1.TAG2
    TAG11 : value1
    TAG12 : value2
TAG1.TAG2.TAG3
    TAG131 : value3
    TAG132 : value4
    TAG133 : value5
    TAG134 : value6
TAG1.TAG2.TAG3
    TAG131 : value11
    TAG132 : value12
    TAG133 : value13
    TAG134 : value14
TAG1.TAG4.TAG5
    TAG21 : value7
    TAG22 : value8
    TAG23 : value9
TAG6 : value10

It's to display xml more human reader.

EDIT 04/07/2022 :
I detect a problem of inconsistent with ""new TreeMap<>(Comparator.comparingInt(MyTag::getAppearanceOrder)"". Indeed, some MyTag object are the same AppearanceOrder, so there is a problem of inconsistent ordering. Some value in Map are so removed.
To resolve I used :
map.entrySet().stream().sorted(Map.Entry.comparingByKey(. . .))

And I store the result in Map with collect().
Below the working code :
public class Main {
    public static void main(String[] args) throws ParserConfigurationException, IOException, SAXException {
        //Accessing the xml file
        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
        DocumentBuilder builder = factory.newDocumentBuilder();

        Document document = builder.parse(new File(""data.xml""));

        document.getDocumentElement().normalize();
        Element root = document.getDocumentElement();

        //Retrieving a List of records where each record contains: the original chain of tags, the numbered chain of tags and the value
        String tagSep = ""."";
        List<Record> listRecords = new ArrayList<>();
        visitXMLFile(listRecords, root.getChildNodes(), tagSep, """", """", new HashMap<>());

        //Queue sorted by the numbered tag's length in descending order (from the longest to the shortest)
        PriorityQueue<Record> queue = new PriorityQueue<>(Comparator.comparing(Record::getTagNumberedLen).reversed());
        queue.addAll(listRecords);

        //Using a set to have unique numbered tags (no duplicates) to group by in the resulting map
        Set<MyTag> setMyTags = new HashSet<>();

        //Checking for each numbered tag if its largest substring is equal to any other numbered tag's beginning:
        //  - if it does, then the substring is collected as a key to group by within the final map
        //
        //  - if it doesn't, then another substring is generated from the previous substring until a matching value is found.
        //          If no value is found, then the numbered tag is collected entirely as a key for the resulting map.
        while (!queue.isEmpty()) {
            Record rec = queue.poll();

            //This loop keeps creating substrings of the current numbered tag until:
            //  - the substring matches another numbered tag's beginning
            //  - or no more substrings can be generated
            int lastIndexTagNum = rec.getTagNumbered().lastIndexOf(tagSep);
            int lastIndexTag = rec.getTag().lastIndexOf(tagSep);
            while (lastIndexTagNum > 0) {

                //Checking if the substring matches the beginning of any numbered tag except the current one
                String subStrTagNum = rec.getTagNumbered().substring(0, lastIndexTagNum);
                if (listRecords.stream().anyMatch(r -> !r.getTagNumbered().equals(rec.getTagNumbered()) && r.getTagNumbered().startsWith(subStrTagNum + tagSep))) {

                    String subStrTag = rec.getTag().substring(0, lastIndexTag);
                    int appearanceOrder = listRecords.stream().filter(r -> r.getTagNumbered().startsWith(subStrTagNum + tagSep)).map(r -> r.getAppearanceOrder()).min(Comparator.naturalOrder()).orElse(0);

                    //If a match is found then the current substring is added to the set and the substring iteration is interrupted
                    setMyTags.add(new MyTag(subStrTag, subStrTagNum + tagSep, appearanceOrder));
                    break;
                }

                //Creating a new substring from the previous substring if no match has been found
                lastIndexTagNum = rec.getTagNumbered().substring(0, lastIndexTagNum).lastIndexOf(tagSep);
                lastIndexTag = rec.getTag().substring(0, lastIndexTag).lastIndexOf(tagSep);
            }

            //If no substrings of the current numbered tag matches the beginning of any other numbered tag,
            //then the current numbered tag is collected as a key for the resulting map
            if (lastIndexTagNum < 0) {
                int appearanceOrder = listRecords.stream().filter(r -> r.getTagNumbered().startsWith(rec.getTagNumbered())).map(r -> r.getAppearanceOrder()).min(Comparator.naturalOrder()).orElse(0);
                setMyTags.add(new MyTag(rec.getTag(), rec.getTagNumbered(), appearanceOrder));
            }
        }

        //Creating a temporary resulting map (not sorted as the input)
        Map<MyTag, List<String>> mapTemp = listRecords.stream()
                .collect(Collectors.toMap(
                                rec -> {
                                    //Looking for the longest numbered tag which matches the beginning of the current record's numbered tag.
                                    //The reason why we need the longest match (i.e. the most accurate) is because some elements
                                    //may share the same parents but be on different levels, for example the values 3, 4, 5 and 6
                                    //have a key whose beginning matches both ""TAG1.TAG2"" and ""TAG1.TAG2.TAG3"", but only the longest
                                    //match is actually the right one.
                                    return setMyTags.stream().filter(mt -> rec.getTagNumbered().startsWith(mt.getTagNumbered())).max(Comparator.comparingInt(MyTag::getTagNumberedLen)).orElseThrow(() -> new RuntimeException(""No key found""));
                                },
                                rec -> {
                                    //Retrieving, like above, the numbered tag that will be used to map the current value
                                    MyTag myTag = setMyTags.stream().filter(mt -> rec.getTagNumbered().startsWith(mt.getTagNumbered())).max(Comparator.comparingInt(MyTag::getTagNumberedLen)).orElseThrow(() -> new RuntimeException(""No key found""));

                                    //If the new numbered tag and the record's numbered tag are equal then a List with the current value is returned
                                    if (myTag.getTagNumbered().equals(rec.getTagNumbered())) {
                                        return new ArrayList<>(List.of(rec.getValue()));
                                    } else {    //If the new numbered tag is a substring of the record's numbered tag then the rest of the current (non-numbered) tag is added to the value
                                        return new ArrayList<>(List.of(rec.getTag().substring(myTag.getTag().length() + 1) + "" : "" + rec.getValue()));
                                    }
                                },
                                //Handling colliding cases by merging the lists together
                                (list1, list2) -> {
                                    list1.addAll(list2);
                                    return list1;
                                }
                        )
                );

        //Creating a TreeMap whose ordering is based on the insertion order of the input
        Map<MyTag, List<String>> mapRes =
                mapTemp.entrySet().stream()
                        .sorted(Map.Entry.comparingByKey(Comparator.comparingInt(MyTag::getAppearanceOrder)))
                        .collect(Collectors.toMap(
                                Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));

        //Printing the resulting map
        for (Map.Entry<MyTag, List<String>> entry : mapRes.entrySet()) {
            System.out.println(entry.getKey());
            for (String value : entry.getValue()) {
                System.out.println(""\t"" + value);
            }
        }
    }

    private static void visitXMLFile(List<Record> listInput, NodeList nodeList, String tagSep, String tag, String tagNumbered, Map<String, Integer> mapTagOccurrence) {
        for (int i = 0; i < nodeList.getLength(); i++) {
            Node node = nodeList.item(i);
            if (node.hasChildNodes()) {
                String newTag = tag.isEmpty() ? node.getNodeName() : tag + tagSep + node.getNodeName();

                //Setting or incrementing the number of appearances of a tag chain
                //(sometimes a same chain of tags can be repeated, ex: TAG1.TAG2.TAG3)
                if (!mapTagOccurrence.containsKey(newTag)) {
                    mapTagOccurrence.put(newTag, 1);
                } else {
                    mapTagOccurrence.computeIfPresent(newTag, (key, val) -> val + 1);
                }

                //Creating a numbered version of the tag where its number of appearances is added at the end.
                //This is done to uniquely identify different groups of tag chain when these are repeated (ex: TAG1.TAG2.TAG3)
                String newTagNum = tagNumbered.isEmpty() ? node.getNodeName() + mapTagOccurrence.get(newTag) : tagNumbered + tagSep + node.getNodeName() + mapTagOccurrence.get(newTag);

                visitXMLFile(listInput, node.getChildNodes(), tagSep, newTag, newTagNum, mapTagOccurrence);
            } else {
                if (!node.getTextContent().trim().equals("""")) {
                    int appearanceOrder = listInput.size() + 1;
                    listInput.add(new Record(tag, tagNumbered, node.getTextContent().trim(), appearanceOrder));
                }
            }
        }
    }
}

class MyTag {

    //Tag chain for the user
    private String tag;

    //Unique tag chain for identification
    private String tagNumbered;

    private int appearanceOrder;

    public MyTag(String tag, String tagNumbered, int appearanceOrder) {
        this.tag = tag;
        this.tagNumbered = tagNumbered;
        this.appearanceOrder = appearanceOrder;
    }

    public String getTag() {
        return tag;
    }

    public String getTagNumbered() {
        return tagNumbered;
    }

    public int getTagNumberedLen() {
        return tagNumbered == null ? 0 : tagNumbered.length();
    }

    public int getAppearanceOrder() {
        return appearanceOrder;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        MyTag tagPair = (MyTag) o;
        return Objects.equals(tagNumbered, tagPair.tagNumbered);
    }

    @Override
    public int hashCode() {
        return Objects.hash(tagNumbered);
    }

    @Override
    public String toString() {
        return tag;
    }
}

class Record {

    //Tag chain for the user
    private String tag;

    //Unique tag chain for identification
    private String tagNumbered;

    private String value;

    private int appearanceOrder;

    public Record(String tag, String tagNumbered, String value, int appearanceOrder) {
        this.tag = tag;
        this.tagNumbered = tagNumbered;
        this.value = value;
        this.appearanceOrder = appearanceOrder;
    }

    public String getTag() {
        return tag;
    }

    public String getTagNumbered() {
        return tagNumbered;
    }

    public int getTagNumberedLen() {
        return tagNumbered == null ? 0 : tagNumbered.length();
    }

    public String getValue() {
        return value;
    }

    public int getAppearanceOrder() {
        return appearanceOrder;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        Record record = (Record) o;
        return Objects.equals(tagNumbered, record.tagNumbered);
    }

    @Override
    public int hashCode() {
        return Objects.hash(tagNumbered);
    }

    @Override
    public String toString() {
        return tag + "" - "" + tagNumbered + "" - "" + value;
    }
}

",1,1,0,0,0,0,0,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
32909767,32909767,Allocating on heap vs allocating on stack in recursive functions,"<p>When I'm defining a recursive  function, is it better/safer to allocate local variables on heap, and then clean them up before the function returns, than to allocate them on stack. The stack size on embedded systems is very limited, and there is a danger of stack overflow, when the recursion runs too deep.</p>
","when im defining a recursive  function is it bettersafer to allocate local variables on heap and then clean them up before the function returns than to allocate them on stack the stack size on embedded systems is very limited and there is a danger of stack overflow when the recursion runs too deep
",1,"When I'm defining a recursive  function, is it better/safer to allocate local variables on heap, and then clean them up before the function returns, than to allocate them on stack. The stack size on embedded systems is very limited, and there is a danger of stack overflow, when the recursion runs too deep.
","Allocating on heap vs allocating on stack in recursive functions
When I'm defining a recursive  function, is it better/safer to allocate local variables on heap, and then clean them up before the function returns, than to allocate them on stack. The stack size on embedded systems is very limited, and there is a danger of stack overflow, when the recursion runs too deep.
",0,0,1,1,0,0,0,- Is a theoretical question,"Label: 1

Reasoning:

This question expresses technical debt because it implies a concern about the performance and reliability of the code, specifically with regards to memory management in a recursive function. The user is asking about the best approach to allocate local variables in a recursive function to avoid stack overflow, which is a common issue in embedded systems with limited stack size.

The question reveals a potential design flaw in the current implementation, as the user is aware of the risk of stack overflow when the recursion runs too deep. This indicates that the code may not be optimal and may require refactoring to improve its reliability and performance.

The user is seeking a safer approach to allocate local variables, which implies that the current implementation may be prone to errors or crashes due to stack overflow. This concern about memory management and the desire to avoid stack overflow reveals a technical debt that needs to be addressed to ensure the code is reliable and efficient.

Additionally, the fact that the user is considering the trade-offs between allocating variables on the heap versus the stack suggests that they are aware of the limitations of their current implementation and are looking for a better approach to manage memory. This awareness of potential issues and the desire to improve the code's performance and reliability further supports the notion that this question expresses technical debt.",Yes,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,1,0,0,,,Infrastructure,Infrastructure,,Infrastructure,Moderate,Moderate,,Moderate
28259636,28259636,"Is this a bug in Files.lines(), or am I misunderstanding something about parallel streams?","<p>Environment: Ubuntu x86_64 (14.10), Oracle JDK 1.8u25</p>

<p>I try and use a parallel stream of <a href=""http://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#lines-java.nio.file.Path-java.nio.charset.Charset-"" rel=""nofollow noreferrer""><code>Files.lines()</code></a> but I want to <a href=""http://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#skip-long-"" rel=""nofollow noreferrer""><code>.skip()</code></a> the first line (it's a CSV file with a header). Therefore I try and do this:</p>

<pre><code>try (
    final Stream&lt;String&gt; stream = Files.lines(thePath, StandardCharsets.UTF_8)
        .skip(1L).parallel();
) {
    // etc
}
</code></pre>

<p>But then one column failed to parse to an int...</p>

<p>So I tried some simple code. The file is question is dead simple:</p>

<pre><code>$ cat info.csv 
startDate;treeDepth;nrMatchers;nrLines;nrChars;nrCodePoints;nrNodes
1422758875023;34;54;151;4375;4375;27486
$
</code></pre>

<p>And the code is equally simple:</p>

<pre><code>public static void main(final String... args)
{
    final Path path = Paths.get(""/home/fge/tmp/dd/info.csv"");
    Files.lines(path, StandardCharsets.UTF_8).skip(1L).parallel()
        .forEach(System.out::println);
}
</code></pre>

<p>And I <em>systematically</em> get the following result (OK, I have only run it something around 20 times):</p>

<pre><code>startDate;treeDepth;nrMatchers;nrLines;nrChars;nrCodePoints;nrNodes
</code></pre>

<p>What am I missing here?</p>

<hr>

<p><strong>EDIT</strong> It seems like the problem, or misunderstanding, is much more rooted than that (the two examples below were cooked up by a fellow on FreeNode's ##java):</p>

<pre><code>public static void main(final String... args)
{
    new BufferedReader(new StringReader(""Hello\nWorld"")).lines()
        .skip(1L).parallel()
        .forEach(System.out::println);

    final Iterator&lt;String&gt; iter
        = Arrays.asList(""Hello"", ""World"").iterator();
    final Spliterator&lt;String&gt; spliterator
        = Spliterators.spliteratorUnknownSize(iter, Spliterator.ORDERED);
    final Stream&lt;String&gt; s
        = StreamSupport.stream(spliterator, true);

    s.skip(1L).forEach(System.out::println);
}
</code></pre>

<p>This prints:</p>

<pre><code>Hello
Hello
</code></pre>

<p>Uh.</p>

<p>@Holger suggested that this happens for any stream which is <code>ORDERED</code> and not <code>SIZED</code> with this other sample:</p>

<pre><code>Stream.of(""Hello"", ""World"")
    .filter(x -&gt; true)
    .parallel()
    .skip(1L)
    .forEach(System.out::println);
</code></pre>

<p>Also, it stems from all the discussion which already took place that the problem (if it is one?) is with <code>.forEach()</code> (as <a href=""https://stackoverflow.com/questions/28259636/is-this-a-bug-in-files-lines-or-am-i-misunderstanding-something-about-paralle#comment44876969_28259636"">@SotiriosDelimanolis first pointed out</a>).</p>
","environment ubuntu x86_64 1410 oracle jdk 18u25
i try and use a parallel stream of fileslines but i want to skip the first line its a csv file with a header therefore i try and do this
but then one column failed to parse to an int
so i tried some simple code the file is question is dead simple
and the code is equally simple
and i systematically get the following result ok i have only run it something around 20 times
what am i missing here

edit it seems like the problem or misunderstanding is much more rooted than that the two examples below were cooked up by a fellow on freenodes java
this prints
uh
holger suggested that this happens for any stream which is ordered and not sized with this other sample
also it stems from all the discussion which already took place that the problem if it is one is with foreach as sotiriosdelimanolis first pointed out
",1,"Environment: Ubuntu x86_64 (14.10), Oracle JDK 1.8u25
I try and use a parallel stream of Files.lines() but I want to .skip() the first line (it's a CSV file with a header). Therefore I try and do this:
try (
    final Stream<String> stream = Files.lines(thePath, StandardCharsets.UTF_8)
        .skip(1L).parallel();
) {
    // etc
}

But then one column failed to parse to an int...
So I tried some simple code. The file is question is dead simple:
$ cat info.csv 
startDate;treeDepth;nrMatchers;nrLines;nrChars;nrCodePoints;nrNodes
1422758875023;34;54;151;4375;4375;27486
$

And the code is equally simple:
public static void main(final String... args)
{
    final Path path = Paths.get(""/home/fge/tmp/dd/info.csv"");
    Files.lines(path, StandardCharsets.UTF_8).skip(1L).parallel()
        .forEach(System.out::println);
}

And I systematically get the following result (OK, I have only run it something around 20 times):
startDate;treeDepth;nrMatchers;nrLines;nrChars;nrCodePoints;nrNodes

What am I missing here?

EDIT It seems like the problem, or misunderstanding, is much more rooted than that (the two examples below were cooked up by a fellow on FreeNode's ##java):
public static void main(final String... args)
{
    new BufferedReader(new StringReader(""Hello\nWorld"")).lines()
        .skip(1L).parallel()
        .forEach(System.out::println);

    final Iterator<String> iter
        = Arrays.asList(""Hello"", ""World"").iterator();
    final Spliterator<String> spliterator
        = Spliterators.spliteratorUnknownSize(iter, Spliterator.ORDERED);
    final Stream<String> s
        = StreamSupport.stream(spliterator, true);

    s.skip(1L).forEach(System.out::println);
}

This prints:
Hello
Hello

Uh.
@Holger suggested that this happens for any stream which is ORDERED and not SIZED with this other sample:
Stream.of(""Hello"", ""World"")
    .filter(x -> true)
    .parallel()
    .skip(1L)
    .forEach(System.out::println);

Also, it stems from all the discussion which already took place that the problem (if it is one?) is with .forEach() (as @SotiriosDelimanolis first pointed out).
","Is this a bug in Files.lines(), or am I misunderstanding something about parallel streams?
Environment: Ubuntu x86_64 (14.10), Oracle JDK 1.8u25
I try and use a parallel stream of Files.lines() but I want to .skip() the first line (it's a CSV file with a header). Therefore I try and do this:
try (
    final Stream<String> stream = Files.lines(thePath, StandardCharsets.UTF_8)
        .skip(1L).parallel();
) {
    // etc
}

But then one column failed to parse to an int...
So I tried some simple code. The file is question is dead simple:
$ cat info.csv 
startDate;treeDepth;nrMatchers;nrLines;nrChars;nrCodePoints;nrNodes
1422758875023;34;54;151;4375;4375;27486
$

And the code is equally simple:
public static void main(final String... args)
{
    final Path path = Paths.get(""/home/fge/tmp/dd/info.csv"");
    Files.lines(path, StandardCharsets.UTF_8).skip(1L).parallel()
        .forEach(System.out::println);
}

And I systematically get the following result (OK, I have only run it something around 20 times):
startDate;treeDepth;nrMatchers;nrLines;nrChars;nrCodePoints;nrNodes

What am I missing here?

EDIT It seems like the problem, or misunderstanding, is much more rooted than that (the two examples below were cooked up by a fellow on FreeNode's ##java):
public static void main(final String... args)
{
    new BufferedReader(new StringReader(""Hello\nWorld"")).lines()
        .skip(1L).parallel()
        .forEach(System.out::println);

    final Iterator<String> iter
        = Arrays.asList(""Hello"", ""World"").iterator();
    final Spliterator<String> spliterator
        = Spliterators.spliteratorUnknownSize(iter, Spliterator.ORDERED);
    final Stream<String> s
        = StreamSupport.stream(spliterator, true);

    s.skip(1L).forEach(System.out::println);
}

This prints:
Hello
Hello

Uh.
@Holger suggested that this happens for any stream which is ORDERED and not SIZED with this other sample:
Stream.of(""Hello"", ""World"")
    .filter(x -> true)
    .parallel()
    .skip(1L)
    .forEach(System.out::println);

Also, it stems from all the discussion which already took place that the problem (if it is one?) is with .forEach() (as @SotiriosDelimanolis first pointed out).
",1,0,1,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,1,0,1,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
41229411,41229411,How to Optimize execution time for RGB to HSL conversion function?,"<p>I've created this function to convert RGB color to HSL color. It works perfect.</p>

<p>But I need to make it run faster, because it's used to replace colors on a canvas, and I need to reduce the time of replacement. Since the image contains 360k pixels (600x600px) anything can make it faster.</p>

<p>That's my current implementation:</p>

<pre><code>/**
 * Convert RGB Color to HSL Color
 * @param {{R: integer, G: integer, B: integer}} rgb
 * @returns {{H: number, S: number, L: number}}
 */
Colorize.prototype.rgbToHsl = function(rgb) {
    var R = rgb.R/255;
    var G = rgb.G/255;
    var B = rgb.B/255;
    var Cmax = Math.max(R,G,B);
    var Cmin = Math.min(R,G,B);
    var delta = Cmax - Cmin;
    var L = (Cmax + Cmin) / 2;
    var S = 0;
    var H = 0;

    if (delta !== 0) {
        S = delta / (1 - Math.abs((2*L) - 1));

        switch (Cmax) {
            case R:
                H = ((G - B) / delta) % 6;
                break;
            case G:
                H = ((B - R) / delta) + 2;
                break;
            case B:
                H = ((R - G) / delta) + 4;
                break;
        }
        H *= 60;
    }

    // Convert negative angles from Hue
    while (H &lt; 0) {
        H += 360;
    }

    return {
        H: H,
        S: S,
        L: L
    };
};
</code></pre>
","ive created this function to convert rgb color to hsl color it works perfect
but i need to make it run faster because its used to replace colors on a canvas and i need to reduce the time of replacement since the image contains 360k pixels 600x600px anything can make it faster
thats my current implementation
",0,"I've created this function to convert RGB color to HSL color. It works perfect.
But I need to make it run faster, because it's used to replace colors on a canvas, and I need to reduce the time of replacement. Since the image contains 360k pixels (600x600px) anything can make it faster.
That's my current implementation:
/**
 * Convert RGB Color to HSL Color
 * @param {{R: integer, G: integer, B: integer}} rgb
 * @returns {{H: number, S: number, L: number}}
 */
Colorize.prototype.rgbToHsl = function(rgb) {
    var R = rgb.R/255;
    var G = rgb.G/255;
    var B = rgb.B/255;
    var Cmax = Math.max(R,G,B);
    var Cmin = Math.min(R,G,B);
    var delta = Cmax - Cmin;
    var L = (Cmax + Cmin) / 2;
    var S = 0;
    var H = 0;

    if (delta !== 0) {
        S = delta / (1 - Math.abs((2*L) - 1));

        switch (Cmax) {
            case R:
                H = ((G - B) / delta) % 6;
                break;
            case G:
                H = ((B - R) / delta) + 2;
                break;
            case B:
                H = ((R - G) / delta) + 4;
                break;
        }
        H *= 60;
    }

    // Convert negative angles from Hue
    while (H < 0) {
        H += 360;
    }

    return {
        H: H,
        S: S,
        L: L
    };
};

","How to Optimize execution time for RGB to HSL conversion function?
I've created this function to convert RGB color to HSL color. It works perfect.
But I need to make it run faster, because it's used to replace colors on a canvas, and I need to reduce the time of replacement. Since the image contains 360k pixels (600x600px) anything can make it faster.
That's my current implementation:
/**
 * Convert RGB Color to HSL Color
 * @param {{R: integer, G: integer, B: integer}} rgb
 * @returns {{H: number, S: number, L: number}}
 */
Colorize.prototype.rgbToHsl = function(rgb) {
    var R = rgb.R/255;
    var G = rgb.G/255;
    var B = rgb.B/255;
    var Cmax = Math.max(R,G,B);
    var Cmin = Math.min(R,G,B);
    var delta = Cmax - Cmin;
    var L = (Cmax + Cmin) / 2;
    var S = 0;
    var H = 0;

    if (delta !== 0) {
        S = delta / (1 - Math.abs((2*L) - 1));

        switch (Cmax) {
            case R:
                H = ((G - B) / delta) % 6;
                break;
            case G:
                H = ((B - R) / delta) + 2;
                break;
            case B:
                H = ((R - G) / delta) + 4;
                break;
        }
        H *= 60;
    }

    // Convert negative angles from Hue
    while (H < 0) {
        H += 360;
    }

    return {
        H: H,
        S: S,
        L: L
    };
};

",1,0,0,0,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,0,0,0,"Change: Implementation to 1, debugging to 0
Reason: User has a working algorithm but wants it to run faster",,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
6456627,6456627,django templates performance,"<p>Which is more efficient in Django: to render an object using the template language? or to push the code to render the object via a templatetag function using the python language?</p>
","which is more efficient in django to render an object using the template language or to push the code to render the object via a templatetag function using the python language
",1,"Which is more efficient in Django: to render an object using the template language? or to push the code to render the object via a templatetag function using the python language?
","django templates performance
Which is more efficient in Django: to render an object using the template language? or to push the code to render the object via a templatetag function using the python language?
",0,1,0,1,0,0,0,- Is a question comparing approaches,"I would label this question as 1, as it expresses technical debt.

The reasoning behind my decision is that the question is asking about efficiency in Django, which implies that the current implementation might be suboptimal and requires improvement. The user is considering two different approaches (template language vs. templatetag function using Python language) to render an object, which suggests that they are aware of potential performance issues or design flaws in their current code.

The fact that they are asking which approach is more efficient implies that they are concerned about the scalability or performance of their application, which is a key indicator of technical debt. The user is seeking a better solution to avoid potential bottlenecks or slowdowns, which indicates that their current implementation may not be optimal.

Furthermore, the question implies a desire to improve the design or architecture of their code, which is another hallmark of technical debt. By asking about the most efficient approach, the user is acknowledging that their current code may not be meeting their requirements, and they need to refactor or optimize it to achieve better performance.

Overall, the question expresses technical debt because it reveals a concern about the efficiency and performance of the code, which is a key aspect of technical debt.",Yes,1,1,Web Performance,Web Performance,,Web Performance,0,1,0,1,0,0,,,Code,Code,,Code,Very Mild,Very Mild,,Very Mild
49091126,49091126,Ensure long-term valid data when passed by reference? How to load my struct with pointer without knowing how its memory was allocated?,"<p>I am surprised that I didn't find the question already posted.  <a href=""https://stackoverflow.com/questions/14857246/when-a-pointer-is-created-in-scope-what-happens-to-the-pointed-to-variable-when?noredirect=1&amp;lq=1"">Here</a> is one that is similar but not what I'm getting at because it only talks about string literals.</p>

<p>Anyway, coming from Java and learning C a bit better.  I love the options that C brings but I know I've got to be careful with memory management.</p>

<p>I have created my own timer utility as an exercise to get familiar with how to do things in C and came across this question... Here's a code sample first (Windows console app):</p>

<pre><code>HANDLE createHPTimer(char *timerName, DWORD duration, UINT16 repeat, bool persist, void *callbackFunc, void *callbackArgs) {

    //...
    //...  (here I ensure struct hptGlobals is properly initialized and that *timerName does not already belong to the collection)
    //...

    HANDLE hHPTimer;
    hHPTimer = malloc(sizeof(HPTimerObj));
    if (hHPTimer != INVALID_HANDLE_VALUE) {
        if (hptGlobals.timerCnt &gt;= hptGlobals.allocCnt) {
            //reallocate more room to global collection to fit this new timer
            hptGlobals.hCollection = (HANDLE*)realloc(hptGlobals.hCollection, sizeof(HANDLE)*(hptGlobals.allocCnt + hpt_ALLOC_RES));
        }
        //add new timer handle to global collection
        hptGlobals.hCollection[hptGlobals.timerCnt] = hHPTimer;

        //here I assign function parameters into the structure for later use
        //timerName is a c string therefore I am only saving the pointer reference
        ((HPTimerObj*)hHPTimer)-&gt;timerName = timerName;
        ((HPTimerObj*)hHPTimer)-&gt;status = hpt_INIT_NOT_STARTED;     //timer is initialized in the not started state
        ((HPTimerObj*)hHPTimer)-&gt;duration = duration;
        ((HPTimerObj*)hHPTimer)-&gt;repeat = repeat;
        ((HPTimerObj*)hHPTimer)-&gt;runCnt = 0;
        ((HPTimerObj*)hHPTimer)-&gt;persist = persist;
        ((HPTimerObj*)hHPTimer)-&gt;callbackFunc = callbackFunc;
        ((HPTimerObj*)hHPTimer)-&gt;callbackArgs = callbackArgs;

        hptGlobals.timerCnt++;
        return hHPTimer;
    } else 
        return INVALID_HANDLE_VALUE;    //return invalid handle if HPTimer creation failed
}
</code></pre>

<p>My HPTimerObj struct looks like this:</p>

<pre><code>typedef struct {
    char *timerName;
    BOOL status;    //uses the following flags: hpt_INIT_NOT_STARTED, hpt_INIT_STARTED, hpt_INIT_PAUSED
    UINT64 timeData;//if the timer is currently running, this is the timestamp of the next scheduled callback; if the timer is paused, this is the elapsed duration when paused
    DWORD duration; 
    UINT16 repeat;  //number of times to run timer before it frees itself (use persist flag to keep timer (pause it) after runCount has been reached)
    UINT16 runCnt;  //number of times timer has triggered
    bool persist;
    void *callbackFunc;
    void *callbackArgs;
} HPTimerObj;
</code></pre>

<p>So I have three pointers that were passed into the function and I need to reference their data later: timerName, callbackFunc, and callbackArgs</p>

<p>This code works (for my test case) as written but I'm not sure it is safe because I do not know how to determine how memory for the pointer timerName was allocated -- is it in the stack or on the heap?  Is it static or a string literal (which I think it would always be valid and safe).  If this function is called from code where timerName was allocated on the stack then there is a possibility that it could be overwritten later.  Am I correct in this thinking?</p>

<p>If I could always know how the memory was allocated then I could copy it from stack to heap if needed but I don't think that is possible (see <a href=""https://stackoverflow.com/questions/3230420/how-to-know-if-a-pointer-points-to-the-heap-or-the-stack"">this SO question</a>).  I think that the other two pointer arguments are always safe because one is a function pointer (I would assume would be global/static) and the other is supposed to be a pointer to a struct which must be on the heap, is that right?</p>

<p>To ensure that timerName always has valid data I could create a copy on the heap as follows:</p>

<pre><code>//instead, to ensure timerName data is on the heap, copy it there
size_t timerNameLen;
timerNameLen = strlen(timerName);
char *newTimerName = (char*)malloc(sizeof(char)*(timerNameLen + 1));    //Must use free on ((HPTimerObj*)hHPTimer)-&gt;timerName when ""killing"" the timer
strcpy_s(newTimerName, timerNameLen + 1, timerName);
((HPTimerObj*)hHPTimer)-&gt;timerName = newTimerName;
</code></pre>

<p>Should I make this a habit in other similar situations or was it okay the way I first wrote it?  I'd like to avoid duplicating values which may take extra time and memory if I don't need to.  Are there any better approaches for this?  Thanks!</p>
","i am surprised that i didnt find the question already posted  here is one that is similar but not what im getting at because it only talks about string literals
anyway coming from java and learning c a bit better  i love the options that c brings but i know ive got to be careful with memory management
i have created my own timer utility as an exercise to get familiar with how to do things in c and came across this question heres a code sample first windows console app
my hptimerobj struct looks like this
so i have three pointers that were passed into the function and i need to reference their data later timername callbackfunc and callbackargs
this code works for my test case as written but im not sure it is safe because i do not know how to determine how memory for the pointer timername was allocated  is it in the stack or on the heap  is it static or a string literal which i think it would always be valid and safe  if this function is called from code where timername was allocated on the stack then there is a possibility that it could be overwritten later  am i correct in this thinking
if i could always know how the memory was allocated then i could copy it from stack to heap if needed but i dont think that is possible see this so question  i think that the other two pointer arguments are always safe because one is a function pointer i would assume would be globalstatic and the other is supposed to be a pointer to a struct which must be on the heap is that right
to ensure that timername always has valid data i could create a copy on the heap as follows
should i make this a habit in other similar situations or was it okay the way i first wrote it  id like to avoid duplicating values which may take extra time and memory if i dont need to  are there any better approaches for this  thanks
",1,"I am surprised that I didn't find the question already posted.  Here is one that is similar but not what I'm getting at because it only talks about string literals.
Anyway, coming from Java and learning C a bit better.  I love the options that C brings but I know I've got to be careful with memory management.
I have created my own timer utility as an exercise to get familiar with how to do things in C and came across this question... Here's a code sample first (Windows console app):
HANDLE createHPTimer(char *timerName, DWORD duration, UINT16 repeat, bool persist, void *callbackFunc, void *callbackArgs) {

    //...
    //...  (here I ensure struct hptGlobals is properly initialized and that *timerName does not already belong to the collection)
    //...

    HANDLE hHPTimer;
    hHPTimer = malloc(sizeof(HPTimerObj));
    if (hHPTimer != INVALID_HANDLE_VALUE) {
        if (hptGlobals.timerCnt >= hptGlobals.allocCnt) {
            //reallocate more room to global collection to fit this new timer
            hptGlobals.hCollection = (HANDLE*)realloc(hptGlobals.hCollection, sizeof(HANDLE)*(hptGlobals.allocCnt + hpt_ALLOC_RES));
        }
        //add new timer handle to global collection
        hptGlobals.hCollection[hptGlobals.timerCnt] = hHPTimer;

        //here I assign function parameters into the structure for later use
        //timerName is a c string therefore I am only saving the pointer reference
        ((HPTimerObj*)hHPTimer)->timerName = timerName;
        ((HPTimerObj*)hHPTimer)->status = hpt_INIT_NOT_STARTED;     //timer is initialized in the not started state
        ((HPTimerObj*)hHPTimer)->duration = duration;
        ((HPTimerObj*)hHPTimer)->repeat = repeat;
        ((HPTimerObj*)hHPTimer)->runCnt = 0;
        ((HPTimerObj*)hHPTimer)->persist = persist;
        ((HPTimerObj*)hHPTimer)->callbackFunc = callbackFunc;
        ((HPTimerObj*)hHPTimer)->callbackArgs = callbackArgs;

        hptGlobals.timerCnt++;
        return hHPTimer;
    } else 
        return INVALID_HANDLE_VALUE;    //return invalid handle if HPTimer creation failed
}

My HPTimerObj struct looks like this:
typedef struct {
    char *timerName;
    BOOL status;    //uses the following flags: hpt_INIT_NOT_STARTED, hpt_INIT_STARTED, hpt_INIT_PAUSED
    UINT64 timeData;//if the timer is currently running, this is the timestamp of the next scheduled callback; if the timer is paused, this is the elapsed duration when paused
    DWORD duration; 
    UINT16 repeat;  //number of times to run timer before it frees itself (use persist flag to keep timer (pause it) after runCount has been reached)
    UINT16 runCnt;  //number of times timer has triggered
    bool persist;
    void *callbackFunc;
    void *callbackArgs;
} HPTimerObj;

So I have three pointers that were passed into the function and I need to reference their data later: timerName, callbackFunc, and callbackArgs
This code works (for my test case) as written but I'm not sure it is safe because I do not know how to determine how memory for the pointer timerName was allocated -- is it in the stack or on the heap?  Is it static or a string literal (which I think it would always be valid and safe).  If this function is called from code where timerName was allocated on the stack then there is a possibility that it could be overwritten later.  Am I correct in this thinking?
If I could always know how the memory was allocated then I could copy it from stack to heap if needed but I don't think that is possible (see this SO question).  I think that the other two pointer arguments are always safe because one is a function pointer (I would assume would be global/static) and the other is supposed to be a pointer to a struct which must be on the heap, is that right?
To ensure that timerName always has valid data I could create a copy on the heap as follows:
//instead, to ensure timerName data is on the heap, copy it there
size_t timerNameLen;
timerNameLen = strlen(timerName);
char *newTimerName = (char*)malloc(sizeof(char)*(timerNameLen + 1));    //Must use free on ((HPTimerObj*)hHPTimer)->timerName when ""killing"" the timer
strcpy_s(newTimerName, timerNameLen + 1, timerName);
((HPTimerObj*)hHPTimer)->timerName = newTimerName;

Should I make this a habit in other similar situations or was it okay the way I first wrote it?  I'd like to avoid duplicating values which may take extra time and memory if I don't need to.  Are there any better approaches for this?  Thanks!
","Ensure long-term valid data when passed by reference? How to load my struct with pointer without knowing how its memory was allocated?
I am surprised that I didn't find the question already posted.  Here is one that is similar but not what I'm getting at because it only talks about string literals.
Anyway, coming from Java and learning C a bit better.  I love the options that C brings but I know I've got to be careful with memory management.
I have created my own timer utility as an exercise to get familiar with how to do things in C and came across this question... Here's a code sample first (Windows console app):
HANDLE createHPTimer(char *timerName, DWORD duration, UINT16 repeat, bool persist, void *callbackFunc, void *callbackArgs) {

    //...
    //...  (here I ensure struct hptGlobals is properly initialized and that *timerName does not already belong to the collection)
    //...

    HANDLE hHPTimer;
    hHPTimer = malloc(sizeof(HPTimerObj));
    if (hHPTimer != INVALID_HANDLE_VALUE) {
        if (hptGlobals.timerCnt >= hptGlobals.allocCnt) {
            //reallocate more room to global collection to fit this new timer
            hptGlobals.hCollection = (HANDLE*)realloc(hptGlobals.hCollection, sizeof(HANDLE)*(hptGlobals.allocCnt + hpt_ALLOC_RES));
        }
        //add new timer handle to global collection
        hptGlobals.hCollection[hptGlobals.timerCnt] = hHPTimer;

        //here I assign function parameters into the structure for later use
        //timerName is a c string therefore I am only saving the pointer reference
        ((HPTimerObj*)hHPTimer)->timerName = timerName;
        ((HPTimerObj*)hHPTimer)->status = hpt_INIT_NOT_STARTED;     //timer is initialized in the not started state
        ((HPTimerObj*)hHPTimer)->duration = duration;
        ((HPTimerObj*)hHPTimer)->repeat = repeat;
        ((HPTimerObj*)hHPTimer)->runCnt = 0;
        ((HPTimerObj*)hHPTimer)->persist = persist;
        ((HPTimerObj*)hHPTimer)->callbackFunc = callbackFunc;
        ((HPTimerObj*)hHPTimer)->callbackArgs = callbackArgs;

        hptGlobals.timerCnt++;
        return hHPTimer;
    } else 
        return INVALID_HANDLE_VALUE;    //return invalid handle if HPTimer creation failed
}

My HPTimerObj struct looks like this:
typedef struct {
    char *timerName;
    BOOL status;    //uses the following flags: hpt_INIT_NOT_STARTED, hpt_INIT_STARTED, hpt_INIT_PAUSED
    UINT64 timeData;//if the timer is currently running, this is the timestamp of the next scheduled callback; if the timer is paused, this is the elapsed duration when paused
    DWORD duration; 
    UINT16 repeat;  //number of times to run timer before it frees itself (use persist flag to keep timer (pause it) after runCount has been reached)
    UINT16 runCnt;  //number of times timer has triggered
    bool persist;
    void *callbackFunc;
    void *callbackArgs;
} HPTimerObj;

So I have three pointers that were passed into the function and I need to reference their data later: timerName, callbackFunc, and callbackArgs
This code works (for my test case) as written but I'm not sure it is safe because I do not know how to determine how memory for the pointer timerName was allocated -- is it in the stack or on the heap?  Is it static or a string literal (which I think it would always be valid and safe).  If this function is called from code where timerName was allocated on the stack then there is a possibility that it could be overwritten later.  Am I correct in this thinking?
If I could always know how the memory was allocated then I could copy it from stack to heap if needed but I don't think that is possible (see this SO question).  I think that the other two pointer arguments are always safe because one is a function pointer (I would assume would be global/static) and the other is supposed to be a pointer to a struct which must be on the heap, is that right?
To ensure that timerName always has valid data I could create a copy on the heap as follows:
//instead, to ensure timerName data is on the heap, copy it there
size_t timerNameLen;
timerNameLen = strlen(timerName);
char *newTimerName = (char*)malloc(sizeof(char)*(timerNameLen + 1));    //Must use free on ((HPTimerObj*)hHPTimer)->timerName when ""killing"" the timer
strcpy_s(newTimerName, timerNameLen + 1, timerName);
((HPTimerObj*)hHPTimer)->timerName = newTimerName;

Should I make this a habit in other similar situations or was it okay the way I first wrote it?  I'd like to avoid duplicating values which may take extra time and memory if I don't need to.  Are there any better approaches for this?  Thanks!
",1,0,1,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,0,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
54800292,54800292,Optimize function for finding a nested property in an object?,"<p>I made the following function which traverses an object recursively  and tries to find if a property exists in that object. If it does, it returns that property and null if it is not found:</p>

<pre><code>export const findNestedPropertyInObject = (sourceObject: any, targetKey: string) =&gt; {
  let results: any;
  const maxIterations = 100;
  let currentIteration = 0;
  const searchInObject = (obj: any, targetKey: string) =&gt; {
    const keys = obj.length ? [] : Object.keys(obj);
    if (keys &amp;&amp; keys.length) {
      if (keys.includes(targetKey)) {
        results = obj[targetKey];
        return;
      } else {
        keys.forEach((key: string) =&gt; {
          if (currentIteration &gt;= maxIterations) {
            return;
          }
          currentIteration++;
          searchInObject(obj[key], targetKey);
        });
      }
    }
  };

  searchInObject(sourceObject, targetKey);
  return results;
};
</code></pre>

<p>this is the sample object:</p>

<pre><code>const aggregations = {
  most_used_skills: {
    doc_count: 388,
    skill: {
      doc_count_error_upper_bound: 10,
      sum_other_doc_count: 325,
      buckets: [
        {
          key: 'C++ Programming Language',
          doc_count: 15,
        },
        {
          key: 'Javascript',
          doc_count: 14,
        },
        {
          key: 'ExtJS',
          doc_count: 12,
        },
        {
          key: 'Amazon Web Services',
          doc_count: 11,
        },
        {
          key: 'Android',
          doc_count: 11,
        },
      ],
    },
  },
};
</code></pre>

<p>this is how i am invoking it:</p>

<pre><code>console.log(findNestedPropertyInObject(aggregations, 'buckets'));
</code></pre>

<p>My question is that, how can i optimize this method? What safety checks can i add to this to make it more fault tolerant and robust?</p>
","i made the following function which traverses an object recursively  and tries to find if a property exists in that object if it does it returns that property and null if it is not found
this is the sample object
this is how i am invoking it
my question is that how can i optimize this method what safety checks can i add to this to make it more fault tolerant and robust
",1,"I made the following function which traverses an object recursively  and tries to find if a property exists in that object. If it does, it returns that property and null if it is not found:
export const findNestedPropertyInObject = (sourceObject: any, targetKey: string) => {
  let results: any;
  const maxIterations = 100;
  let currentIteration = 0;
  const searchInObject = (obj: any, targetKey: string) => {
    const keys = obj.length ? [] : Object.keys(obj);
    if (keys && keys.length) {
      if (keys.includes(targetKey)) {
        results = obj[targetKey];
        return;
      } else {
        keys.forEach((key: string) => {
          if (currentIteration >= maxIterations) {
            return;
          }
          currentIteration++;
          searchInObject(obj[key], targetKey);
        });
      }
    }
  };

  searchInObject(sourceObject, targetKey);
  return results;
};

this is the sample object:
const aggregations = {
  most_used_skills: {
    doc_count: 388,
    skill: {
      doc_count_error_upper_bound: 10,
      sum_other_doc_count: 325,
      buckets: [
        {
          key: 'C++ Programming Language',
          doc_count: 15,
        },
        {
          key: 'Javascript',
          doc_count: 14,
        },
        {
          key: 'ExtJS',
          doc_count: 12,
        },
        {
          key: 'Amazon Web Services',
          doc_count: 11,
        },
        {
          key: 'Android',
          doc_count: 11,
        },
      ],
    },
  },
};

this is how i am invoking it:
console.log(findNestedPropertyInObject(aggregations, 'buckets'));

My question is that, how can i optimize this method? What safety checks can i add to this to make it more fault tolerant and robust?
","Optimize function for finding a nested property in an object?
I made the following function which traverses an object recursively  and tries to find if a property exists in that object. If it does, it returns that property and null if it is not found:
export const findNestedPropertyInObject = (sourceObject: any, targetKey: string) => {
  let results: any;
  const maxIterations = 100;
  let currentIteration = 0;
  const searchInObject = (obj: any, targetKey: string) => {
    const keys = obj.length ? [] : Object.keys(obj);
    if (keys && keys.length) {
      if (keys.includes(targetKey)) {
        results = obj[targetKey];
        return;
      } else {
        keys.forEach((key: string) => {
          if (currentIteration >= maxIterations) {
            return;
          }
          currentIteration++;
          searchInObject(obj[key], targetKey);
        });
      }
    }
  };

  searchInObject(sourceObject, targetKey);
  return results;
};

this is the sample object:
const aggregations = {
  most_used_skills: {
    doc_count: 388,
    skill: {
      doc_count_error_upper_bound: 10,
      sum_other_doc_count: 325,
      buckets: [
        {
          key: 'C++ Programming Language',
          doc_count: 15,
        },
        {
          key: 'Javascript',
          doc_count: 14,
        },
        {
          key: 'ExtJS',
          doc_count: 12,
        },
        {
          key: 'Amazon Web Services',
          doc_count: 11,
        },
        {
          key: 'Android',
          doc_count: 11,
        },
      ],
    },
  },
};

this is how i am invoking it:
console.log(findNestedPropertyInObject(aggregations, 'buckets'));

My question is that, how can i optimize this method? What safety checks can i add to this to make it more fault tolerant and robust?
",0,1,0,1,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
27090902,27090902,Confusion with unique_ptr,"<p>I would expect the following code to crash at runtime with a null pointer error:</p>

<pre><code>#include &lt;memory&gt;
#include &lt;iostream&gt;
#include &lt;cassert&gt;

struct Foo {
    void echo() {std::cout &lt;&lt; ""Echo"" &lt;&lt; std::endl;}
};

int main()
{
    std::unique_ptr&lt;Foo&gt; up(new Foo());
    up.reset(nullptr);

    assert(up.get() == nullptr);

    up.get()-&gt;echo();
}
</code></pre>

<p>however both gcc (4.7/4.8/4.9), msvc (2013 and upcoming 2015) and clang (3.5) happily outputs:</p>

<pre><code>Echo
</code></pre>

<p>and assert is not firing so <code>up.get()</code> is <code>nullptr</code>.</p>
","i would expect the following code to crash at runtime with a null pointer error
however both gcc 474849 msvc 2013 and upcoming 2015 and clang 35 happily outputs
and assert is not firing so upget is nullptr
",1,"I would expect the following code to crash at runtime with a null pointer error:
#include <memory>
#include <iostream>
#include <cassert>

struct Foo {
    void echo() {std::cout << ""Echo"" << std::endl;}
};

int main()
{
    std::unique_ptr<Foo> up(new Foo());
    up.reset(nullptr);

    assert(up.get() == nullptr);

    up.get()->echo();
}

however both gcc (4.7/4.8/4.9), msvc (2013 and upcoming 2015) and clang (3.5) happily outputs:
Echo

and assert is not firing so up.get() is nullptr.
","Confusion with unique_ptr
I would expect the following code to crash at runtime with a null pointer error:
#include <memory>
#include <iostream>
#include <cassert>

struct Foo {
    void echo() {std::cout << ""Echo"" << std::endl;}
};

int main()
{
    std::unique_ptr<Foo> up(new Foo());
    up.reset(nullptr);

    assert(up.get() == nullptr);

    up.get()->echo();
}

however both gcc (4.7/4.8/4.9), msvc (2013 and upcoming 2015) and clang (3.5) happily outputs:
Echo

and assert is not firing so up.get() is nullptr.
",1,0,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,0,0,0,0,0,,,Code,Code,,Code,Very Mild,Very Mild,,Very Mild
69787772,69787772,Fast/vectorized iterating and updating of numpy 2D array,"<p>This question is quite close to my heart as I have been doing something like this for almost 2 years and always wondered if there is a vectorized way of modifying large array/dataframe when row <code>i</code> depends upon row <code>i-1</code>, i.e., when recursion sounds mandatory. I am very keen to hear if there are either <em><strong>clever algorithms</strong></em> or <em><strong>clever tools (cython, numba, get rid of redundant operations, etc.)</strong></em> to optimize the runtime.</p>
<p><strong>Problem</strong>:<br>
I have 3 big numpy arrays: <code>x</code>, <code>y</code> and <code>z</code> of shape (1million by 500), (1million by 500) and (1million by 1). Clip/winsorize each element in any given row <code>i</code> of <code>x</code> based on whether <code>|(x[i] - x[i-1]) * z[i] / y[i]| &gt; thresh</code>. I am doing this in the following way which is taking extremely long time for my simulations to run (esp when this step repeats thousands of time to tune the hyperparameters):</p>
<pre><code>t = x.copy()
t[0] = np.clip(t[0] * z[0]/ y[0], -1 * thresh, thresh) * y[0] / z[0]
for i in range(1, t.shape[0]):
    t[i] = np.clip((t[i] - t[i-1]) * z[i] / y[i], -1* thresh, thresh) * y[i] / z[i] + t[i-1]
</code></pre>
<p><strong>Sample input</strong>:</p>
<pre><code>import numpy as np
import random
x = np.random.rand(1000000, 500)
y = np.random.rand(1000000, 500)
z = np.random.rand(1000000, 1)
thresh = 0.7
</code></pre>
<p><strong>Edit</strong>: <em>Modified to remove append as suggested by</em> @Mad Physicist <em>and redundant if-else as suggested by</em> @Pedro Maia</p>
","this question is quite close to my heart as i have been doing something like this for almost 2 years and always wondered if there is a vectorized way of modifying large arraydataframe when row i depends upon row i1 ie when recursion sounds mandatory i am very keen to hear if there are either clever algorithms or clever tools cython numba get rid of redundant operations etc to optimize the runtime
problem
i have 3 big numpy arrays x y and z of shape 1million by 500 1million by 500 and 1million by 1 clipwinsorize each element in any given row i of x based on whether xi  xi1  zi  yi  thresh i am doing this in the following way which is taking extremely long time for my simulations to run esp when this step repeats thousands of time to tune the hyperparameters
sample input
edit modified to remove append as suggested by mad physicist and redundant ifelse as suggested by pedro maia
",1,"This question is quite close to my heart as I have been doing something like this for almost 2 years and always wondered if there is a vectorized way of modifying large array/dataframe when row i depends upon row i-1, i.e., when recursion sounds mandatory. I am very keen to hear if there are either clever algorithms or clever tools (cython, numba, get rid of redundant operations, etc.) to optimize the runtime.
Problem:
I have 3 big numpy arrays: x, y and z of shape (1million by 500), (1million by 500) and (1million by 1). Clip/winsorize each element in any given row i of x based on whether |(x[i] - x[i-1]) * z[i] / y[i]| > thresh. I am doing this in the following way which is taking extremely long time for my simulations to run (esp when this step repeats thousands of time to tune the hyperparameters):
t = x.copy()
t[0] = np.clip(t[0] * z[0]/ y[0], -1 * thresh, thresh) * y[0] / z[0]
for i in range(1, t.shape[0]):
    t[i] = np.clip((t[i] - t[i-1]) * z[i] / y[i], -1* thresh, thresh) * y[i] / z[i] + t[i-1]

Sample input:
import numpy as np
import random
x = np.random.rand(1000000, 500)
y = np.random.rand(1000000, 500)
z = np.random.rand(1000000, 1)
thresh = 0.7

Edit: Modified to remove append as suggested by @Mad Physicist and redundant if-else as suggested by @Pedro Maia
","Fast/vectorized iterating and updating of numpy 2D array
This question is quite close to my heart as I have been doing something like this for almost 2 years and always wondered if there is a vectorized way of modifying large array/dataframe when row i depends upon row i-1, i.e., when recursion sounds mandatory. I am very keen to hear if there are either clever algorithms or clever tools (cython, numba, get rid of redundant operations, etc.) to optimize the runtime.
Problem:
I have 3 big numpy arrays: x, y and z of shape (1million by 500), (1million by 500) and (1million by 1). Clip/winsorize each element in any given row i of x based on whether |(x[i] - x[i-1]) * z[i] / y[i]| > thresh. I am doing this in the following way which is taking extremely long time for my simulations to run (esp when this step repeats thousands of time to tune the hyperparameters):
t = x.copy()
t[0] = np.clip(t[0] * z[0]/ y[0], -1 * thresh, thresh) * y[0] / z[0]
for i in range(1, t.shape[0]):
    t[i] = np.clip((t[i] - t[i-1]) * z[i] / y[i], -1* thresh, thresh) * y[i] / z[i] + t[i-1]

Sample input:
import numpy as np
import random
x = np.random.rand(1000000, 500)
y = np.random.rand(1000000, 500)
z = np.random.rand(1000000, 1)
thresh = 0.7

Edit: Modified to remove append as suggested by @Mad Physicist and redundant if-else as suggested by @Pedro Maia
",0,1,0,1,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,1,0,0,,,Design,Design,,Design,Moderate,Moderate,,Moderate
24980934,24980934,Is Java fork/join (divide and conquor) possible when inserting line-by-line into MongoDB from InputStream?,"<p>I list code below which performs batch inserts into MongoDB. This code takes a long time to run--about an hour to insert 20 million Mongo documents. </p>

<p>The time consuming portion of the code--the scanner.hasNextLine()--nextLine()--insert loop--runs as slowly as 20 seconds an iteration at times. This slowness becomes pronounced toward the middle of the job, I note. (Answers on this forum indicate that the mongo insert-batch or regular can be expensive, owing to the conversion of json into binary format, bson.)</p>

<p>I want to speed up this process. I would like to process this job in parallel on several cores. Can I do this using Fork/join?  I ask because I could not see a way how to apply a divide and conquer strategy in the case of this code with its while loop on an input stream.  </p>

<p>Another possibility is to use the ThreadPoolExecutor. Would use of an executor be best? Would an executor distribute the job over several cores? </p>

<p>The code:</p>

<pre><code>Scanner lineScan = new Scanner(inputStream, encoding);
while (lineScan.hasNextLine() {
  //add to list of DBObjects to be inserted as a batch
  //do batch insert here if object-count threshold is reached.
}
</code></pre>

<p>Similar code using a ThreadPoolExecutor ( see </p>

<p><a href=""https://stackoverflow.com/questions/9455280/java-iterator-concurrency"">Java Iterator Concurrency</a> and  <a href=""https://stackoverflow.com/questions/16159183/java-concurrent-reads-on-an-inputstream"">Java: Concurrent reads on an InputStream</a> ):</p>

<pre><code>ExecutorService executor = Executors.newCachedThreadPool();
Iterator&lt;Long&gt; i = getUserIDs();
while (i.hasNext()) {
    final Long l = i.next();

    Runnable task = new Runnable() {
        public void run() {
            someObject.doSomething(l);
            anotheObject.doSomething(l);
        }
    }

    executor.submit(task);
}

executor.shutdown();
</code></pre>

<p>Any perspectives on which technique might best expedite this loop and insert would be greatly appreciated.
Many thanks in advance!</p>
","i list code below which performs batch inserts into mongodb this code takes a long time to runabout an hour to insert 20 million mongo documents 
the time consuming portion of the codethe scannerhasnextlinenextlineinsert loopruns as slowly as 20 seconds an iteration at times this slowness becomes pronounced toward the middle of the job i note answers on this forum indicate that the mongo insertbatch or regular can be expensive owing to the conversion of json into binary format bson
i want to speed up this process i would like to process this job in parallel on several cores can i do this using forkjoin  i ask because i could not see a way how to apply a divide and conquer strategy in the case of this code with its while loop on an input stream  
another possibility is to use the threadpoolexecutor would use of an executor be best would an executor distribute the job over several cores 
the code
similar code using a threadpoolexecutor  see 
java iterator concurrency and  java concurrent reads on an inputstream 
any perspectives on which technique might best expedite this loop and insert would be greatly appreciated
many thanks in advance
",1,"I list code below which performs batch inserts into MongoDB. This code takes a long time to run--about an hour to insert 20 million Mongo documents. 
The time consuming portion of the code--the scanner.hasNextLine()--nextLine()--insert loop--runs as slowly as 20 seconds an iteration at times. This slowness becomes pronounced toward the middle of the job, I note. (Answers on this forum indicate that the mongo insert-batch or regular can be expensive, owing to the conversion of json into binary format, bson.)
I want to speed up this process. I would like to process this job in parallel on several cores. Can I do this using Fork/join?  I ask because I could not see a way how to apply a divide and conquer strategy in the case of this code with its while loop on an input stream.  
Another possibility is to use the ThreadPoolExecutor. Would use of an executor be best? Would an executor distribute the job over several cores? 
The code:
Scanner lineScan = new Scanner(inputStream, encoding);
while (lineScan.hasNextLine() {
  //add to list of DBObjects to be inserted as a batch
  //do batch insert here if object-count threshold is reached.
}

Similar code using a ThreadPoolExecutor ( see 
Java Iterator Concurrency and  Java: Concurrent reads on an InputStream ):
ExecutorService executor = Executors.newCachedThreadPool();
Iterator<Long> i = getUserIDs();
while (i.hasNext()) {
    final Long l = i.next();

    Runnable task = new Runnable() {
        public void run() {
            someObject.doSomething(l);
            anotheObject.doSomething(l);
        }
    }

    executor.submit(task);
}

executor.shutdown();

Any perspectives on which technique might best expedite this loop and insert would be greatly appreciated.
Many thanks in advance!
","Is Java fork/join (divide and conquor) possible when inserting line-by-line into MongoDB from InputStream?
I list code below which performs batch inserts into MongoDB. This code takes a long time to run--about an hour to insert 20 million Mongo documents. 
The time consuming portion of the code--the scanner.hasNextLine()--nextLine()--insert loop--runs as slowly as 20 seconds an iteration at times. This slowness becomes pronounced toward the middle of the job, I note. (Answers on this forum indicate that the mongo insert-batch or regular can be expensive, owing to the conversion of json into binary format, bson.)
I want to speed up this process. I would like to process this job in parallel on several cores. Can I do this using Fork/join?  I ask because I could not see a way how to apply a divide and conquer strategy in the case of this code with its while loop on an input stream.  
Another possibility is to use the ThreadPoolExecutor. Would use of an executor be best? Would an executor distribute the job over several cores? 
The code:
Scanner lineScan = new Scanner(inputStream, encoding);
while (lineScan.hasNextLine() {
  //add to list of DBObjects to be inserted as a batch
  //do batch insert here if object-count threshold is reached.
}

Similar code using a ThreadPoolExecutor ( see 
Java Iterator Concurrency and  Java: Concurrent reads on an InputStream ):
ExecutorService executor = Executors.newCachedThreadPool();
Iterator<Long> i = getUserIDs();
while (i.hasNext()) {
    final Long l = i.next();

    Runnable task = new Runnable() {
        public void run() {
            someObject.doSomething(l);
            anotheObject.doSomething(l);
        }
    }

    executor.submit(task);
}

executor.shutdown();

Any perspectives on which technique might best expedite this loop and insert would be greatly appreciated.
Many thanks in advance!
",1,1,0,1,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,1,1,0,1,0,0,,,Architecture,Design,Design seems like the bigger issue since the code works fine but needs to be faster and more scalable,Design,Moderate,Moderate,,Moderate
41872809,41872809,How to correctly handle input argument of a python script when using parallel?,"<p>I've the followint situation:</p>

<p>my script (say script.py) takes an argument as input. I have a .txt file with a sequence of possible inputs (each input is a string representing an ID).</p>

<p>within the script I do something like this</p>

<pre><code>#!/usr/bin/python                                                  

import sys

def doStuff(ID):

        ID = str(ID)
        path = ID + ""/posts?""

        print ""\n\n\t""+path
        foo = ID + ""hello""
        print foo
        print ID+""hello""
        sys.exit(0)

inputID =sys.argv[1]
print ""Input ID\t-\t"" + str(inputID)
doStuff(inputID)
</code></pre>

<p>if I run the code with</p>

<pre><code>python script.py 123456789
</code></pre>

<p>I correctly get</p>

<pre><code>        Input ID    -    123456789
        123456789/posts?
        123456789hello
        123456789hello
</code></pre>

<p>but if I do something like</p>

<pre><code>cat IDs.txt | parallel python script.py {}
</code></pre>

<p>I get</p>

<pre><code>Input ID    -    123456789
/posts? 123456789
hello6789
hello6789
</code></pre>

<p>what is happening here? when i concatenate strings the order is reverted, moreover in the last two prints the inputID is overwritten by ""hello""</p>
","ive the followint situation
my script say scriptpy takes an argument as input i have a txt file with a sequence of possible inputs each input is a string representing an id
within the script i do something like this
if i run the code with
i correctly get
but if i do something like
i get
what is happening here when i concatenate strings the order is reverted moreover in the last two prints the inputid is overwritten by hello
",0,"I've the followint situation:
my script (say script.py) takes an argument as input. I have a .txt file with a sequence of possible inputs (each input is a string representing an ID).
within the script I do something like this
#!/usr/bin/python                                                  

import sys

def doStuff(ID):

        ID = str(ID)
        path = ID + ""/posts?""

        print ""\n\n\t""+path
        foo = ID + ""hello""
        print foo
        print ID+""hello""
        sys.exit(0)

inputID =sys.argv[1]
print ""Input ID\t-\t"" + str(inputID)
doStuff(inputID)

if I run the code with
python script.py 123456789

I correctly get
        Input ID    -    123456789
        123456789/posts?
        123456789hello
        123456789hello

but if I do something like
cat IDs.txt | parallel python script.py {}

I get
Input ID    -    123456789
/posts? 123456789
hello6789
hello6789

what is happening here? when i concatenate strings the order is reverted, moreover in the last two prints the inputID is overwritten by ""hello""
","How to correctly handle input argument of a python script when using parallel?
I've the followint situation:
my script (say script.py) takes an argument as input. I have a .txt file with a sequence of possible inputs (each input is a string representing an ID).
within the script I do something like this
#!/usr/bin/python                                                  

import sys

def doStuff(ID):

        ID = str(ID)
        path = ID + ""/posts?""

        print ""\n\n\t""+path
        foo = ID + ""hello""
        print foo
        print ID+""hello""
        sys.exit(0)

inputID =sys.argv[1]
print ""Input ID\t-\t"" + str(inputID)
doStuff(inputID)

if I run the code with
python script.py 123456789

I correctly get
        Input ID    -    123456789
        123456789/posts?
        123456789hello
        123456789hello

but if I do something like
cat IDs.txt | parallel python script.py {}

I get
Input ID    -    123456789
/posts? 123456789
hello6789
hello6789

what is happening here? when i concatenate strings the order is reverted, moreover in the last two prints the inputID is overwritten by ""hello""
",0,1,0,0,0,0,1,- Code not working as intended,,,1,1,I/O Optimization,I/O Optimization,,I/O Optimization,0,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
44350933,44350933,What would be the best way to refactor following script - why value does not carry back at beginning of loop. if or for statement broken - Javascript,"<p>I was working on transforming numerals to Roman numbers.
With a little a bit of work I got my code to work, but for some reason, I feel it is off. </p>

<p>Here is the code below. Run the code snippet. You will see the result will display in the console. </p>

<p>Basically after the for and the if statement start, you will see the following.</p>

<pre><code>      cardinal = roman[i].key;
      converter = roman[i].romano;
      converted.push(converter);
      array = num.toString().split('');
      array.splice(0,1);
      rejoin = array.join('');
      num = Number(rejoin);
</code></pre>

<p>after <strong><em>num=Number(rejoin);</em></strong> I wanted to have the code go back to the beginning and repeat until num=0. But I cannot seem to get it work. it is like the loop or the if the statement is broken. The new value of <strong><em>num</em></strong> does not carry back at the very beginning of the loop.</p>

<p>So I had to refactor it a bit and add a function cardinal that just repeats the initial code and add a do while. though it works I am still puzzled why the <strong><em>num</em></strong> does not carry back at the beginning. Any explanation will be appreciated.</p>

<p><strong><em>Explanation of the work on num</em></strong></p>

<p>Let's say you enter a value a 450. As the 450 goes through the code, the code checks the closest reference to 450. It will check in the object and find 400 and convert it to Roman. Next, I want to drop the number 4 of 450 and stay with the 50 and I want the 50 to be referenced in the object and converted. I want to do it until the last number is 0.</p>

<p>Thanks for your feedbacks.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>function convertToRoman(num) {
  var cardinal ;
  var array = [];
  var converter =[];
  var converted=[];
  var rejoin=[];
  var roman = [
    {""key"": 1, ""romano"":'I'},
    {""key"": 2, ""romano"":'II'},
    {""key"": 3, ""romano"":'III'},
    {""key"": 4, ""romano"":'IV'},
    {""key"": 5, ""romano"":'V'},
    {""key"": 6, ""romano"":'VI'},
    {""key"": 7, ""romano"":'VII'},
    {""key"": 8, ""romano"":'VIII'},
    {""key"": 9, ""romano"":'IX'},
    {""key"": 10, ""romano"":'X'},
    {""key"": 20, ""romano"":'XX'},
    {""key"": 30, ""romano"":'XXX'},
    {""key"": 40, ""romano"":'XL'},
    {""key"": 50, ""romano"":'L'},
    {""key"": 60, ""romano"":'LX'},
    {""key"": 70, ""romano"":'LXX'},
    {""key"": 80, ""romano"":'LXXX'},
    {""key"": 90, ""romano"":'XC'},
    {""key"": 100, ""romano"":'C'},
    {""key"": 200, ""romano"":'CC'},
    {""key"": 300, ""romano"":'CCC'},
    {""key"": 400, ""romano"":'CD'},
    {""key"": 500, ""romano"":'D'},
    {""key"": 600, ""romano"":'DC'},
    {""key"": 700, ""romano"":'DCC'},
    {""key"": 800, ""romano"":'DCCC'},
    {""key"": 900, ""romano"":'CM'},
    {""key"": 1000, ""romano"":'M'},
    {""key"": 2000, ""romano"":'MM'},
    {""key"": 3000, ""romano"":'MMM'},
    {""key"": 4000, ""romano"":'MV'},
    {""key"": 5000, ""romano"":'|V'}
    ];
  
  function cardinals (num) {
      for (i=0 ; i&lt; roman.length; i++) {
          if ( (num &gt; roman[i].key &amp;&amp; num &lt; roman[i+1].key) || (num == roman[i].key) ) {
            cardinal = roman[i].key;
            converter = roman[i].romano;
            converted.push(converter);
         }
     }
    
  }
  
  
  
  for (i=0 ; i&lt; roman.length; i++) {
      if ( (num &gt; roman[i].key &amp;&amp; num &lt; roman[i+1].key) || (num == roman[i].key) ) {
          cardinal = roman[i].key;
          converter = roman[i].romano;
          converted.push(converter);
          array = num.toString().split('');
          array.splice(0,1);
          rejoin = array.join('');
          num = Number(rejoin);
          cardinals(num);
          do {
            array = num.toString().split('');
            array.splice(0,1);
            rejoin = array.join('');
            num = Number(rejoin);
            cardinals(num);
          } while (num!==0);
      }
  }
  converted = converted.join('');
  return converted;
}

console.log(convertToRoman(3999));</code></pre>
</div>
</div>
</p>
","i was working on transforming numerals to roman numbers
with a little a bit of work i got my code to work but for some reason i feel it is off 
here is the code below run the code snippet you will see the result will display in the console 
basically after the for and the if statement start you will see the following
after numnumberrejoin i wanted to have the code go back to the beginning and repeat until num0 but i cannot seem to get it work it is like the loop or the if the statement is broken the new value of num does not carry back at the very beginning of the loop
so i had to refactor it a bit and add a function cardinal that just repeats the initial code and add a do while though it works i am still puzzled why the num does not carry back at the beginning any explanation will be appreciated
explanation of the work on num
lets say you enter a value a 450 as the 450 goes through the code the code checks the closest reference to 450 it will check in the object and find 400 and convert it to roman next i want to drop the number 4 of 450 and stay with the 50 and i want the 50 to be referenced in the object and converted i want to do it until the last number is 0
thanks for your feedbacks


function converttoromannum 
  var cardinal 
  var array  
  var converter 
  var converted
  var rejoin
  var roman  
    key 1 romanoi
    key 2 romanoii
    key 3 romanoiii
    key 4 romanoiv
    key 5 romanov
    key 6 romanovi
    key 7 romanovii
    key 8 romanoviii
    key 9 romanoix
    key 10 romanox
    key 20 romanoxx
    key 30 romanoxxx
    key 40 romanoxl
    key 50 romanol
    key 60 romanolx
    key 70 romanolxx
    key 80 romanolxxx
    key 90 romanoxc
    key 100 romanoc
    key 200 romanocc
    key 300 romanoccc
    key 400 romanocd
    key 500 romanod
    key 600 romanodc
    key 700 romanodcc
    key 800 romanodccc
    key 900 romanocm
    key 1000 romanom
    key 2000 romanomm
    key 3000 romanommm
    key 4000 romanomv
    key 5000 romanov
    
  
  function cardinals num 
      for i0  i romanlength i 
          if  num  romanikey  num  romani1key  num  romanikey  
            cardinal  romanikey
            converter  romaniromano
            convertedpushconverter
         
     
    
  
  
  
  
  for i0  i romanlength i 
      if  num  romanikey  num  romani1key  num  romanikey  
          cardinal  romanikey
          converter  romaniromano
          convertedpushconverter
          array  numtostringsplit
          arraysplice01
          rejoin  arrayjoin
          num  numberrejoin
          cardinalsnum
          do 
            array  numtostringsplit
            arraysplice01
            rejoin  arrayjoin
            num  numberrejoin
            cardinalsnum
           while num0
      
  
  converted  convertedjoin
  return converted


consolelogconverttoroman3999



",1,"I was working on transforming numerals to Roman numbers.
With a little a bit of work I got my code to work, but for some reason, I feel it is off. 
Here is the code below. Run the code snippet. You will see the result will display in the console. 
Basically after the for and the if statement start, you will see the following.
      cardinal = roman[i].key;
      converter = roman[i].romano;
      converted.push(converter);
      array = num.toString().split('');
      array.splice(0,1);
      rejoin = array.join('');
      num = Number(rejoin);

after num=Number(rejoin); I wanted to have the code go back to the beginning and repeat until num=0. But I cannot seem to get it work. it is like the loop or the if the statement is broken. The new value of num does not carry back at the very beginning of the loop.
So I had to refactor it a bit and add a function cardinal that just repeats the initial code and add a do while. though it works I am still puzzled why the num does not carry back at the beginning. Any explanation will be appreciated.
Explanation of the work on num
Let's say you enter a value a 450. As the 450 goes through the code, the code checks the closest reference to 450. It will check in the object and find 400 and convert it to Roman. Next, I want to drop the number 4 of 450 and stay with the 50 and I want the 50 to be referenced in the object and converted. I want to do it until the last number is 0.
Thanks for your feedbacks.


function convertToRoman(num) {
  var cardinal ;
  var array = [];
  var converter =[];
  var converted=[];
  var rejoin=[];
  var roman = [
    {""key"": 1, ""romano"":'I'},
    {""key"": 2, ""romano"":'II'},
    {""key"": 3, ""romano"":'III'},
    {""key"": 4, ""romano"":'IV'},
    {""key"": 5, ""romano"":'V'},
    {""key"": 6, ""romano"":'VI'},
    {""key"": 7, ""romano"":'VII'},
    {""key"": 8, ""romano"":'VIII'},
    {""key"": 9, ""romano"":'IX'},
    {""key"": 10, ""romano"":'X'},
    {""key"": 20, ""romano"":'XX'},
    {""key"": 30, ""romano"":'XXX'},
    {""key"": 40, ""romano"":'XL'},
    {""key"": 50, ""romano"":'L'},
    {""key"": 60, ""romano"":'LX'},
    {""key"": 70, ""romano"":'LXX'},
    {""key"": 80, ""romano"":'LXXX'},
    {""key"": 90, ""romano"":'XC'},
    {""key"": 100, ""romano"":'C'},
    {""key"": 200, ""romano"":'CC'},
    {""key"": 300, ""romano"":'CCC'},
    {""key"": 400, ""romano"":'CD'},
    {""key"": 500, ""romano"":'D'},
    {""key"": 600, ""romano"":'DC'},
    {""key"": 700, ""romano"":'DCC'},
    {""key"": 800, ""romano"":'DCCC'},
    {""key"": 900, ""romano"":'CM'},
    {""key"": 1000, ""romano"":'M'},
    {""key"": 2000, ""romano"":'MM'},
    {""key"": 3000, ""romano"":'MMM'},
    {""key"": 4000, ""romano"":'MV'},
    {""key"": 5000, ""romano"":'|V'}
    ];
  
  function cardinals (num) {
      for (i=0 ; i< roman.length; i++) {
          if ( (num > roman[i].key && num < roman[i+1].key) || (num == roman[i].key) ) {
            cardinal = roman[i].key;
            converter = roman[i].romano;
            converted.push(converter);
         }
     }
    
  }
  
  
  
  for (i=0 ; i< roman.length; i++) {
      if ( (num > roman[i].key && num < roman[i+1].key) || (num == roman[i].key) ) {
          cardinal = roman[i].key;
          converter = roman[i].romano;
          converted.push(converter);
          array = num.toString().split('');
          array.splice(0,1);
          rejoin = array.join('');
          num = Number(rejoin);
          cardinals(num);
          do {
            array = num.toString().split('');
            array.splice(0,1);
            rejoin = array.join('');
            num = Number(rejoin);
            cardinals(num);
          } while (num!==0);
      }
  }
  converted = converted.join('');
  return converted;
}

console.log(convertToRoman(3999));



","What would be the best way to refactor following script - why value does not carry back at beginning of loop. if or for statement broken - Javascript
I was working on transforming numerals to Roman numbers.
With a little a bit of work I got my code to work, but for some reason, I feel it is off. 
Here is the code below. Run the code snippet. You will see the result will display in the console. 
Basically after the for and the if statement start, you will see the following.
      cardinal = roman[i].key;
      converter = roman[i].romano;
      converted.push(converter);
      array = num.toString().split('');
      array.splice(0,1);
      rejoin = array.join('');
      num = Number(rejoin);

after num=Number(rejoin); I wanted to have the code go back to the beginning and repeat until num=0. But I cannot seem to get it work. it is like the loop or the if the statement is broken. The new value of num does not carry back at the very beginning of the loop.
So I had to refactor it a bit and add a function cardinal that just repeats the initial code and add a do while. though it works I am still puzzled why the num does not carry back at the beginning. Any explanation will be appreciated.
Explanation of the work on num
Let's say you enter a value a 450. As the 450 goes through the code, the code checks the closest reference to 450. It will check in the object and find 400 and convert it to Roman. Next, I want to drop the number 4 of 450 and stay with the 50 and I want the 50 to be referenced in the object and converted. I want to do it until the last number is 0.
Thanks for your feedbacks.


function convertToRoman(num) {
  var cardinal ;
  var array = [];
  var converter =[];
  var converted=[];
  var rejoin=[];
  var roman = [
    {""key"": 1, ""romano"":'I'},
    {""key"": 2, ""romano"":'II'},
    {""key"": 3, ""romano"":'III'},
    {""key"": 4, ""romano"":'IV'},
    {""key"": 5, ""romano"":'V'},
    {""key"": 6, ""romano"":'VI'},
    {""key"": 7, ""romano"":'VII'},
    {""key"": 8, ""romano"":'VIII'},
    {""key"": 9, ""romano"":'IX'},
    {""key"": 10, ""romano"":'X'},
    {""key"": 20, ""romano"":'XX'},
    {""key"": 30, ""romano"":'XXX'},
    {""key"": 40, ""romano"":'XL'},
    {""key"": 50, ""romano"":'L'},
    {""key"": 60, ""romano"":'LX'},
    {""key"": 70, ""romano"":'LXX'},
    {""key"": 80, ""romano"":'LXXX'},
    {""key"": 90, ""romano"":'XC'},
    {""key"": 100, ""romano"":'C'},
    {""key"": 200, ""romano"":'CC'},
    {""key"": 300, ""romano"":'CCC'},
    {""key"": 400, ""romano"":'CD'},
    {""key"": 500, ""romano"":'D'},
    {""key"": 600, ""romano"":'DC'},
    {""key"": 700, ""romano"":'DCC'},
    {""key"": 800, ""romano"":'DCCC'},
    {""key"": 900, ""romano"":'CM'},
    {""key"": 1000, ""romano"":'M'},
    {""key"": 2000, ""romano"":'MM'},
    {""key"": 3000, ""romano"":'MMM'},
    {""key"": 4000, ""romano"":'MV'},
    {""key"": 5000, ""romano"":'|V'}
    ];
  
  function cardinals (num) {
      for (i=0 ; i< roman.length; i++) {
          if ( (num > roman[i].key && num < roman[i+1].key) || (num == roman[i].key) ) {
            cardinal = roman[i].key;
            converter = roman[i].romano;
            converted.push(converter);
         }
     }
    
  }
  
  
  
  for (i=0 ; i< roman.length; i++) {
      if ( (num > roman[i].key && num < roman[i+1].key) || (num == roman[i].key) ) {
          cardinal = roman[i].key;
          converter = roman[i].romano;
          converted.push(converter);
          array = num.toString().split('');
          array.splice(0,1);
          rejoin = array.join('');
          num = Number(rejoin);
          cardinals(num);
          do {
            array = num.toString().split('');
            array.splice(0,1);
            rejoin = array.join('');
            num = Number(rejoin);
            cardinals(num);
          } while (num!==0);
      }
  }
  converted = converted.join('');
  return converted;
}

console.log(convertToRoman(3999));



",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
64490532,64490532,Merge intervals in one for-loop,"<p>This might be a strange question but I'm trying to merge interval times and am just bothered that there's one line outside of the for-loop -- I really want to merge that line (haha.) into the for-loop and have everything happen at once.</p>
<p>Problem: Say for example you're given the Intervals (1, 5), (3, 7), (4, 6), (6, 8), (10, 12), (12, 15) the program should return (1,8),(10,15) as these merge all overlapping intervals.</p>
<p>My approach is that I have a current value with the values of the first pair, I then run a for-loop from 1 to the end and check if the Pair that I'm on merges with my current or not. If it does merge I update the values in my current pair and if it doesn't merge then I add current to my result list and then update its values to whatever node I'm currently on.</p>
<p>Feel free to change up the code however necessary to make it all work in one for-loop.
If you see any other errors with the code, please let me know. Thank you!</p>
<pre><code>class Pair{
    public int first;
    public int second;
    
    public Pair(int x, int y){
      this.first = x;
      this.second = y; 
    }
}

class MergeIntervals{
  static ArrayList&lt;Pair&gt; mergeIntervals(ArrayList&lt;Pair&gt; v) {
  ArrayList&lt;Pair&gt; result = new ArrayList&lt;Pair&gt;();
 
  //check for size &amp; null
  if(v == null || v.size() == 0) {
    return null;
  } 
  if(v.size() == 0) {
    result.add(v.get(0));
    return result;
  }

  Pair current = new Pair(v.get(0).first, v.get(0).second);

  for(int i = 1; i &lt; v.size(); i++) {

    //want to merge
    if(v.get(i).first &lt; current.second) {
      if(v.get(i).second &gt; current.second) {
        current.second = v.get(i).second;
      }
    }
    else {
      result.add(new Pair(current.first, current.second));
      current.first = v.get(i).first;
      current.second = v.get(i).second;
    }
  }
  //loop broke before was able to merge
  result.add(new Pair(current.first, current.second));

  return result;
  }
}
</code></pre>
","this might be a strange question but im trying to merge interval times and am just bothered that theres one line outside of the forloop  i really want to merge that line haha into the forloop and have everything happen at once
problem say for example youre given the intervals 1 5 3 7 4 6 6 8 10 12 12 15 the program should return 181015 as these merge all overlapping intervals
my approach is that i have a current value with the values of the first pair i then run a forloop from 1 to the end and check if the pair that im on merges with my current or not if it does merge i update the values in my current pair and if it doesnt merge then i add current to my result list and then update its values to whatever node im currently on
feel free to change up the code however necessary to make it all work in one forloop
if you see any other errors with the code please let me know thank you
",1,"This might be a strange question but I'm trying to merge interval times and am just bothered that there's one line outside of the for-loop -- I really want to merge that line (haha.) into the for-loop and have everything happen at once.
Problem: Say for example you're given the Intervals (1, 5), (3, 7), (4, 6), (6, 8), (10, 12), (12, 15) the program should return (1,8),(10,15) as these merge all overlapping intervals.
My approach is that I have a current value with the values of the first pair, I then run a for-loop from 1 to the end and check if the Pair that I'm on merges with my current or not. If it does merge I update the values in my current pair and if it doesn't merge then I add current to my result list and then update its values to whatever node I'm currently on.
Feel free to change up the code however necessary to make it all work in one for-loop.
If you see any other errors with the code, please let me know. Thank you!
class Pair{
    public int first;
    public int second;
    
    public Pair(int x, int y){
      this.first = x;
      this.second = y; 
    }
}

class MergeIntervals{
  static ArrayList<Pair> mergeIntervals(ArrayList<Pair> v) {
  ArrayList<Pair> result = new ArrayList<Pair>();
 
  //check for size & null
  if(v == null || v.size() == 0) {
    return null;
  } 
  if(v.size() == 0) {
    result.add(v.get(0));
    return result;
  }

  Pair current = new Pair(v.get(0).first, v.get(0).second);

  for(int i = 1; i < v.size(); i++) {

    //want to merge
    if(v.get(i).first < current.second) {
      if(v.get(i).second > current.second) {
        current.second = v.get(i).second;
      }
    }
    else {
      result.add(new Pair(current.first, current.second));
      current.first = v.get(i).first;
      current.second = v.get(i).second;
    }
  }
  //loop broke before was able to merge
  result.add(new Pair(current.first, current.second));

  return result;
  }
}

","Merge intervals in one for-loop
This might be a strange question but I'm trying to merge interval times and am just bothered that there's one line outside of the for-loop -- I really want to merge that line (haha.) into the for-loop and have everything happen at once.
Problem: Say for example you're given the Intervals (1, 5), (3, 7), (4, 6), (6, 8), (10, 12), (12, 15) the program should return (1,8),(10,15) as these merge all overlapping intervals.
My approach is that I have a current value with the values of the first pair, I then run a for-loop from 1 to the end and check if the Pair that I'm on merges with my current or not. If it does merge I update the values in my current pair and if it doesn't merge then I add current to my result list and then update its values to whatever node I'm currently on.
Feel free to change up the code however necessary to make it all work in one for-loop.
If you see any other errors with the code, please let me know. Thank you!
class Pair{
    public int first;
    public int second;
    
    public Pair(int x, int y){
      this.first = x;
      this.second = y; 
    }
}

class MergeIntervals{
  static ArrayList<Pair> mergeIntervals(ArrayList<Pair> v) {
  ArrayList<Pair> result = new ArrayList<Pair>();
 
  //check for size & null
  if(v == null || v.size() == 0) {
    return null;
  } 
  if(v.size() == 0) {
    result.add(v.get(0));
    return result;
  }

  Pair current = new Pair(v.get(0).first, v.get(0).second);

  for(int i = 1; i < v.size(); i++) {

    //want to merge
    if(v.get(i).first < current.second) {
      if(v.get(i).second > current.second) {
        current.second = v.get(i).second;
      }
    }
    else {
      result.add(new Pair(current.first, current.second));
      current.first = v.get(i).first;
      current.second = v.get(i).second;
    }
  }
  //loop broke before was able to merge
  result.add(new Pair(current.first, current.second));

  return result;
  }
}

",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Very Mild,Very Mild,,Very Mild
20903047,20903047,Every function in seperated js files but included only if used,"<p>Hey all im trying something (because im boring) every function as js files can be used but if only needed file will bi apended. so i have write something like this. some parts from this site.</p>

<p>library.js:</p>

<pre><code>var jsfiles = new Array(
    ""jsLibrary/try.js"",
    ""jsLibrary/try2.js""
);

var loadedJsFiles = [];

function loadFile(filepath,callback){
    var head = document.getElementsByTagName('head')[0];
    var script = document.createElement('script');
    script.type = 'text/javascript';
    script.src = filepath;
    script.onreadystatechange = callback;
    script.onload = callback;
    head.appendChild(script);
}

function loadalljs(callback,jsnum){
    if(!jsnum){jsnum = 0;}
    if(jsnum&lt;jsfiles.length){
        if(!isArrayNode(jsfiles[jsnum],loadedJsFiles)){
            loadedJsFiles.push(jsfiles[jsnum]);
            loadFile(jsfiles[jsnum],function(){
                loadalljs(callback,jsnum+1);    
            });
        }else{
            loadalljs(callback,jsnum+1);
        }
    }else if(jsnum==jsfiles.length){
        callback();
    }
}

function isArrayNode(str, arr) {
    for (var i = 0; i &lt; arr.length; i++) {
        if (str == arr[i]) {
            return true;
        }
    }
    return false;
}

function lb(){
    this.scripts = function(callback,includePageJs){
        loadalljs(function(){
            if(includePageJs){
                loadFile(includePageJs,function(){
                    callback(); 
                })
            }else{
                callback();
            }
        }); 
    };
    this.a = 5;
}

var library = new lb;
</code></pre>

<p>try.js:</p>

<pre><code>function tryfunction(){
    alert(""try"");   
}
</code></pre>

<p>try2.js:</p>

<pre><code>function try2(){
    alert(""try2"");  
}
</code></pre>

<p>mypage.js:</p>

<pre><code>function mypagefunction(){
    alert(""mypagejs"");  
}
</code></pre>

<p>and im using it like this:</p>

<pre><code>&lt;script type=""text/javascript"" src=""jsLibrary/library.js""&gt;&lt;/script&gt;
&lt;script&gt;
    library.scripts(function(){
        tryfunction();
        try2();
        mypagefunction();
    },""jsLibrary/mypagejs.js"");

    library.scripts(function(){
        alert(library.a)
    });
&lt;/script&gt;
</code></pre>

<p>is there any way to do this with easy way? thanks for any idea or help. (Not in jquery please)</p>
","hey all im trying something because im boring every function as js files can be used but if only needed file will bi apended so i have write something like this some parts from this site
libraryjs
tryjs
try2js
mypagejs
and im using it like this
is there any way to do this with easy way thanks for any idea or help not in jquery please
",0,"Hey all im trying something (because im boring) every function as js files can be used but if only needed file will bi apended. so i have write something like this. some parts from this site.
library.js:
var jsfiles = new Array(
    ""jsLibrary/try.js"",
    ""jsLibrary/try2.js""
);

var loadedJsFiles = [];

function loadFile(filepath,callback){
    var head = document.getElementsByTagName('head')[0];
    var script = document.createElement('script');
    script.type = 'text/javascript';
    script.src = filepath;
    script.onreadystatechange = callback;
    script.onload = callback;
    head.appendChild(script);
}

function loadalljs(callback,jsnum){
    if(!jsnum){jsnum = 0;}
    if(jsnum<jsfiles.length){
        if(!isArrayNode(jsfiles[jsnum],loadedJsFiles)){
            loadedJsFiles.push(jsfiles[jsnum]);
            loadFile(jsfiles[jsnum],function(){
                loadalljs(callback,jsnum+1);    
            });
        }else{
            loadalljs(callback,jsnum+1);
        }
    }else if(jsnum==jsfiles.length){
        callback();
    }
}

function isArrayNode(str, arr) {
    for (var i = 0; i < arr.length; i++) {
        if (str == arr[i]) {
            return true;
        }
    }
    return false;
}

function lb(){
    this.scripts = function(callback,includePageJs){
        loadalljs(function(){
            if(includePageJs){
                loadFile(includePageJs,function(){
                    callback(); 
                })
            }else{
                callback();
            }
        }); 
    };
    this.a = 5;
}

var library = new lb;

try.js:
function tryfunction(){
    alert(""try"");   
}

try2.js:
function try2(){
    alert(""try2"");  
}

mypage.js:
function mypagefunction(){
    alert(""mypagejs"");  
}

and im using it like this:
<script type=""text/javascript"" src=""jsLibrary/library.js""></script>
<script>
    library.scripts(function(){
        tryfunction();
        try2();
        mypagefunction();
    },""jsLibrary/mypagejs.js"");

    library.scripts(function(){
        alert(library.a)
    });
</script>

is there any way to do this with easy way? thanks for any idea or help. (Not in jquery please)
","Every function in seperated js files but included only if used
Hey all im trying something (because im boring) every function as js files can be used but if only needed file will bi apended. so i have write something like this. some parts from this site.
library.js:
var jsfiles = new Array(
    ""jsLibrary/try.js"",
    ""jsLibrary/try2.js""
);

var loadedJsFiles = [];

function loadFile(filepath,callback){
    var head = document.getElementsByTagName('head')[0];
    var script = document.createElement('script');
    script.type = 'text/javascript';
    script.src = filepath;
    script.onreadystatechange = callback;
    script.onload = callback;
    head.appendChild(script);
}

function loadalljs(callback,jsnum){
    if(!jsnum){jsnum = 0;}
    if(jsnum<jsfiles.length){
        if(!isArrayNode(jsfiles[jsnum],loadedJsFiles)){
            loadedJsFiles.push(jsfiles[jsnum]);
            loadFile(jsfiles[jsnum],function(){
                loadalljs(callback,jsnum+1);    
            });
        }else{
            loadalljs(callback,jsnum+1);
        }
    }else if(jsnum==jsfiles.length){
        callback();
    }
}

function isArrayNode(str, arr) {
    for (var i = 0; i < arr.length; i++) {
        if (str == arr[i]) {
            return true;
        }
    }
    return false;
}

function lb(){
    this.scripts = function(callback,includePageJs){
        loadalljs(function(){
            if(includePageJs){
                loadFile(includePageJs,function(){
                    callback(); 
                })
            }else{
                callback();
            }
        }); 
    };
    this.a = 5;
}

var library = new lb;

try.js:
function tryfunction(){
    alert(""try"");   
}

try2.js:
function try2(){
    alert(""try2"");  
}

mypage.js:
function mypagefunction(){
    alert(""mypagejs"");  
}

and im using it like this:
<script type=""text/javascript"" src=""jsLibrary/library.js""></script>
<script>
    library.scripts(function(){
        tryfunction();
        try2();
        mypagefunction();
    },""jsLibrary/mypagejs.js"");

    library.scripts(function(){
        alert(library.a)
    });
</script>

is there any way to do this with easy way? thanks for any idea or help. (Not in jquery please)
",0,0,0,1,0,0,1,- Suboptimal code,,,1,1,Web Performance,Load Balancing,,Web Performance,0,0,0,1,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
63448790,63448790,decrease algorithm time on python program calculation,"<p>i have  <strong>algorithm on python</strong> that calculate a <strong>filters</strong> (low_pass,high_pass...) ,  in my program I'm reading data from real time device  and i  need to process it,  and  transmit it back to device.
each chunks of data comes every 10 millisecond, so in this time i must calculate as many filter as i can.
<strong>my algorithm</strong> :</p>
<pre><code>def do_calculation(self,indata,outdata):
  for i in range(0,len(indata)):
    val=self.a0*indata[i]
    val+=self.a1*self.xn1 
    val+=self.a2*self.xn2 
    val-=self.b1*self.yn1 
    val-=self.b2*self.yn2
    outdata[i]= val
    self.xn2 = self.xn1
    self.xn1 = indata[i]
    self.yn2 = self.yn1
    self.yn1 = outdata[i]
</code></pre>
<p>the coefficient(a0,a1,a2,b1,b2) i calculate before i start the read/write.</p>
<p>i call this function from the Main function  with every input that i get from device , process it , and write it back to device by using outdata.
<strong>indata is list in size of 512 like this</strong>
list[[x0,x1][x1,x2].....[x510,x511]]</p>
<p>there is any way that i can <strong>improve performance of this function</strong>? or maybe python is very limitation  about it.
for now, each filter take something like <strong>2 or 3</strong>(there is some filter that take even 5) millisecond and i want to decrease it so i can create more filter in this range of 10 millisecond.</p>
<p>thanks for help!</p>
","i have  algorithm on python that calculate a filters low_passhigh_pass   in my program im reading data from real time device  and i  need to process it  and  transmit it back to device
each chunks of data comes every 10 millisecond so in this time i must calculate as many filter as i can
my algorithm 
the coefficienta0a1a2b1b2 i calculate before i start the readwrite
i call this function from the main function  with every input that i get from device  process it  and write it back to device by using outdata
indata is list in size of 512 like this
listx0x1x1x2x510x511
there is any way that i can improve performance of this function or maybe python is very limitation  about it
for now each filter take something like 2 or 3there is some filter that take even 5 millisecond and i want to decrease it so i can create more filter in this range of 10 millisecond
thanks for help
",0,"i have  algorithm on python that calculate a filters (low_pass,high_pass...) ,  in my program I'm reading data from real time device  and i  need to process it,  and  transmit it back to device.
each chunks of data comes every 10 millisecond, so in this time i must calculate as many filter as i can.
my algorithm :
def do_calculation(self,indata,outdata):
  for i in range(0,len(indata)):
    val=self.a0*indata[i]
    val+=self.a1*self.xn1 
    val+=self.a2*self.xn2 
    val-=self.b1*self.yn1 
    val-=self.b2*self.yn2
    outdata[i]= val
    self.xn2 = self.xn1
    self.xn1 = indata[i]
    self.yn2 = self.yn1
    self.yn1 = outdata[i]

the coefficient(a0,a1,a2,b1,b2) i calculate before i start the read/write.
i call this function from the Main function  with every input that i get from device , process it , and write it back to device by using outdata.
indata is list in size of 512 like this
list[[x0,x1][x1,x2].....[x510,x511]]
there is any way that i can improve performance of this function? or maybe python is very limitation  about it.
for now, each filter take something like 2 or 3(there is some filter that take even 5) millisecond and i want to decrease it so i can create more filter in this range of 10 millisecond.
thanks for help!
","decrease algorithm time on python program calculation
i have  algorithm on python that calculate a filters (low_pass,high_pass...) ,  in my program I'm reading data from real time device  and i  need to process it,  and  transmit it back to device.
each chunks of data comes every 10 millisecond, so in this time i must calculate as many filter as i can.
my algorithm :
def do_calculation(self,indata,outdata):
  for i in range(0,len(indata)):
    val=self.a0*indata[i]
    val+=self.a1*self.xn1 
    val+=self.a2*self.xn2 
    val-=self.b1*self.yn1 
    val-=self.b2*self.yn2
    outdata[i]= val
    self.xn2 = self.xn1
    self.xn1 = indata[i]
    self.yn2 = self.yn1
    self.yn1 = outdata[i]

the coefficient(a0,a1,a2,b1,b2) i calculate before i start the read/write.
i call this function from the Main function  with every input that i get from device , process it , and write it back to device by using outdata.
indata is list in size of 512 like this
list[[x0,x1][x1,x2].....[x510,x511]]
there is any way that i can improve performance of this function? or maybe python is very limitation  about it.
for now, each filter take something like 2 or 3(there is some filter that take even 5) millisecond and i want to decrease it so i can create more filter in this range of 10 millisecond.
thanks for help!
",0,1,0,1,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Architecture,Architecture,,Architecture,Very Severe,Very Severe,,Very Severe
45278602,45278602,I am looking for an efficient approach to concatenate and merge rows in pandas DataFrame with regard to some criteria,"<p>I am working on a code for a data set in pandas. To simplify my question, suppose L_1={2, 5, 8, 19} (elements in L_1 are sorted in ascending order). I am wondering based on L1, how to generate another set L_2 = {(2,5), (2,8), (2,19), (5,8), (5,19), (8,19)}. Then, based on L2, how to again generate another set L_3 = {(2,5,8), (2,5,19), (2,8,19), (5,8,19)}. Again, based on L_3, how to generate L_4 = {(2,5,8,19)}. So, the based idea of generating L_k from L_(k-1) is that</p>

<p>1) each element in L_k is a k-tuple (a_1,...,a_k) such that a_1 &lt; a_2 &lt; ... &lt; a_k</p>

<p>2) if (a_1,...,a_(k-1)) and (b_1,...,b_(k-1)) belong to L_(k-1), then they can be combined to gather if a_i = b_i, i = 1,..., k-2 and a_(k-1) &lt; b_(k-1). This results in ((a_1,...,a_(k-1), b_(k-1)) as a k-tuple element for L_k.</p>

<p>I am wondering how can I generate the above sets by pandas in an efficient way, since L_1 may have more than 100 thousands elements. Thanks!</p>
","i am working on a code for a data set in pandas to simplify my question suppose l_12 5 8 19 elements in l_1 are sorted in ascending order i am wondering based on l1 how to generate another set l_2  25 28 219 58 519 819 then based on l2 how to again generate another set l_3  258 2519 2819 5819 again based on l_3 how to generate l_4  25819 so the based idea of generating l_k from l_k1 is that
1 each element in l_k is a ktuple a_1a_k such that a_1  a_2    a_k
2 if a_1a_k1 and b_1b_k1 belong to l_k1 then they can be combined to gather if a_i  b_i i  1 k2 and a_k1  b_k1 this results in a_1a_k1 b_k1 as a ktuple element for l_k
i am wondering how can i generate the above sets by pandas in an efficient way since l_1 may have more than 100 thousands elements thanks
",1,"I am working on a code for a data set in pandas. To simplify my question, suppose L_1={2, 5, 8, 19} (elements in L_1 are sorted in ascending order). I am wondering based on L1, how to generate another set L_2 = {(2,5), (2,8), (2,19), (5,8), (5,19), (8,19)}. Then, based on L2, how to again generate another set L_3 = {(2,5,8), (2,5,19), (2,8,19), (5,8,19)}. Again, based on L_3, how to generate L_4 = {(2,5,8,19)}. So, the based idea of generating L_k from L_(k-1) is that
1) each element in L_k is a k-tuple (a_1,...,a_k) such that a_1 < a_2 < ... < a_k
2) if (a_1,...,a_(k-1)) and (b_1,...,b_(k-1)) belong to L_(k-1), then they can be combined to gather if a_i = b_i, i = 1,..., k-2 and a_(k-1) < b_(k-1). This results in ((a_1,...,a_(k-1), b_(k-1)) as a k-tuple element for L_k.
I am wondering how can I generate the above sets by pandas in an efficient way, since L_1 may have more than 100 thousands elements. Thanks!
","I am looking for an efficient approach to concatenate and merge rows in pandas DataFrame with regard to some criteria
I am working on a code for a data set in pandas. To simplify my question, suppose L_1={2, 5, 8, 19} (elements in L_1 are sorted in ascending order). I am wondering based on L1, how to generate another set L_2 = {(2,5), (2,8), (2,19), (5,8), (5,19), (8,19)}. Then, based on L2, how to again generate another set L_3 = {(2,5,8), (2,5,19), (2,8,19), (5,8,19)}. Again, based on L_3, how to generate L_4 = {(2,5,8,19)}. So, the based idea of generating L_k from L_(k-1) is that
1) each element in L_k is a k-tuple (a_1,...,a_k) such that a_1 < a_2 < ... < a_k
2) if (a_1,...,a_(k-1)) and (b_1,...,b_(k-1)) belong to L_(k-1), then they can be combined to gather if a_i = b_i, i = 1,..., k-2 and a_(k-1) < b_(k-1). This results in ((a_1,...,a_(k-1), b_(k-1)) as a k-tuple element for L_k.
I am wondering how can I generate the above sets by pandas in an efficient way, since L_1 may have more than 100 thousands elements. Thanks!
",0,1,1,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,1,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
22999828,22999828,How do I simplify the function below?,"<p>I was able to achieve a content switcher with the block of code below but I'm looking for a way to simplify it. There are up to 10 or more topics to switch between, how do I simplify it so that the code wouldn't be too large, instead of having a block of code per DIV.</p>

<pre><code>jQuery(document) .ready(function () {
    $('.topic-intro:not(:nth-of-type(1))') .hide();
    $('#mid-nav-in ul li:nth-of-type(1)') .addClass('active');
    $('#mid-nav-in ul li a:nth-of-type(1)') .click(function () {
        $('.topic-intro:not(:nth-of-type(1))') .hide();
        $('.topic-intro:nth-of-type(1)') .show();
        $('#mid-nav-in ul li:not(:nth-of-type(1))') .removeClass('active');
        $('#mid-nav-in ul li:nth-of-type(1)') .addClass('active');
    });
});
jQuery(document) .ready(function () {
    $('#mid-nav-in ul li:nth-of-type(2) a') .click(function () {
        $('.topic-intro:not(:nth-of-type(2))') .hide();
        $('.topic-intro:nth-of-type(2)') .show();
        $('#mid-nav-in ul li:nth-of-type(2)') .addClass('active');
        $('#mid-nav-in ul li:not(:nth-of-type(2))') .removeClass('active');
    });
});
</code></pre>
","i was able to achieve a content switcher with the block of code below but im looking for a way to simplify it there are up to 10 or more topics to switch between how do i simplify it so that the code wouldnt be too large instead of having a block of code per div
",1,"I was able to achieve a content switcher with the block of code below but I'm looking for a way to simplify it. There are up to 10 or more topics to switch between, how do I simplify it so that the code wouldn't be too large, instead of having a block of code per DIV.
jQuery(document) .ready(function () {
    $('.topic-intro:not(:nth-of-type(1))') .hide();
    $('#mid-nav-in ul li:nth-of-type(1)') .addClass('active');
    $('#mid-nav-in ul li a:nth-of-type(1)') .click(function () {
        $('.topic-intro:not(:nth-of-type(1))') .hide();
        $('.topic-intro:nth-of-type(1)') .show();
        $('#mid-nav-in ul li:not(:nth-of-type(1))') .removeClass('active');
        $('#mid-nav-in ul li:nth-of-type(1)') .addClass('active');
    });
});
jQuery(document) .ready(function () {
    $('#mid-nav-in ul li:nth-of-type(2) a') .click(function () {
        $('.topic-intro:not(:nth-of-type(2))') .hide();
        $('.topic-intro:nth-of-type(2)') .show();
        $('#mid-nav-in ul li:nth-of-type(2)') .addClass('active');
        $('#mid-nav-in ul li:not(:nth-of-type(2))') .removeClass('active');
    });
});

","How do I simplify the function below?
I was able to achieve a content switcher with the block of code below but I'm looking for a way to simplify it. There are up to 10 or more topics to switch between, how do I simplify it so that the code wouldn't be too large, instead of having a block of code per DIV.
jQuery(document) .ready(function () {
    $('.topic-intro:not(:nth-of-type(1))') .hide();
    $('#mid-nav-in ul li:nth-of-type(1)') .addClass('active');
    $('#mid-nav-in ul li a:nth-of-type(1)') .click(function () {
        $('.topic-intro:not(:nth-of-type(1))') .hide();
        $('.topic-intro:nth-of-type(1)') .show();
        $('#mid-nav-in ul li:not(:nth-of-type(1))') .removeClass('active');
        $('#mid-nav-in ul li:nth-of-type(1)') .addClass('active');
    });
});
jQuery(document) .ready(function () {
    $('#mid-nav-in ul li:nth-of-type(2) a') .click(function () {
        $('.topic-intro:not(:nth-of-type(2))') .hide();
        $('.topic-intro:nth-of-type(2)') .show();
        $('#mid-nav-in ul li:nth-of-type(2)') .addClass('active');
        $('#mid-nav-in ul li:not(:nth-of-type(2))') .removeClass('active');
    });
});

",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
27456845,27456845,What's a more efficient way to make sure I'm not stuck in an infinite loop in this quicksort function? (C++),"<p>Here is the relative part of my partition function:</p>

<pre><code>    while(low &lt; high){

      while(arr[low] &lt; pivot)
         low += 1;

      while( arr[high] &gt; pivot)
         high -= 1;

       people temp = arr[low];
       arr[low] = arr[high];
       arr[high] = temp;

       if(L==low &amp;&amp; H==high)
         {low+=1; high -= 1;}
       L=low; H = high;
   }
</code></pre>

<p>I used L and H to see whether or not low or high changed at the end of the enclosing loop. Without them I may get stuck in a loop if I do not increment low or high, but at the same time I need to retest the value after a swap. </p>
","here is the relative part of my partition function
i used l and h to see whether or not low or high changed at the end of the enclosing loop without them i may get stuck in a loop if i do not increment low or high but at the same time i need to retest the value after a swap 
",1,"Here is the relative part of my partition function:
    while(low < high){

      while(arr[low] < pivot)
         low += 1;

      while( arr[high] > pivot)
         high -= 1;

       people temp = arr[low];
       arr[low] = arr[high];
       arr[high] = temp;

       if(L==low && H==high)
         {low+=1; high -= 1;}
       L=low; H = high;
   }

I used L and H to see whether or not low or high changed at the end of the enclosing loop. Without them I may get stuck in a loop if I do not increment low or high, but at the same time I need to retest the value after a swap. 
","What's a more efficient way to make sure I'm not stuck in an infinite loop in this quicksort function? (C++)
Here is the relative part of my partition function:
    while(low < high){

      while(arr[low] < pivot)
         low += 1;

      while( arr[high] > pivot)
         high -= 1;

       people temp = arr[low];
       arr[low] = arr[high];
       arr[high] = temp;

       if(L==low && H==high)
         {low+=1; high -= 1;}
       L=low; H = high;
   }

I used L and H to see whether or not low or high changed at the end of the enclosing loop. Without them I may get stuck in a loop if I do not increment low or high, but at the same time I need to retest the value after a swap. 
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
11002247,11002247,How To Reduce Python Script Memory Usage,"<p>I have a very large python script, 200K, that I would like to use as little memory as possible.  It looks something like:</p>

<pre><code># a lot of data structures
r = [34, 78, 43, 12, 99]

# a lot of functions that I use all the time
def func1(word):
    return len(word) + 2

# a lot of functions that I rarely use
def func1(word):
    return len(word) + 2


# my main loop
while 1:
   # lots of code
   # calls functions
</code></pre>

<p>If I put the functions that I rarely use in a module, and import them dynamically only if necessary, I can't access the data.  That's as far as I've gotten.</p>

<p>I'm new at python.</p>

<p>Can anyone put me on the right path?  How can I break this large script down so that it uses less memory?  Is it worth putting rarely used code in modules and only calling them when needed?</p>
","i have a very large python script 200k that i would like to use as little memory as possible  it looks something like
if i put the functions that i rarely use in a module and import them dynamically only if necessary i cant access the data  thats as far as ive gotten
im new at python
can anyone put me on the right path  how can i break this large script down so that it uses less memory  is it worth putting rarely used code in modules and only calling them when needed
",1,"I have a very large python script, 200K, that I would like to use as little memory as possible.  It looks something like:
# a lot of data structures
r = [34, 78, 43, 12, 99]

# a lot of functions that I use all the time
def func1(word):
    return len(word) + 2

# a lot of functions that I rarely use
def func1(word):
    return len(word) + 2


# my main loop
while 1:
   # lots of code
   # calls functions

If I put the functions that I rarely use in a module, and import them dynamically only if necessary, I can't access the data.  That's as far as I've gotten.
I'm new at python.
Can anyone put me on the right path?  How can I break this large script down so that it uses less memory?  Is it worth putting rarely used code in modules and only calling them when needed?
","How To Reduce Python Script Memory Usage
I have a very large python script, 200K, that I would like to use as little memory as possible.  It looks something like:
# a lot of data structures
r = [34, 78, 43, 12, 99]

# a lot of functions that I use all the time
def func1(word):
    return len(word) + 2

# a lot of functions that I rarely use
def func1(word):
    return len(word) + 2


# my main loop
while 1:
   # lots of code
   # calls functions

If I put the functions that I rarely use in a module, and import them dynamically only if necessary, I can't access the data.  That's as far as I've gotten.
I'm new at python.
Can anyone put me on the right path?  How can I break this large script down so that it uses less memory?  Is it worth putting rarely used code in modules and only calling them when needed?
",0,1,0,1,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,0,1,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
36130102,36130102,How to increase performance of the update operation in Mongo?,"<pre><code>foreach (var doc in await records.Find(filter).ToListAsync())
{
    var query = Builders&lt;JobInfoRecord&gt;.Filter.Eq(""JobTypeValue"", doc.JobTypeValue);
    var updatedJobInfo = Regex.Replace(doc.SerializedBackgroundJobInfo, pattern, ""&lt;$1&gt;&lt;/$1&gt;"");
    var update = Builders&lt;JobInfoRecord&gt;.Update.Set(""SerializedBackgroundJobInfo"", updatedJobInfo);

    records.UpdateOneAsync(query, update).Wait();
}
</code></pre>

<p>Is it the best way to update document (I'm changed a values of tags in xml string that contains <code>password</code> in name to empty tag: â€‹<code>&lt;adminPassword&gt;&lt;/adminPassword&gt;</code> or <a href=""http://regexstorm.net/tester?p=(%3Fsi)%3C(%5B%5E%5Cs%3C%5D*password%5B%5E%5Cs%3C%5D*)%3E.*%3F%3C%2F%5C1%3E&amp;i=%3Cjob%20xmlns%3Ai%3D%22...%22%20xmlns%3D%22...%22%3E%0D%0A%3Cpassword%3Easdfasdf%3C%2Fpassword%3E%0D%0A%3CadminPassword%3Easd%3C%2FadminPassword%3E%0D%0A...%3C%2Fjob%3E&amp;r=%3C%241%3E%3C%2F%241%3E"" rel=""nofollow"">demo</a> )? I'm using Mongo driver 2.0.2</p>

<p>I have a collection with <code>500 000</code> documents, of which I am performing updates each minute (hopefully) of approx. <code>3000</code> documents.</p>

<p>How can I increase performance of the <code>update</code> operation?</p>
","
is it the best way to update document im changed a values of tags in xml string that contains password in name to empty tag adminpasswordadminpassword or demo  im using mongo driver 202
i have a collection with 500 000 documents of which i am performing updates each minute hopefully of approx 3000 documents
how can i increase performance of the update operation
",0,"foreach (var doc in await records.Find(filter).ToListAsync())
{
    var query = Builders<JobInfoRecord>.Filter.Eq(""JobTypeValue"", doc.JobTypeValue);
    var updatedJobInfo = Regex.Replace(doc.SerializedBackgroundJobInfo, pattern, ""<$1></$1>"");
    var update = Builders<JobInfoRecord>.Update.Set(""SerializedBackgroundJobInfo"", updatedJobInfo);

    records.UpdateOneAsync(query, update).Wait();
}

Is it the best way to update document (I'm changed a values of tags in xml string that contains password in name to empty tag: â€‹<adminPassword></adminPassword> or demo )? I'm using Mongo driver 2.0.2
I have a collection with 500 000 documents, of which I am performing updates each minute (hopefully) of approx. 3000 documents.
How can I increase performance of the update operation?
","How to increase performance of the update operation in Mongo?
foreach (var doc in await records.Find(filter).ToListAsync())
{
    var query = Builders<JobInfoRecord>.Filter.Eq(""JobTypeValue"", doc.JobTypeValue);
    var updatedJobInfo = Regex.Replace(doc.SerializedBackgroundJobInfo, pattern, ""<$1></$1>"");
    var update = Builders<JobInfoRecord>.Update.Set(""SerializedBackgroundJobInfo"", updatedJobInfo);

    records.UpdateOneAsync(query, update).Wait();
}

Is it the best way to update document (I'm changed a values of tags in xml string that contains password in name to empty tag: â€‹<adminPassword></adminPassword> or demo )? I'm using Mongo driver 2.0.2
I have a collection with 500 000 documents, of which I am performing updates each minute (hopefully) of approx. 3000 documents.
How can I increase performance of the update operation?
",0,0,0,1,0,0,1,- Suboptimal code ,,,1,1,Database Performance,Database Performance,,Database Performance,0,0,0,1,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
65945086,65945086,C# Optimize Conditional in Loop where input is a value that needs to be between a start/end value found in each list item,"<p><strong>EDIT: This is a general problem when you are stuck in a situation that you have to use a List instead of a Dictionary because instead of searching for a key you may be searching for a range of number like a text minimum and maximum index, and each item may have inner items in them, in my case it added up to 100k+ items before I found a workable solution just after posting this question.</strong></p>
<p><strong>Solution below involves partitioning the list into smaller chunks, much like database indexing.</strong></p>
<p>I call the following a lot in a particular project. It works fine, but there's a problem with this as far as optimization is concerned. I have an index and need to get all text spans where the index is between the start and end, as shown below.</p>
<p><strong>Example:</strong></p>
<pre><code>List&lt;Lexer.Span&gt; spans = new List&lt;Lexer.Span&gt;();
for (int i1 = 0; i1 &lt; input.OrganizedSpans.Count; i1++)
{
   Lexer.Span span1 = input.OrganizedSpans[i1];
   if (index &gt;= span1.Start &amp;&amp; index &lt; span1.End) { spans.Add(span1); }
}
</code></pre>
<p>...</p>
<pre><code>public class Lexer {
...
    public class Span
    {
        public int Start {get;set;}
        public int End {get;set;}
        public string Text {get;set;}
        public List&lt;Span&gt; InnerSpans {get;set;} = new List&lt;Span&gt;();
    }
}
</code></pre>
<p>OrganizedSpans has 6,000+ items stemming from lexemes tokenized of an 18k HTML document, so it gets weighed down significantly doing a straight iteration. I will show you a partitioning optimization in the answer that will help in situations like this. <strong>It brought my execution down from a few seconds to near-instantaneous.</strong></p>
<p>Original: <em>It works, but is a bottleneck according to the Visual Studio Profiler on the if conditional line. It gets called quite a bit from the parser code.
<em>Is there a faster way to do what I want to do here?</em> I've looked at several optimization articles and stackoverflow posts, but am having trouble finding it.</em></p>
<p><strong>UPDATE: Yes there is. :)</strong></p>
","edit this is a general problem when you are stuck in a situation that you have to use a list instead of a dictionary because instead of searching for a key you may be searching for a range of number like a text minimum and maximum index and each item may have inner items in them in my case it added up to 100k items before i found a workable solution just after posting this question
solution below involves partitioning the list into smaller chunks much like database indexing
i call the following a lot in a particular project it works fine but theres a problem with this as far as optimization is concerned i have an index and need to get all text spans where the index is between the start and end as shown below
example

organizedspans has 6000 items stemming from lexemes tokenized of an 18k html document so it gets weighed down significantly doing a straight iteration i will show you a partitioning optimization in the answer that will help in situations like this it brought my execution down from a few seconds to nearinstantaneous
original it works but is a bottleneck according to the visual studio profiler on the if conditional line it gets called quite a bit from the parser code
is there a faster way to do what i want to do here ive looked at several optimization articles and stackoverflow posts but am having trouble finding it
update yes there is 
",1,"EDIT: This is a general problem when you are stuck in a situation that you have to use a List instead of a Dictionary because instead of searching for a key you may be searching for a range of number like a text minimum and maximum index, and each item may have inner items in them, in my case it added up to 100k+ items before I found a workable solution just after posting this question.
Solution below involves partitioning the list into smaller chunks, much like database indexing.
I call the following a lot in a particular project. It works fine, but there's a problem with this as far as optimization is concerned. I have an index and need to get all text spans where the index is between the start and end, as shown below.
Example:
List<Lexer.Span> spans = new List<Lexer.Span>();
for (int i1 = 0; i1 < input.OrganizedSpans.Count; i1++)
{
   Lexer.Span span1 = input.OrganizedSpans[i1];
   if (index >= span1.Start && index < span1.End) { spans.Add(span1); }
}

...
public class Lexer {
...
    public class Span
    {
        public int Start {get;set;}
        public int End {get;set;}
        public string Text {get;set;}
        public List<Span> InnerSpans {get;set;} = new List<Span>();
    }
}

OrganizedSpans has 6,000+ items stemming from lexemes tokenized of an 18k HTML document, so it gets weighed down significantly doing a straight iteration. I will show you a partitioning optimization in the answer that will help in situations like this. It brought my execution down from a few seconds to near-instantaneous.
Original: It works, but is a bottleneck according to the Visual Studio Profiler on the if conditional line. It gets called quite a bit from the parser code.
Is there a faster way to do what I want to do here? I've looked at several optimization articles and stackoverflow posts, but am having trouble finding it.
UPDATE: Yes there is. :)
","C# Optimize Conditional in Loop where input is a value that needs to be between a start/end value found in each list item
EDIT: This is a general problem when you are stuck in a situation that you have to use a List instead of a Dictionary because instead of searching for a key you may be searching for a range of number like a text minimum and maximum index, and each item may have inner items in them, in my case it added up to 100k+ items before I found a workable solution just after posting this question.
Solution below involves partitioning the list into smaller chunks, much like database indexing.
I call the following a lot in a particular project. It works fine, but there's a problem with this as far as optimization is concerned. I have an index and need to get all text spans where the index is between the start and end, as shown below.
Example:
List<Lexer.Span> spans = new List<Lexer.Span>();
for (int i1 = 0; i1 < input.OrganizedSpans.Count; i1++)
{
   Lexer.Span span1 = input.OrganizedSpans[i1];
   if (index >= span1.Start && index < span1.End) { spans.Add(span1); }
}

...
public class Lexer {
...
    public class Span
    {
        public int Start {get;set;}
        public int End {get;set;}
        public string Text {get;set;}
        public List<Span> InnerSpans {get;set;} = new List<Span>();
    }
}

OrganizedSpans has 6,000+ items stemming from lexemes tokenized of an 18k HTML document, so it gets weighed down significantly doing a straight iteration. I will show you a partitioning optimization in the answer that will help in situations like this. It brought my execution down from a few seconds to near-instantaneous.
Original: It works, but is a bottleneck according to the Visual Studio Profiler on the if conditional line. It gets called quite a bit from the parser code.
Is there a faster way to do what I want to do here? I've looked at several optimization articles and stackoverflow posts, but am having trouble finding it.
UPDATE: Yes there is. :)
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
24930690,24930690,C++ operator overload performance issue,"<p>Consider following scheme. We have 3 files:</p>

<p>main.cpp:</p>

<pre><code>int main() {   
    clock_t begin = clock();
    int a = 0;
    for (int i = 0; i &lt; 1000000000; ++i) {
        a += i;
    }
    clock_t end = clock();
    printf(""Number: %d, Elapsed time: %f\n"",
            a, double(end - begin) / CLOCKS_PER_SEC);

    begin = clock();
    C b(0);
    for (int i = 0; i &lt; 1000000000; ++i) {
        b += C(i);
    }
    end = clock();
    printf(""Number: %d, Elapsed time: %f\n"",
            a, double(end - begin) / CLOCKS_PER_SEC);
    return 0;
}
</code></pre>

<p>class.h:</p>

<pre><code>#include &lt;iostream&gt;
struct C {
public:
    int m_number;
    C(int number);
    void operator+=(const C &amp; rhs);
};
</code></pre>

<p>class.cpp</p>

<pre><code>C::C(int number)
: m_number(number)
{
}
void 
C::operator+=(const C &amp; rhs) {
    m_number += rhs.m_number;
}
</code></pre>

<p>Files are compiled using clang++ with flags <code>-std=c++11 -O3</code>.</p>

<p>What I expected were very similar performance results, since I thought that compiler will optimize the operators not to be called as functions. The reality though was a bit different, here is the result:</p>

<pre><code>Number: -1243309312, Elapsed time: 0.000003
Number: -1243309312, Elapsed time: 5.375751
</code></pre>

<p>I played around a bit and found out, that if I paste all of the code from class.* into the main.cpp the speed dramatically improves and results are very similar.</p>

<pre><code>Number: -1243309312, Elapsed time: 0.000003
Number: -1243309312, Elapsed time: 0.000003
</code></pre>

<p>Than I realized that this behavior is probably caused by the fact, that compilation of main.cpp and class.cpp is completely separated and therefore compiler is unable to perform adequate optimizations.</p>

<p>My question: Is there any way of keeping the 3-file scheme and still achieve the optimization level as if the files were merged into one and than compiled? I have read something about 'unity builds' but that seems like an overkill.</p>
","consider following scheme we have 3 files
maincpp
classh
classcpp
files are compiled using clang with flags stdc11 o3
what i expected were very similar performance results since i thought that compiler will optimize the operators not to be called as functions the reality though was a bit different here is the result
i played around a bit and found out that if i paste all of the code from class into the maincpp the speed dramatically improves and results are very similar
than i realized that this behavior is probably caused by the fact that compilation of maincpp and classcpp is completely separated and therefore compiler is unable to perform adequate optimizations
my question is there any way of keeping the 3file scheme and still achieve the optimization level as if the files were merged into one and than compiled i have read something about unity builds but that seems like an overkill
",1,"Consider following scheme. We have 3 files:
main.cpp:
int main() {   
    clock_t begin = clock();
    int a = 0;
    for (int i = 0; i < 1000000000; ++i) {
        a += i;
    }
    clock_t end = clock();
    printf(""Number: %d, Elapsed time: %f\n"",
            a, double(end - begin) / CLOCKS_PER_SEC);

    begin = clock();
    C b(0);
    for (int i = 0; i < 1000000000; ++i) {
        b += C(i);
    }
    end = clock();
    printf(""Number: %d, Elapsed time: %f\n"",
            a, double(end - begin) / CLOCKS_PER_SEC);
    return 0;
}

class.h:
#include <iostream>
struct C {
public:
    int m_number;
    C(int number);
    void operator+=(const C & rhs);
};

class.cpp
C::C(int number)
: m_number(number)
{
}
void 
C::operator+=(const C & rhs) {
    m_number += rhs.m_number;
}

Files are compiled using clang++ with flags -std=c++11 -O3.
What I expected were very similar performance results, since I thought that compiler will optimize the operators not to be called as functions. The reality though was a bit different, here is the result:
Number: -1243309312, Elapsed time: 0.000003
Number: -1243309312, Elapsed time: 5.375751

I played around a bit and found out, that if I paste all of the code from class.* into the main.cpp the speed dramatically improves and results are very similar.
Number: -1243309312, Elapsed time: 0.000003
Number: -1243309312, Elapsed time: 0.000003

Than I realized that this behavior is probably caused by the fact, that compilation of main.cpp and class.cpp is completely separated and therefore compiler is unable to perform adequate optimizations.
My question: Is there any way of keeping the 3-file scheme and still achieve the optimization level as if the files were merged into one and than compiled? I have read something about 'unity builds' but that seems like an overkill.
","C++ operator overload performance issue
Consider following scheme. We have 3 files:
main.cpp:
int main() {   
    clock_t begin = clock();
    int a = 0;
    for (int i = 0; i < 1000000000; ++i) {
        a += i;
    }
    clock_t end = clock();
    printf(""Number: %d, Elapsed time: %f\n"",
            a, double(end - begin) / CLOCKS_PER_SEC);

    begin = clock();
    C b(0);
    for (int i = 0; i < 1000000000; ++i) {
        b += C(i);
    }
    end = clock();
    printf(""Number: %d, Elapsed time: %f\n"",
            a, double(end - begin) / CLOCKS_PER_SEC);
    return 0;
}

class.h:
#include <iostream>
struct C {
public:
    int m_number;
    C(int number);
    void operator+=(const C & rhs);
};

class.cpp
C::C(int number)
: m_number(number)
{
}
void 
C::operator+=(const C & rhs) {
    m_number += rhs.m_number;
}

Files are compiled using clang++ with flags -std=c++11 -O3.
What I expected were very similar performance results, since I thought that compiler will optimize the operators not to be called as functions. The reality though was a bit different, here is the result:
Number: -1243309312, Elapsed time: 0.000003
Number: -1243309312, Elapsed time: 5.375751

I played around a bit and found out, that if I paste all of the code from class.* into the main.cpp the speed dramatically improves and results are very similar.
Number: -1243309312, Elapsed time: 0.000003
Number: -1243309312, Elapsed time: 0.000003

Than I realized that this behavior is probably caused by the fact, that compilation of main.cpp and class.cpp is completely separated and therefore compiler is unable to perform adequate optimizations.
My question: Is there any way of keeping the 3-file scheme and still achieve the optimization level as if the files were merged into one and than compiled? I have read something about 'unity builds' but that seems like an overkill.
",1,1,0,0,0,0,1,,,,1,1,Compiler and Compilation Strategies,Compiler and Compilation Strategies,,Compiler and Compilation Strategies,1,1,0,0,0,0,,,Build,Build,,Build,Severe,Severe,,Severe
4408521,4408521,How will increasing each memory allocation size by a fixed number of bytes affect heap fragmentation?,"<p>I have <code>operator new()</code> replaced in my C++ program so that it <a href=""https://stackoverflow.com/questions/4134195/how-do-i-call-the-original-operator-new-if-i-have-overloaded-it"">allocates a slightly bigger block to store extra data</a>. So the program performs exactly the same set of allocations except that now it requests several bytes more memory in each allocation. Otherwise its behavior is completely the same and it processes exactly same data. The program allocates lots of blocks (millions, I suppose) of various sizes during its runtime.</p>

<p>How will increasing each allocation size by a fixed number of bytes (same for every allocation) affect heap fragmentation?</p>
","i have operator new replaced in my c program so that it allocates a slightly bigger block to store extra data so the program performs exactly the same set of allocations except that now it requests several bytes more memory in each allocation otherwise its behavior is completely the same and it processes exactly same data the program allocates lots of blocks millions i suppose of various sizes during its runtime
how will increasing each allocation size by a fixed number of bytes same for every allocation affect heap fragmentation
",1,"I have operator new() replaced in my C++ program so that it allocates a slightly bigger block to store extra data. So the program performs exactly the same set of allocations except that now it requests several bytes more memory in each allocation. Otherwise its behavior is completely the same and it processes exactly same data. The program allocates lots of blocks (millions, I suppose) of various sizes during its runtime.
How will increasing each allocation size by a fixed number of bytes (same for every allocation) affect heap fragmentation?
","How will increasing each memory allocation size by a fixed number of bytes affect heap fragmentation?
I have operator new() replaced in my C++ program so that it allocates a slightly bigger block to store extra data. So the program performs exactly the same set of allocations except that now it requests several bytes more memory in each allocation. Otherwise its behavior is completely the same and it processes exactly same data. The program allocates lots of blocks (millions, I suppose) of various sizes during its runtime.
How will increasing each allocation size by a fixed number of bytes (same for every allocation) affect heap fragmentation?
",0,0,1,1,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,1,0,0,,,Infrastructure,Infrastructure,,Infrastructure,None,None,,None
49508373,49508373,"ASP.NET MVC several DropDownListFor, page too slow. How to late bind or group?","<p>I'm facing with a very hard problem (for me).</p>

<p>I have a page in ASP.NET MVC with a list of contracts.</p>

<p>For each contract I have 3 DropDown (Contract, Payee, Currency)</p>

<p>These dropdown are used to change the details of the contracts.</p>

<p>The dropdowns are full of options, but they are always the same for each row.</p>

<p>What I obtain is a page very heavy to load, and it freeze the browser while loads all the dropdown lists.</p>

<p>Do you kindly know a way to bind all the dropdowns to the same list of options?</p>

<p>this is part of the code:</p>

<pre><code> @foreach (Models.ContractsModel CM in Model)
          { ...
&lt;div class=""form-group""&gt;
&lt;label class=""hidden-lg""&gt;Site&lt;/label&gt;
@Html.DropDownListFor(model =&gt; CM.Site, StudyGlobalProperties.FillDropDownList(...), """", new { @class = ""form-control"" })
</code></pre>

<p>Thank you very much for all your help.
KR</p>
","im facing with a very hard problem for me
i have a page in aspnet mvc with a list of contracts
for each contract i have 3 dropdown contract payee currency
these dropdown are used to change the details of the contracts
the dropdowns are full of options but they are always the same for each row
what i obtain is a page very heavy to load and it freeze the browser while loads all the dropdown lists
do you kindly know a way to bind all the dropdowns to the same list of options
this is part of the code
thank you very much for all your help
kr
",1,"I'm facing with a very hard problem (for me).
I have a page in ASP.NET MVC with a list of contracts.
For each contract I have 3 DropDown (Contract, Payee, Currency)
These dropdown are used to change the details of the contracts.
The dropdowns are full of options, but they are always the same for each row.
What I obtain is a page very heavy to load, and it freeze the browser while loads all the dropdown lists.
Do you kindly know a way to bind all the dropdowns to the same list of options?
this is part of the code:
 @foreach (Models.ContractsModel CM in Model)
          { ...
<div class=""form-group"">
<label class=""hidden-lg"">Site</label>
@Html.DropDownListFor(model => CM.Site, StudyGlobalProperties.FillDropDownList(...), """", new { @class = ""form-control"" })

Thank you very much for all your help.
KR
","ASP.NET MVC several DropDownListFor, page too slow. How to late bind or group?
I'm facing with a very hard problem (for me).
I have a page in ASP.NET MVC with a list of contracts.
For each contract I have 3 DropDown (Contract, Payee, Currency)
These dropdown are used to change the details of the contracts.
The dropdowns are full of options, but they are always the same for each row.
What I obtain is a page very heavy to load, and it freeze the browser while loads all the dropdown lists.
Do you kindly know a way to bind all the dropdowns to the same list of options?
this is part of the code:
 @foreach (Models.ContractsModel CM in Model)
          { ...
<div class=""form-group"">
<label class=""hidden-lg"">Site</label>
@Html.DropDownListFor(model => CM.Site, StudyGlobalProperties.FillDropDownList(...), """", new { @class = ""form-control"" })

Thank you very much for all your help.
KR
",1,1,0,0,0,0,1,,,,1,1,Web Performance,Web Performance,,Web Performance,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
10784951,10784951,Do any JVM's JIT compilers generate code that uses vectorized floating point instructions?,"<p>Let's say the bottleneck of my Java program really is some tight loops to compute a bunch of vector dot products. Yes I've profiled, yes it's the bottleneck, yes it's significant, yes that's just how the algorithm is, yes I've run Proguard to optimize the byte code, etc.</p>

<p>The work is, essentially, dot products. As in, I have two <code>float[50]</code> and I need to compute the sum of pairwise products. I know processor instruction sets exist to perform these kind of operations quickly and in bulk, like SSE or MMX.</p>

<p>Yes I can probably access these by writing some native code in JNI. The JNI call turns out to  be pretty expensive.</p>

<p>I know you can't guarantee what a JIT will compile or not compile. Has anyone <em>ever</em> heard of a JIT generating code that uses these instructions? and if so, is there anything about the Java code that helps make it compilable this way?</p>

<p>Probably a ""no""; worth asking.</p>
","lets say the bottleneck of my java program really is some tight loops to compute a bunch of vector dot products yes ive profiled yes its the bottleneck yes its significant yes thats just how the algorithm is yes ive run proguard to optimize the byte code etc
the work is essentially dot products as in i have two float50 and i need to compute the sum of pairwise products i know processor instruction sets exist to perform these kind of operations quickly and in bulk like sse or mmx
yes i can probably access these by writing some native code in jni the jni call turns out to  be pretty expensive
i know you cant guarantee what a jit will compile or not compile has anyone ever heard of a jit generating code that uses these instructions and if so is there anything about the java code that helps make it compilable this way
probably a no worth asking
",1,"Let's say the bottleneck of my Java program really is some tight loops to compute a bunch of vector dot products. Yes I've profiled, yes it's the bottleneck, yes it's significant, yes that's just how the algorithm is, yes I've run Proguard to optimize the byte code, etc.
The work is, essentially, dot products. As in, I have two float[50] and I need to compute the sum of pairwise products. I know processor instruction sets exist to perform these kind of operations quickly and in bulk, like SSE or MMX.
Yes I can probably access these by writing some native code in JNI. The JNI call turns out to  be pretty expensive.
I know you can't guarantee what a JIT will compile or not compile. Has anyone ever heard of a JIT generating code that uses these instructions? and if so, is there anything about the Java code that helps make it compilable this way?
Probably a ""no""; worth asking.
","Do any JVM's JIT compilers generate code that uses vectorized floating point instructions?
Let's say the bottleneck of my Java program really is some tight loops to compute a bunch of vector dot products. Yes I've profiled, yes it's the bottleneck, yes it's significant, yes that's just how the algorithm is, yes I've run Proguard to optimize the byte code, etc.
The work is, essentially, dot products. As in, I have two float[50] and I need to compute the sum of pairwise products. I know processor instruction sets exist to perform these kind of operations quickly and in bulk, like SSE or MMX.
Yes I can probably access these by writing some native code in JNI. The JNI call turns out to  be pretty expensive.
I know you can't guarantee what a JIT will compile or not compile. Has anyone ever heard of a JIT generating code that uses these instructions? and if so, is there anything about the Java code that helps make it compilable this way?
Probably a ""no""; worth asking.
",0,1,1,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,1,0,0,0,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
56687992,56687992,What could cause a return statement not to work in an if code block,"<p>I am trying to compare two binary trees to see if they are equal in structure and value, but at some point in the algorithm, a return statement doesn't work when placed in an if code block for just the values i want to compare.
i.e</p>

<pre><code>let a = ""myVal""
let b = ""myVal""

if(a = b){
    return false
}
</code></pre>

<p>The above doesn't work with only the variabes i want to compare but works well with every other variable.</p>

<p>I have checked the type and value of both variables and they are indeed equal. </p>

<p>Also, when i throw an error for within the if code block, it works when th e condition is met but a <code>return</code> statement doesn't just work.</p>

<p>Here's the full code </p>

<pre><code>function compare(a, b){
    if(a === null &amp;&amp; b === null){
        return true;
    }
    if(typeof a === 'object' &amp;&amp; typeof b === 'object'){
        // compare their structures
        let aRoot = Object.keys(a);
        let bRoot = Object.keys(b);
        if(aRoot.length !== bRoot.length){
            console.log('0')
            return false; //Trees are of different structures
        }

        //Loop through the roots of the tree
        for(let i in aRoot){
            if(aRoot[i] !== bRoot[i]){
                //Make sure the roots are represented with equal names
                console.log('1')
                return false;
            }
            let aValue = a[aRoot[i]];
            let bValue = b[bRoot[i]];
            if(typeof aValue !== typeof bValue){
                console.log('2')
                return false
            }
            if(aValue !== null &amp;&amp; bValue !== null){
                //If they are both of the same types compare their values check if they are child nodes or not
                if(typeof aValue !== 'object'){
                    //Here's the main problem
                    if(aValue !== bValue){
                        // console.log(""aValue : "", aValue, "" bValue : "", bValue)
                        // console.log(""aValue type : "", typeof aValue, "" bValue type : "", typeof bValue)
                        return false;
                    }
                }
                else{
                    // console.log('a ', aValue)
                    compare(aValue, bValue);
                }
            }
        }
    }
    return true;
}


let aNode = {val: 1, left: null, right: null, d: {val: 1, left: null, right: null, f: {val: 2, left: null, right: null}}};

let bNode = {val: 1, left: null, right: null, d: {val: 3/* This differs from aNode*/, left: null, right: null, f: {val: 2, left: null, right: null}}};

console.log(compare(aNode, bNode))

</code></pre>

<p>It works with trees with no root nodes, but doesn't with trees with root nodes.</p>

<p>The problem arises from the <code>if(aValue !== bValue)</code> part of the code. That code block doesn't just return false but when an error is thrown, it works.</p>
","i am trying to compare two binary trees to see if they are equal in structure and value but at some point in the algorithm a return statement doesnt work when placed in an if code block for just the values i want to compare
ie
the above doesnt work with only the variabes i want to compare but works well with every other variable
i have checked the type and value of both variables and they are indeed equal 
also when i throw an error for within the if code block it works when th e condition is met but a return statement doesnt just work
heres the full code 
it works with trees with no root nodes but doesnt with trees with root nodes
the problem arises from the ifavalue  bvalue part of the code that code block doesnt just return false but when an error is thrown it works
",1,"I am trying to compare two binary trees to see if they are equal in structure and value, but at some point in the algorithm, a return statement doesn't work when placed in an if code block for just the values i want to compare.
i.e
let a = ""myVal""
let b = ""myVal""

if(a = b){
    return false
}

The above doesn't work with only the variabes i want to compare but works well with every other variable.
I have checked the type and value of both variables and they are indeed equal. 
Also, when i throw an error for within the if code block, it works when th e condition is met but a return statement doesn't just work.
Here's the full code 
function compare(a, b){
    if(a === null && b === null){
        return true;
    }
    if(typeof a === 'object' && typeof b === 'object'){
        // compare their structures
        let aRoot = Object.keys(a);
        let bRoot = Object.keys(b);
        if(aRoot.length !== bRoot.length){
            console.log('0')
            return false; //Trees are of different structures
        }

        //Loop through the roots of the tree
        for(let i in aRoot){
            if(aRoot[i] !== bRoot[i]){
                //Make sure the roots are represented with equal names
                console.log('1')
                return false;
            }
            let aValue = a[aRoot[i]];
            let bValue = b[bRoot[i]];
            if(typeof aValue !== typeof bValue){
                console.log('2')
                return false
            }
            if(aValue !== null && bValue !== null){
                //If they are both of the same types compare their values check if they are child nodes or not
                if(typeof aValue !== 'object'){
                    //Here's the main problem
                    if(aValue !== bValue){
                        // console.log(""aValue : "", aValue, "" bValue : "", bValue)
                        // console.log(""aValue type : "", typeof aValue, "" bValue type : "", typeof bValue)
                        return false;
                    }
                }
                else{
                    // console.log('a ', aValue)
                    compare(aValue, bValue);
                }
            }
        }
    }
    return true;
}


let aNode = {val: 1, left: null, right: null, d: {val: 1, left: null, right: null, f: {val: 2, left: null, right: null}}};

let bNode = {val: 1, left: null, right: null, d: {val: 3/* This differs from aNode*/, left: null, right: null, f: {val: 2, left: null, right: null}}};

console.log(compare(aNode, bNode))


It works with trees with no root nodes, but doesn't with trees with root nodes.
The problem arises from the if(aValue !== bValue) part of the code. That code block doesn't just return false but when an error is thrown, it works.
","What could cause a return statement not to work in an if code block
I am trying to compare two binary trees to see if they are equal in structure and value, but at some point in the algorithm, a return statement doesn't work when placed in an if code block for just the values i want to compare.
i.e
let a = ""myVal""
let b = ""myVal""

if(a = b){
    return false
}

The above doesn't work with only the variabes i want to compare but works well with every other variable.
I have checked the type and value of both variables and they are indeed equal. 
Also, when i throw an error for within the if code block, it works when th e condition is met but a return statement doesn't just work.
Here's the full code 
function compare(a, b){
    if(a === null && b === null){
        return true;
    }
    if(typeof a === 'object' && typeof b === 'object'){
        // compare their structures
        let aRoot = Object.keys(a);
        let bRoot = Object.keys(b);
        if(aRoot.length !== bRoot.length){
            console.log('0')
            return false; //Trees are of different structures
        }

        //Loop through the roots of the tree
        for(let i in aRoot){
            if(aRoot[i] !== bRoot[i]){
                //Make sure the roots are represented with equal names
                console.log('1')
                return false;
            }
            let aValue = a[aRoot[i]];
            let bValue = b[bRoot[i]];
            if(typeof aValue !== typeof bValue){
                console.log('2')
                return false
            }
            if(aValue !== null && bValue !== null){
                //If they are both of the same types compare their values check if they are child nodes or not
                if(typeof aValue !== 'object'){
                    //Here's the main problem
                    if(aValue !== bValue){
                        // console.log(""aValue : "", aValue, "" bValue : "", bValue)
                        // console.log(""aValue type : "", typeof aValue, "" bValue type : "", typeof bValue)
                        return false;
                    }
                }
                else{
                    // console.log('a ', aValue)
                    compare(aValue, bValue);
                }
            }
        }
    }
    return true;
}


let aNode = {val: 1, left: null, right: null, d: {val: 1, left: null, right: null, f: {val: 2, left: null, right: null}}};

let bNode = {val: 1, left: null, right: null, d: {val: 3/* This differs from aNode*/, left: null, right: null, f: {val: 2, left: null, right: null}}};

console.log(compare(aNode, bNode))


It works with trees with no root nodes, but doesn't with trees with root nodes.
The problem arises from the if(aValue !== bValue) part of the code. That code block doesn't just return false but when an error is thrown, it works.
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
16425046,16425046,how do I parallelize a simple python def with multiple argument,"<p>I would like to parallelize a python script. I've create a definition:</p>

<pre><code>def dummy(list1,list2):
  do usefull calculations ...
</code></pre>

<p>The list1 and list2 contain a list of files name that I should read and then make calculation with them. The files are independent. The list1 and 2 contain the same number of argument. </p>

<p>Let us assume I have 2 cpus (I want to impose the number of cpus to be used). I would like the first cpu to call the definition with a list that contain only the first half of list1 and list2 and at the same time the second cpu should call the same def dummy with the second half of list1 and list2. </p>

<p>Something like:</p>

<pre><code>import multiprocessing
nb_cpus = 2
pool = multiprocessing.Pool(processes=nb_cpus)
for ii in nb_cpus:
  list_half1 = list1[0:max/nb_cpus]
  list_half2 = list2[0:max/nb_cpus]
  result[ii] = pool.map(dummy,list_half1,list_half2)
</code></pre>

<p>The problem is that the pool.map can only work if the def has 1 argument and that I cannot loop over cpus.</p>

<p>Thank you for any help on that problem !</p>

<p>PS: It is not possible for me to concatenate the two arguments into one because in the real case I'm having I'm passing much more arguments. </p>
","i would like to parallelize a python script ive create a definition
the list1 and list2 contain a list of files name that i should read and then make calculation with them the files are independent the list1 and 2 contain the same number of argument 
let us assume i have 2 cpus i want to impose the number of cpus to be used i would like the first cpu to call the definition with a list that contain only the first half of list1 and list2 and at the same time the second cpu should call the same def dummy with the second half of list1 and list2 
something like
the problem is that the poolmap can only work if the def has 1 argument and that i cannot loop over cpus
thank you for any help on that problem 
ps it is not possible for me to concatenate the two arguments into one because in the real case im having im passing much more arguments 
",1,"I would like to parallelize a python script. I've create a definition:
def dummy(list1,list2):
  do usefull calculations ...

The list1 and list2 contain a list of files name that I should read and then make calculation with them. The files are independent. The list1 and 2 contain the same number of argument. 
Let us assume I have 2 cpus (I want to impose the number of cpus to be used). I would like the first cpu to call the definition with a list that contain only the first half of list1 and list2 and at the same time the second cpu should call the same def dummy with the second half of list1 and list2. 
Something like:
import multiprocessing
nb_cpus = 2
pool = multiprocessing.Pool(processes=nb_cpus)
for ii in nb_cpus:
  list_half1 = list1[0:max/nb_cpus]
  list_half2 = list2[0:max/nb_cpus]
  result[ii] = pool.map(dummy,list_half1,list_half2)

The problem is that the pool.map can only work if the def has 1 argument and that I cannot loop over cpus.
Thank you for any help on that problem !
PS: It is not possible for me to concatenate the two arguments into one because in the real case I'm having I'm passing much more arguments. 
","how do I parallelize a simple python def with multiple argument
I would like to parallelize a python script. I've create a definition:
def dummy(list1,list2):
  do usefull calculations ...

The list1 and list2 contain a list of files name that I should read and then make calculation with them. The files are independent. The list1 and 2 contain the same number of argument. 
Let us assume I have 2 cpus (I want to impose the number of cpus to be used). I would like the first cpu to call the definition with a list that contain only the first half of list1 and list2 and at the same time the second cpu should call the same def dummy with the second half of list1 and list2. 
Something like:
import multiprocessing
nb_cpus = 2
pool = multiprocessing.Pool(processes=nb_cpus)
for ii in nb_cpus:
  list_half1 = list1[0:max/nb_cpus]
  list_half2 = list2[0:max/nb_cpus]
  result[ii] = pool.map(dummy,list_half1,list_half2)

The problem is that the pool.map can only work if the def has 1 argument and that I cannot loop over cpus.
Thank you for any help on that problem !
PS: It is not possible for me to concatenate the two arguments into one because in the real case I'm having I'm passing much more arguments. 
",0,1,0,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,0,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
17726407,17726407,Re assign a list efficiently,"<p>This is a <code>MWE</code> of the re-arrainging I need to do:</p>

<pre><code>a = [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]
b = [[], [], []]

for item in a:
    b[0].append(item[0])    
    b[1].append(item[1])
    b[2].append(item[2])
</code></pre>

<p>which makes <code>b</code> lool like this:</p>

<pre><code>b = [[1, 4, 7, 10], [2, 5, 8, 11], [3, 6, 9, 12]]
</code></pre>

<p>I.e., every first item in every list inside <code>a</code> will be stored in the first list in <code>b</code> and the same for lists two and three in <code>b</code>.</p>

<p>I need to apply this to a somewhat big <code>a</code> list, is there a more efficient way to do this?</p>
","this is a mwe of the rearrainging i need to do
which makes b lool like this
ie every first item in every list inside a will be stored in the first list in b and the same for lists two and three in b
i need to apply this to a somewhat big a list is there a more efficient way to do this
",0,"This is a MWE of the re-arrainging I need to do:
a = [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]
b = [[], [], []]

for item in a:
    b[0].append(item[0])    
    b[1].append(item[1])
    b[2].append(item[2])

which makes b lool like this:
b = [[1, 4, 7, 10], [2, 5, 8, 11], [3, 6, 9, 12]]

I.e., every first item in every list inside a will be stored in the first list in b and the same for lists two and three in b.
I need to apply this to a somewhat big a list, is there a more efficient way to do this?
","Re assign a list efficiently
This is a MWE of the re-arrainging I need to do:
a = [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]
b = [[], [], []]

for item in a:
    b[0].append(item[0])    
    b[1].append(item[1])
    b[2].append(item[2])

which makes b lool like this:
b = [[1, 4, 7, 10], [2, 5, 8, 11], [3, 6, 9, 12]]

I.e., every first item in every list inside a will be stored in the first list in b and the same for lists two and three in b.
I need to apply this to a somewhat big a list, is there a more efficient way to do this?
",0,0,0,1,0,0,1,- Suboptimal code ,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,0,0,1,0,0,,,Infrastructure,Code,User’s code functionality is fine but is much more complicated than necessary,Code,Mild,Mild,,Mild
24262590,24262590,Comparing three ways of inserting dom elements + enhancing them with jQuery mobile,"<p>I have perfomance issues when inserting data into the dom. </p>

<p>Insertions + jQuery mobile enhancement done on pagecontainerbeforeshow() event,
jQuery mobile version 1.4.2.</p>

<p>I have tried to see which would be the fastest approach by comparing three simplified versions of what I want to do:</p>

<p>jQuery approach :</p>

<pre><code>for(var i=0;i&lt;2000;++i){
    $('&lt;div&gt;').attr({'data-role':'collapsible','id':'asdf'+i+''}).html('&lt;h2&gt;asdf&lt;/h2&gt;').appendTo(""#manage_content"");
    $('&lt;ul&gt;').attr({'data-role':'listview'}).html('&lt;li&gt;bit&lt;/li&gt;').appendTo('#asdf'+i+'');
    }
$('#manage').trigger('create');
</code></pre>

<p>Pure js, creating all nodes :</p>

<pre><code>var d=document.createDocumentFragment();
var title,listitem,list;
var coll=new Array();
for(var i=0;i&lt;2000;++i){
coll[i]=document.createElement('div');
coll[i].setAttribute(""data-role"", ""collapsible"");
title = document.createElement('h2');
title.innerHTML='asdf';
coll[i].appendChild(title);
list=document.createElement('ul');
list.setAttribute(""data-role"",""listview"");
listitem = document.createElement('li');
listitem.innerHTML='bit';
list.appendChild(listitem);
coll[i].appendChild(list);
d.appendChild(coll[i]);
}
document.getElementById(""manage_content"").appendChild(d);
$('#manage').trigger('create');
</code></pre>

<p>jQuery with big string :</p>

<pre><code>var html='';
for(var i=0;i&lt;2000;++i){
    html+='&lt;div data-role=""collapsible""&gt;&lt;h2&gt;asdf&lt;h2&gt;&lt;ul data-role=""listview""&gt;&lt;li&gt;bit&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;';
}
$('#manage_content').append(html);
$('#manage').trigger('create');
</code></pre>

<p>To my surprise, the three ways of doing this give same result (around 7seconds execution time...) </p>

<p>Am I doing any of the ways wrong? Is there a better way? </p>

<p>I have seen lots of questions related to this topic but mostly outdated or stating that the pure javascript should be faster, which is not the case for me.</p>

<h3> Without jQuery enhancement : </h3>

<p><li>jQuery standard : ~400ms
<li>Pure JS : ~40ms
<li>jQuery big string : ~80ms <br>
So pure javascript with document fragments is the best even though it is horrible to read/write x_x </p>

<h3> With jQuery enhancement : </h3>

<p><a href=""http://jsperf.com/collapsible"" rel=""nofollow"">Test</a> here (credits to @Omar)</p>
","i have perfomance issues when inserting data into the dom 
insertions  jquery mobile enhancement done on pagecontainerbeforeshow event
jquery mobile version 142
i have tried to see which would be the fastest approach by comparing three simplified versions of what i want to do
jquery approach 
pure js creating all nodes 
jquery with big string 
to my surprise the three ways of doing this give same result around 7seconds execution time 
am i doing any of the ways wrong is there a better way 
i have seen lots of questions related to this topic but mostly outdated or stating that the pure javascript should be faster which is not the case for me
 without jquery enhancement  
jquery standard  400ms
pure js  40ms
jquery big string  80ms 
so pure javascript with document fragments is the best even though it is horrible to readwrite x_x 
 with jquery enhancement  
test here credits to omar
",1,"I have perfomance issues when inserting data into the dom. 
Insertions + jQuery mobile enhancement done on pagecontainerbeforeshow() event,
jQuery mobile version 1.4.2.
I have tried to see which would be the fastest approach by comparing three simplified versions of what I want to do:
jQuery approach :
for(var i=0;i<2000;++i){
    $('<div>').attr({'data-role':'collapsible','id':'asdf'+i+''}).html('<h2>asdf</h2>').appendTo(""#manage_content"");
    $('<ul>').attr({'data-role':'listview'}).html('<li>bit</li>').appendTo('#asdf'+i+'');
    }
$('#manage').trigger('create');

Pure js, creating all nodes :
var d=document.createDocumentFragment();
var title,listitem,list;
var coll=new Array();
for(var i=0;i<2000;++i){
coll[i]=document.createElement('div');
coll[i].setAttribute(""data-role"", ""collapsible"");
title = document.createElement('h2');
title.innerHTML='asdf';
coll[i].appendChild(title);
list=document.createElement('ul');
list.setAttribute(""data-role"",""listview"");
listitem = document.createElement('li');
listitem.innerHTML='bit';
list.appendChild(listitem);
coll[i].appendChild(list);
d.appendChild(coll[i]);
}
document.getElementById(""manage_content"").appendChild(d);
$('#manage').trigger('create');

jQuery with big string :
var html='';
for(var i=0;i<2000;++i){
    html+='<div data-role=""collapsible""><h2>asdf<h2><ul data-role=""listview""><li>bit</li></ul></div>';
}
$('#manage_content').append(html);
$('#manage').trigger('create');

To my surprise, the three ways of doing this give same result (around 7seconds execution time...) 
Am I doing any of the ways wrong? Is there a better way? 
I have seen lots of questions related to this topic but mostly outdated or stating that the pure javascript should be faster, which is not the case for me.
 Without jQuery enhancement : 
jQuery standard : ~400ms
Pure JS : ~40ms
jQuery big string : ~80ms 
So pure javascript with document fragments is the best even though it is horrible to read/write x_x 
 With jQuery enhancement : 
Test here (credits to @Omar)
","Comparing three ways of inserting dom elements + enhancing them with jQuery mobile
I have perfomance issues when inserting data into the dom. 
Insertions + jQuery mobile enhancement done on pagecontainerbeforeshow() event,
jQuery mobile version 1.4.2.
I have tried to see which would be the fastest approach by comparing three simplified versions of what I want to do:
jQuery approach :
for(var i=0;i<2000;++i){
    $('<div>').attr({'data-role':'collapsible','id':'asdf'+i+''}).html('<h2>asdf</h2>').appendTo(""#manage_content"");
    $('<ul>').attr({'data-role':'listview'}).html('<li>bit</li>').appendTo('#asdf'+i+'');
    }
$('#manage').trigger('create');

Pure js, creating all nodes :
var d=document.createDocumentFragment();
var title,listitem,list;
var coll=new Array();
for(var i=0;i<2000;++i){
coll[i]=document.createElement('div');
coll[i].setAttribute(""data-role"", ""collapsible"");
title = document.createElement('h2');
title.innerHTML='asdf';
coll[i].appendChild(title);
list=document.createElement('ul');
list.setAttribute(""data-role"",""listview"");
listitem = document.createElement('li');
listitem.innerHTML='bit';
list.appendChild(listitem);
coll[i].appendChild(list);
d.appendChild(coll[i]);
}
document.getElementById(""manage_content"").appendChild(d);
$('#manage').trigger('create');

jQuery with big string :
var html='';
for(var i=0;i<2000;++i){
    html+='<div data-role=""collapsible""><h2>asdf<h2><ul data-role=""listview""><li>bit</li></ul></div>';
}
$('#manage_content').append(html);
$('#manage').trigger('create');

To my surprise, the three ways of doing this give same result (around 7seconds execution time...) 
Am I doing any of the ways wrong? Is there a better way? 
I have seen lots of questions related to this topic but mostly outdated or stating that the pure javascript should be faster, which is not the case for me.
 Without jQuery enhancement : 
jQuery standard : ~400ms
Pure JS : ~40ms
jQuery big string : ~80ms 
So pure javascript with document fragments is the best even though it is horrible to read/write x_x 
 With jQuery enhancement : 
Test here (credits to @Omar)
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Architecture,Code,"User’s code is poorly written and more complicating than necessary requiring a refactor, but the underlying functionality is acceptable",Architecture,Mild,Mild,,Mild
28838276,28838276,How to process a large amount of in Java,"<p>I have a requirement where I need to check each row of a value which is available in Database or Not in a Java Batch.</p>

<p>Here in my requirement,</p>

<p>I have a file which contains 200 rows of data at present perhaps this file might grow or shrink later for each n everyday.</p>

<p>The requirement here, i need to Insert and Update the reference table by checking the data from the file.</p>

<p><strong>Example File</strong></p>

<pre><code>+++++++++++++++++++++++++++++++++++++++++

Emp No  Emp Name

1        Name-1   
2        Name-2
3        Name-3
4        Name-4
5        Name-5
6        Name-6
7        Name-7
8        Name-8
,,..... it goes on
+++++++++++++++++++++++++++++++++++++++++++++++++++++
</code></pre>

<p>I have a reference table (Standalone Table) which contains 2000 rows of data.</p>

<p><strong>For Example Emp Table</strong></p>

<pre><code>******************************************************
Emp No Emp Name
1      Name-1
2      Name-2
3      Name-3
4      Name-4
5      Name-5
6      Name-6
7      Name-7
8      Name-8
,,..... it goes on
******************************************************
</code></pre>

<p>Currently a program is written in Java, where the data is fetched from the reference table and added to a Map with Emp Number as Key and Emp Details as value (Map). </p>

<p>Once it is added, then the data in file checked with Map using the key whether required data is available. if available then the data is updated or adding the new value to the Map.</p>

<p>Then finally batch insert is done.</p>

<p><strong><em>Here's my question whether this approach is good?</em></strong> </p>

<p>Since I don't want to check the data with table each time, since i hope it is costly operation as the connection has to be open for long time. </p>

<p>But there is another costly thing, as far as now we have 2000 rows of data in the table this may be grow a large part of it even 10,000 (no surprise in it). Now here the issue hold large amount of data in the Map and processing it would be another hectic one right.</p>

<hr>

<p>As both hits the performance issue in later stage</p>

<p><strong><em>What is the best way to manipulate such kind of requirement, is there any algorithm.</em></strong></p>

<hr>

<p>Thanks in advance, could somebody give me a right direction in it.</p>
","i have a requirement where i need to check each row of a value which is available in database or not in a java batch
here in my requirement
i have a file which contains 200 rows of data at present perhaps this file might grow or shrink later for each n everyday
the requirement here i need to insert and update the reference table by checking the data from the file
example file
i have a reference table standalone table which contains 2000 rows of data
for example emp table
currently a program is written in java where the data is fetched from the reference table and added to a map with emp number as key and emp details as value map 
once it is added then the data in file checked with map using the key whether required data is available if available then the data is updated or adding the new value to the map
then finally batch insert is done
heres my question whether this approach is good 
since i dont want to check the data with table each time since i hope it is costly operation as the connection has to be open for long time 
but there is another costly thing as far as now we have 2000 rows of data in the table this may be grow a large part of it even 10000 no surprise in it now here the issue hold large amount of data in the map and processing it would be another hectic one right

as both hits the performance issue in later stage
what is the best way to manipulate such kind of requirement is there any algorithm

thanks in advance could somebody give me a right direction in it
",1,"I have a requirement where I need to check each row of a value which is available in Database or Not in a Java Batch.
Here in my requirement,
I have a file which contains 200 rows of data at present perhaps this file might grow or shrink later for each n everyday.
The requirement here, i need to Insert and Update the reference table by checking the data from the file.
Example File
+++++++++++++++++++++++++++++++++++++++++

Emp No  Emp Name

1        Name-1   
2        Name-2
3        Name-3
4        Name-4
5        Name-5
6        Name-6
7        Name-7
8        Name-8
,,..... it goes on
+++++++++++++++++++++++++++++++++++++++++++++++++++++

I have a reference table (Standalone Table) which contains 2000 rows of data.
For Example Emp Table
******************************************************
Emp No Emp Name
1      Name-1
2      Name-2
3      Name-3
4      Name-4
5      Name-5
6      Name-6
7      Name-7
8      Name-8
,,..... it goes on
******************************************************

Currently a program is written in Java, where the data is fetched from the reference table and added to a Map with Emp Number as Key and Emp Details as value (Map). 
Once it is added, then the data in file checked with Map using the key whether required data is available. if available then the data is updated or adding the new value to the Map.
Then finally batch insert is done.
Here's my question whether this approach is good? 
Since I don't want to check the data with table each time, since i hope it is costly operation as the connection has to be open for long time. 
But there is another costly thing, as far as now we have 2000 rows of data in the table this may be grow a large part of it even 10,000 (no surprise in it). Now here the issue hold large amount of data in the Map and processing it would be another hectic one right.

As both hits the performance issue in later stage
What is the best way to manipulate such kind of requirement, is there any algorithm.

Thanks in advance, could somebody give me a right direction in it.
","How to process a large amount of in Java
I have a requirement where I need to check each row of a value which is available in Database or Not in a Java Batch.
Here in my requirement,
I have a file which contains 200 rows of data at present perhaps this file might grow or shrink later for each n everyday.
The requirement here, i need to Insert and Update the reference table by checking the data from the file.
Example File
+++++++++++++++++++++++++++++++++++++++++

Emp No  Emp Name

1        Name-1   
2        Name-2
3        Name-3
4        Name-4
5        Name-5
6        Name-6
7        Name-7
8        Name-8
,,..... it goes on
+++++++++++++++++++++++++++++++++++++++++++++++++++++

I have a reference table (Standalone Table) which contains 2000 rows of data.
For Example Emp Table
******************************************************
Emp No Emp Name
1      Name-1
2      Name-2
3      Name-3
4      Name-4
5      Name-5
6      Name-6
7      Name-7
8      Name-8
,,..... it goes on
******************************************************

Currently a program is written in Java, where the data is fetched from the reference table and added to a Map with Emp Number as Key and Emp Details as value (Map). 
Once it is added, then the data in file checked with Map using the key whether required data is available. if available then the data is updated or adding the new value to the Map.
Then finally batch insert is done.
Here's my question whether this approach is good? 
Since I don't want to check the data with table each time, since i hope it is costly operation as the connection has to be open for long time. 
But there is another costly thing, as far as now we have 2000 rows of data in the table this may be grow a large part of it even 10,000 (no surprise in it). Now here the issue hold large amount of data in the Map and processing it would be another hectic one right.

As both hits the performance issue in later stage
What is the best way to manipulate such kind of requirement, is there any algorithm.

Thanks in advance, could somebody give me a right direction in it.
",0,1,0,1,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,0,1,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
22191176,22191176,Python Pandas performance,"<p>Someone told if you are looping in python you are doing something wrong, and I tend to agree with that, so I did some perf. analysis on my program and I'm surprised by the results:</p>

<p>I'm trying to retrieving the indexes of the non-Nan data of a pandas Series with dropna(), and it seems to be slower than looping :</p>

<pre><code>from pandas import Series
import numpy as np
import timeit

def test1():
    s = Series([25.9,25.8,np.nan,34.8],index=['a','b','c','d'])
    return s.dropna().index

def test2():
    s = Series([25.9,25.8,np.nan,34.8],index=['a','b','c','d'])
    res = []
    for i in s.index:
        if not np.isnan(s[i]):
            res.append(i)
    return res


&gt;&gt;&gt; timeit.timeit(test1,number=10000)
1.931797840017623
&gt;&gt;&gt; timeit.timeit(test2,number=10000)
1.602180508842423
</code></pre>

<hr>

<p>Am I missing something here? Or it is just because I'm returning an array instead of pandas index?</p>

<p>Thanks in advance</p>
","someone told if you are looping in python you are doing something wrong and i tend to agree with that so i did some perf analysis on my program and im surprised by the results
im trying to retrieving the indexes of the nonnan data of a pandas series with dropna and it seems to be slower than looping 

am i missing something here or it is just because im returning an array instead of pandas index
thanks in advance
",1,"Someone told if you are looping in python you are doing something wrong, and I tend to agree with that, so I did some perf. analysis on my program and I'm surprised by the results:
I'm trying to retrieving the indexes of the non-Nan data of a pandas Series with dropna(), and it seems to be slower than looping :
from pandas import Series
import numpy as np
import timeit

def test1():
    s = Series([25.9,25.8,np.nan,34.8],index=['a','b','c','d'])
    return s.dropna().index

def test2():
    s = Series([25.9,25.8,np.nan,34.8],index=['a','b','c','d'])
    res = []
    for i in s.index:
        if not np.isnan(s[i]):
            res.append(i)
    return res


>>> timeit.timeit(test1,number=10000)
1.931797840017623
>>> timeit.timeit(test2,number=10000)
1.602180508842423


Am I missing something here? Or it is just because I'm returning an array instead of pandas index?
Thanks in advance
","Python Pandas performance
Someone told if you are looping in python you are doing something wrong, and I tend to agree with that, so I did some perf. analysis on my program and I'm surprised by the results:
I'm trying to retrieving the indexes of the non-Nan data of a pandas Series with dropna(), and it seems to be slower than looping :
from pandas import Series
import numpy as np
import timeit

def test1():
    s = Series([25.9,25.8,np.nan,34.8],index=['a','b','c','d'])
    return s.dropna().index

def test2():
    s = Series([25.9,25.8,np.nan,34.8],index=['a','b','c','d'])
    res = []
    for i in s.index:
        if not np.isnan(s[i]):
            res.append(i)
    return res


>>> timeit.timeit(test1,number=10000)
1.931797840017623
>>> timeit.timeit(test2,number=10000)
1.602180508842423


Am I missing something here? Or it is just because I'm returning an array instead of pandas index?
Thanks in advance
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Very Mild,Very Mild,,Very Mild
37348557,37348557,Loop not vectorized due to reason '1300',"<p>I am trying to vectorize a loop that does a lot of  iteration (over 300 000 000) to gain some computation time:</p>

<pre><code>uint16_t* samples = (uint16_t*)pixmap-&gt;samples;

Image image(pixmap-&gt;w, pixmap-&gt;h);
uint8_t *dest = (uint8_t*)image[0];

for (int x = 0; x &lt; len; x++)
{
    dest[x] = samples[x] &amp; 0xFF;
}
</code></pre>

<p>But qvec-report say it could not be vectorized due to reason 1300.</p>

<p>According to the <a href=""https://msdn.microsoft.com/en-us/library/jj658585(v=vs.110).aspx"" rel=""nofollow"">MSDN</a>, reason 1300 is :</p>

<blockquote>
  <p>Loop body contains noâ€”or very littleâ€”computation.</p>
</blockquote>

<p>Indeed, my loop body does very little computation, but since there is a lot of iteration, it still take some time.</p>

<p>Why does the vectorization is not done in that case ? Is it because it would not be worth it ? If yes, why ?</p>

<p>If no, is there any way or any tricks to ""force"" it ?</p>
","i am trying to vectorize a loop that does a lot of  iteration over 300 000 000 to gain some computation time
but qvecreport say it could not be vectorized due to reason 1300
according to the msdn reason 1300 is 

loop body contains noor very littlecomputation

indeed my loop body does very little computation but since there is a lot of iteration it still take some time
why does the vectorization is not done in that case  is it because it would not be worth it  if yes why 
if no is there any way or any tricks to force it 
",1,"I am trying to vectorize a loop that does a lot of  iteration (over 300 000 000) to gain some computation time:
uint16_t* samples = (uint16_t*)pixmap->samples;

Image image(pixmap->w, pixmap->h);
uint8_t *dest = (uint8_t*)image[0];

for (int x = 0; x < len; x++)
{
    dest[x] = samples[x] & 0xFF;
}

But qvec-report say it could not be vectorized due to reason 1300.
According to the MSDN, reason 1300 is :

Loop body contains noâ€”or very littleâ€”computation.

Indeed, my loop body does very little computation, but since there is a lot of iteration, it still take some time.
Why does the vectorization is not done in that case ? Is it because it would not be worth it ? If yes, why ?
If no, is there any way or any tricks to ""force"" it ?
","Loop not vectorized due to reason '1300'
I am trying to vectorize a loop that does a lot of  iteration (over 300 000 000) to gain some computation time:
uint16_t* samples = (uint16_t*)pixmap->samples;

Image image(pixmap->w, pixmap->h);
uint8_t *dest = (uint8_t*)image[0];

for (int x = 0; x < len; x++)
{
    dest[x] = samples[x] & 0xFF;
}

But qvec-report say it could not be vectorized due to reason 1300.
According to the MSDN, reason 1300 is :

Loop body contains noâ€”or very littleâ€”computation.

Indeed, my loop body does very little computation, but since there is a lot of iteration, it still take some time.
Why does the vectorization is not done in that case ? Is it because it would not be worth it ? If yes, why ?
If no, is there any way or any tricks to ""force"" it ?
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
60882975,60882975,How to avoid async function in promise executor,"<p>I am trying to understand Promises in Javascript. I have a situation where my code has some ""nested"" promises, and when installing ESLint I was made aware that async functions in Promise executors is considered an anti-pattern. I have code along the lines of:</p>

<pre class=""lang-js prettyprint-override""><code>const main = (indata) =&gt; {
  heavyOperation(indata).then( (res) =&gt; {
    console.log(res);
  }).catch( err =&gt; {
    console.err(err);
  }).finally( () =&gt; {
    cleanUp();
  }
}

async const heavyOperation = (indata) =&gt; {
  return new Promise( (resolve, reject) =&gt; {
    preProcess(indata);
    try {
      const result = await calculate(indata);
    } catch (err) {
      console.err('Error in calculation.');
      reject(err);
    }      
    const processedResult = postProcess(result);
    resolve(processedResult);
  }
}

const preProcess = (indata) =&gt; { 
  //preprocessing
}

async const calculate = () =&gt; {
  // do stuff
}

const postProcess = (result) =&gt; { 
  //postprocessing
}

main(indata);
</code></pre>

<p>How can I refactor this code? Should I get rid of <code>heavyOperation</code> returning the promise entirely? How can I then achieve the <code>.then</code>, <code>.catch</code>, <code>.error</code> functionality?</p>
","i am trying to understand promises in javascript i have a situation where my code has some nested promises and when installing eslint i was made aware that async functions in promise executors is considered an antipattern i have code along the lines of
const main  indata  
  heavyoperationindatathen res  
    consolelogres
  catch err  
    consoleerrerr
  finally   
    cleanup
  


async const heavyoperation  indata  
  return new promise resolve reject  
    preprocessindata
    try 
      const result  await calculateindata
     catch err 
      consoleerrerror in calculation
      rejecterr
          
    const processedresult  postprocessresult
    resolveprocessedresult
  


const preprocess  indata   
  preprocessing


async const calculate    
   do stuff


const postprocess  result   
  postprocessing


mainindata

how can i refactor this code should i get rid of heavyoperation returning the promise entirely how can i then achieve the then catch error functionality
",1,"I am trying to understand Promises in Javascript. I have a situation where my code has some ""nested"" promises, and when installing ESLint I was made aware that async functions in Promise executors is considered an anti-pattern. I have code along the lines of:
const main = (indata) => {
  heavyOperation(indata).then( (res) => {
    console.log(res);
  }).catch( err => {
    console.err(err);
  }).finally( () => {
    cleanUp();
  }
}

async const heavyOperation = (indata) => {
  return new Promise( (resolve, reject) => {
    preProcess(indata);
    try {
      const result = await calculate(indata);
    } catch (err) {
      console.err('Error in calculation.');
      reject(err);
    }      
    const processedResult = postProcess(result);
    resolve(processedResult);
  }
}

const preProcess = (indata) => { 
  //preprocessing
}

async const calculate = () => {
  // do stuff
}

const postProcess = (result) => { 
  //postprocessing
}

main(indata);

How can I refactor this code? Should I get rid of heavyOperation returning the promise entirely? How can I then achieve the .then, .catch, .error functionality?
","How to avoid async function in promise executor
I am trying to understand Promises in Javascript. I have a situation where my code has some ""nested"" promises, and when installing ESLint I was made aware that async functions in Promise executors is considered an anti-pattern. I have code along the lines of:
const main = (indata) => {
  heavyOperation(indata).then( (res) => {
    console.log(res);
  }).catch( err => {
    console.err(err);
  }).finally( () => {
    cleanUp();
  }
}

async const heavyOperation = (indata) => {
  return new Promise( (resolve, reject) => {
    preProcess(indata);
    try {
      const result = await calculate(indata);
    } catch (err) {
      console.err('Error in calculation.');
      reject(err);
    }      
    const processedResult = postProcess(result);
    resolve(processedResult);
  }
}

const preProcess = (indata) => { 
  //preprocessing
}

async const calculate = () => {
  // do stuff
}

const postProcess = (result) => { 
  //postprocessing
}

main(indata);

How can I refactor this code? Should I get rid of heavyOperation returning the promise entirely? How can I then achieve the .then, .catch, .error functionality?
",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
67464255,67464255,How to stay dry while connecting to database with every fucntion?,"<p>I am working on discordpy bot, I connect and disconnect to database in every single function, I don't think this is a good method but I tried using decorators to connect and disconnect to the database and sending the cursor as a parameter to the function.
<br/>
here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>def connectToDB(func):
    async def wrapper(*args, **kwargs):
        db = sqlite3.connect(DB_DIR)
        cursor = db.cursor()
        print(*args, **kwargs)

        res = await func(*args, **kwargs, cursor=cursor)

        db.commit()
        cursor.close()
        db.close()
        return res

    return wrapper


class MusicBot(commands.Cog):
    def __init__(self, bot):
        self.bot = bot

    @commands.command(name=&quot;myplaylist&quot;, aliases=['mypl'])
    @connectToDB
    async def myPlaylist(self, ctx, name=None, currentPage=0, cursor=None):
        currentPage = int(currentPage)
        if name:
            await self.getPlaylistByName(ctx, name, cursor)
            return

        result = cursor.execute(f&quot;SELECT playlist_name, playlist_items, playlist_length, date&quot;
                                f&quot; FROM PLAYLIST WHERE user = ?&quot;, (str(ctx.author),))
        playlists = result.fetchall()

        embed = getPlaylistsEmbed(ctx, playlists, currentPage)
        await ctx.send(embed=embed)

def setup(bot):
    bot.add_cog(MusicBot(bot))
</code></pre>
<p>But this does not optmize the code, because discordpy must know what parameter do you want for the command, and so in wrapper I have said <code>def wrapper(*args, **kwargs):</code> which doesn't specify any parameter so discordpy assumes I only need the default parameters.
<br/><br/>
To solve this I have tried:</p>
<ol>
<li>Using exec to change the needed parameters with each run, and inspect to get the parameters, like so:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>def connectToDB(f):
    global func
    func = f
    params = inspect.signature(func)
    exec(
       f&quot;async def wrapper{str(params)}:&quot;
        &quot;    db = sqlite3.connect(DB_DIR)&quot;
        &quot;    cursor = db.cursor()&quot;
        &quot;    print(*args, **kwargs)&quot;
       f&quot;    res = await func{str(params)}&quot;
        &quot;    db.commit()&quot;
        &quot;    cursor.close()&quot;
        &quot;    db.close()&quot;
        &quot;    return res&quot;, globals()
    )

    return wrapper
</code></pre>
<p>this dosen't work, because after running for each command in the end it will stay with its last change.<br/>
Â Â Â 2. asigning the decorators like this</p>
<pre class=""lang-py prettyprint-override""><code>@connectToDB
@commands.command(name=&quot;myplaylist&quot;, aliases=['mypl'])
</code></pre>
<p>Â Â Â 3. this one is quite stupid. Making a decorator factory to get the params</p>
<pre class=""lang-py prettyprint-override""><code>def connectToDB(*args, **kwargs):
    print(args, kwargs)

    def decorator(func):
        async def wrapper(*_args, **_kwargs):
            db = sqlite3.connect(DB_DIR)
            cursor = db.cursor()
            print(*_args, **_kwargs)

            res = await func(*_args, **_kwargs, cursor=cursor)

            db.commit()
            cursor.close()
            db.close()
            return res

        return wrapper

    return decorator


@commands.command(name=&quot;myplaylist&quot;, aliases=['mypl'])
@connectToDB(self=None, ctx=None, name=None, page=0, msg=None, cursor=None)
async def myPlaylist(self, ctx, name=None, page=0, msg=None, cursor=None):
   ...
</code></pre>
","i am working on discordpy bot i connect and disconnect to database in every single function i dont think this is a good method but i tried using decorators to connect and disconnect to the database and sending the cursor as a parameter to the function

here is my code
def connecttodbfunc
    async def wrapperargs kwargs
        db  sqlite3connectdb_dir
        cursor  dbcursor
        printargs kwargs

        res  await funcargs kwargs cursorcursor

        dbcommit
        cursorclose
        dbclose
        return res

    return wrapper


class musicbotcommandscog
    def __init__self bot
        selfbot  bot

    commandscommandnamemyplaylist aliasesmypl
    connecttodb
    async def myplaylistself ctx namenone currentpage0 cursornone
        currentpage  intcurrentpage
        if name
            await selfgetplaylistbynamectx name cursor
            return

        result  cursorexecutefselect playlist_name playlist_items playlist_length date
                                f from playlist where user   strctxauthor
        playlists  resultfetchall

        embed  getplaylistsembedctx playlists currentpage
        await ctxsendembedembed

def setupbot
    botadd_cogmusicbotbot

but this does not optmize the code because discordpy must know what parameter do you want for the command and so in wrapper i have said def wrapperargs kwargs which doesnt specify any parameter so discordpy assumes i only need the default parameters

to solve this i have tried

using exec to change the needed parameters with each run and inspect to get the parameters like so

def connecttodbf
    global func
    func  f
    params  inspectsignaturefunc
    exec
       fasync def wrapperstrparams
            db  sqlite3connectdb_dir
            cursor  dbcursor
            printargs kwargs
       f    res  await funcstrparams
            dbcommit
            cursorclose
            dbclose
            return res globals
    

    return wrapper

this dosent work because after running for each command in the end it will stay with its last change
Â Â Â 2 asigning the decorators like this
connecttodb
commandscommandnamemyplaylist aliasesmypl

Â Â Â 3 this one is quite stupid making a decorator factory to get the params
def connecttodbargs kwargs
    printargs kwargs

    def decoratorfunc
        async def wrapper_args _kwargs
            db  sqlite3connectdb_dir
            cursor  dbcursor
            print_args _kwargs

            res  await func_args _kwargs cursorcursor

            dbcommit
            cursorclose
            dbclose
            return res

        return wrapper

    return decorator


commandscommandnamemyplaylist aliasesmypl
connecttodbselfnone ctxnone namenone page0 msgnone cursornone
async def myplaylistself ctx namenone page0 msgnone cursornone
   

",1,"I am working on discordpy bot, I connect and disconnect to database in every single function, I don't think this is a good method but I tried using decorators to connect and disconnect to the database and sending the cursor as a parameter to the function.

here is my code:
def connectToDB(func):
    async def wrapper(*args, **kwargs):
        db = sqlite3.connect(DB_DIR)
        cursor = db.cursor()
        print(*args, **kwargs)

        res = await func(*args, **kwargs, cursor=cursor)

        db.commit()
        cursor.close()
        db.close()
        return res

    return wrapper


class MusicBot(commands.Cog):
    def __init__(self, bot):
        self.bot = bot

    @commands.command(name=""myplaylist"", aliases=['mypl'])
    @connectToDB
    async def myPlaylist(self, ctx, name=None, currentPage=0, cursor=None):
        currentPage = int(currentPage)
        if name:
            await self.getPlaylistByName(ctx, name, cursor)
            return

        result = cursor.execute(f""SELECT playlist_name, playlist_items, playlist_length, date""
                                f"" FROM PLAYLIST WHERE user = ?"", (str(ctx.author),))
        playlists = result.fetchall()

        embed = getPlaylistsEmbed(ctx, playlists, currentPage)
        await ctx.send(embed=embed)

def setup(bot):
    bot.add_cog(MusicBot(bot))

But this does not optmize the code, because discordpy must know what parameter do you want for the command, and so in wrapper I have said def wrapper(*args, **kwargs): which doesn't specify any parameter so discordpy assumes I only need the default parameters.

To solve this I have tried:

Using exec to change the needed parameters with each run, and inspect to get the parameters, like so:

def connectToDB(f):
    global func
    func = f
    params = inspect.signature(func)
    exec(
       f""async def wrapper{str(params)}:""
        ""    db = sqlite3.connect(DB_DIR)""
        ""    cursor = db.cursor()""
        ""    print(*args, **kwargs)""
       f""    res = await func{str(params)}""
        ""    db.commit()""
        ""    cursor.close()""
        ""    db.close()""
        ""    return res"", globals()
    )

    return wrapper

this dosen't work, because after running for each command in the end it will stay with its last change.
Â Â Â 2. asigning the decorators like this
@connectToDB
@commands.command(name=""myplaylist"", aliases=['mypl'])

Â Â Â 3. this one is quite stupid. Making a decorator factory to get the params
def connectToDB(*args, **kwargs):
    print(args, kwargs)

    def decorator(func):
        async def wrapper(*_args, **_kwargs):
            db = sqlite3.connect(DB_DIR)
            cursor = db.cursor()
            print(*_args, **_kwargs)

            res = await func(*_args, **_kwargs, cursor=cursor)

            db.commit()
            cursor.close()
            db.close()
            return res

        return wrapper

    return decorator


@commands.command(name=""myplaylist"", aliases=['mypl'])
@connectToDB(self=None, ctx=None, name=None, page=0, msg=None, cursor=None)
async def myPlaylist(self, ctx, name=None, page=0, msg=None, cursor=None):
   ...

","How to stay dry while connecting to database with every fucntion?
I am working on discordpy bot, I connect and disconnect to database in every single function, I don't think this is a good method but I tried using decorators to connect and disconnect to the database and sending the cursor as a parameter to the function.

here is my code:
def connectToDB(func):
    async def wrapper(*args, **kwargs):
        db = sqlite3.connect(DB_DIR)
        cursor = db.cursor()
        print(*args, **kwargs)

        res = await func(*args, **kwargs, cursor=cursor)

        db.commit()
        cursor.close()
        db.close()
        return res

    return wrapper


class MusicBot(commands.Cog):
    def __init__(self, bot):
        self.bot = bot

    @commands.command(name=""myplaylist"", aliases=['mypl'])
    @connectToDB
    async def myPlaylist(self, ctx, name=None, currentPage=0, cursor=None):
        currentPage = int(currentPage)
        if name:
            await self.getPlaylistByName(ctx, name, cursor)
            return

        result = cursor.execute(f""SELECT playlist_name, playlist_items, playlist_length, date""
                                f"" FROM PLAYLIST WHERE user = ?"", (str(ctx.author),))
        playlists = result.fetchall()

        embed = getPlaylistsEmbed(ctx, playlists, currentPage)
        await ctx.send(embed=embed)

def setup(bot):
    bot.add_cog(MusicBot(bot))

But this does not optmize the code, because discordpy must know what parameter do you want for the command, and so in wrapper I have said def wrapper(*args, **kwargs): which doesn't specify any parameter so discordpy assumes I only need the default parameters.

To solve this I have tried:

Using exec to change the needed parameters with each run, and inspect to get the parameters, like so:

def connectToDB(f):
    global func
    func = f
    params = inspect.signature(func)
    exec(
       f""async def wrapper{str(params)}:""
        ""    db = sqlite3.connect(DB_DIR)""
        ""    cursor = db.cursor()""
        ""    print(*args, **kwargs)""
       f""    res = await func{str(params)}""
        ""    db.commit()""
        ""    cursor.close()""
        ""    db.close()""
        ""    return res"", globals()
    )

    return wrapper

this dosen't work, because after running for each command in the end it will stay with its last change.
Â Â Â 2. asigning the decorators like this
@connectToDB
@commands.command(name=""myplaylist"", aliases=['mypl'])

Â Â Â 3. this one is quite stupid. Making a decorator factory to get the params
def connectToDB(*args, **kwargs):
    print(args, kwargs)

    def decorator(func):
        async def wrapper(*_args, **_kwargs):
            db = sqlite3.connect(DB_DIR)
            cursor = db.cursor()
            print(*_args, **_kwargs)

            res = await func(*_args, **_kwargs, cursor=cursor)

            db.commit()
            cursor.close()
            db.close()
            return res

        return wrapper

    return decorator


@commands.command(name=""myplaylist"", aliases=['mypl'])
@connectToDB(self=None, ctx=None, name=None, page=0, msg=None, cursor=None)
async def myPlaylist(self, ctx, name=None, page=0, msg=None, cursor=None):
   ...

",1,1,0,0,0,0,1,,,,1,1,Database Performance,Database Performance,,Database Performance,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
3011141,3011141,How to cancel a deeply nested process,"<p>I have a class that is a ""manager"" sort of class.  One of it's functions is to signal that the long running process of the class should shut down.  It does this by setting a boolean called ""IsStopping"" in class.  </p>

<pre><code>public class Foo
{
    bool isStoping

    void DoWork() {
        while (!isStopping)
        {
            // do work...
        }
    }
}
</code></pre>

<p>Now, DoWork() was a gigantic function, and I decided to refactor it out and as part of the process broke some of it into other classes.  The problem is, Some of these classes also have long running functions that need to check if isStopping is true.  </p>

<pre><code>public class Foo
{
    bool isStoping

    void DoWork() {
        while (!isStopping)
        {
            MoreWork mw = new MoreWork()
            mw.DoMoreWork() // possibly long running
            // do work...
        }
    }
}
</code></pre>

<p>What are my options here?</p>

<p>I have considered passing isStopping by reference, which I don't really like because it requires there to be an outside object.  I would prefer to make the additional classes as stand alone and dependancy free as possible.</p>

<p>I have also considered making isStopping a property, and then then having it call an event that the inner classes could be subscribed to, but this seems overly complex.  </p>

<p>Another option was to create a ""Process Cancelation Token"" class, similar to what .net 4 Tasks use, then that token be passed to those classes.</p>

<p>How have you handled this situation?  </p>

<p>EDIT:</p>

<p>Also consider that MoreWork might have a EvenMoreWork object that it instantiates and calls a potentially long running method on... and so on.  I guess what i'm looking for is a way to be able to signal an arbitrary number of objects down a call tree to tell them to stop what they're doing and clean up and return.</p>

<p>EDIT2:</p>

<p>Thanks for the responses so far.  Seems like there's no real consensus on methods to use, and everyone has a different opinion.  Seems like this should be a design pattern...</p>
","i have a class that is a manager sort of class  one of its functions is to signal that the long running process of the class should shut down  it does this by setting a boolean called isstopping in class  
now dowork was a gigantic function and i decided to refactor it out and as part of the process broke some of it into other classes  the problem is some of these classes also have long running functions that need to check if isstopping is true  
what are my options here
i have considered passing isstopping by reference which i dont really like because it requires there to be an outside object  i would prefer to make the additional classes as stand alone and dependancy free as possible
i have also considered making isstopping a property and then then having it call an event that the inner classes could be subscribed to but this seems overly complex  
another option was to create a process cancelation token class similar to what net 4 tasks use then that token be passed to those classes
how have you handled this situation  
edit
also consider that morework might have a evenmorework object that it instantiates and calls a potentially long running method on and so on  i guess what im looking for is a way to be able to signal an arbitrary number of objects down a call tree to tell them to stop what theyre doing and clean up and return
edit2
thanks for the responses so far  seems like theres no real consensus on methods to use and everyone has a different opinion  seems like this should be a design pattern
",1,"I have a class that is a ""manager"" sort of class.  One of it's functions is to signal that the long running process of the class should shut down.  It does this by setting a boolean called ""IsStopping"" in class.  
public class Foo
{
    bool isStoping

    void DoWork() {
        while (!isStopping)
        {
            // do work...
        }
    }
}

Now, DoWork() was a gigantic function, and I decided to refactor it out and as part of the process broke some of it into other classes.  The problem is, Some of these classes also have long running functions that need to check if isStopping is true.  
public class Foo
{
    bool isStoping

    void DoWork() {
        while (!isStopping)
        {
            MoreWork mw = new MoreWork()
            mw.DoMoreWork() // possibly long running
            // do work...
        }
    }
}

What are my options here?
I have considered passing isStopping by reference, which I don't really like because it requires there to be an outside object.  I would prefer to make the additional classes as stand alone and dependancy free as possible.
I have also considered making isStopping a property, and then then having it call an event that the inner classes could be subscribed to, but this seems overly complex.  
Another option was to create a ""Process Cancelation Token"" class, similar to what .net 4 Tasks use, then that token be passed to those classes.
How have you handled this situation?  
EDIT:
Also consider that MoreWork might have a EvenMoreWork object that it instantiates and calls a potentially long running method on... and so on.  I guess what i'm looking for is a way to be able to signal an arbitrary number of objects down a call tree to tell them to stop what they're doing and clean up and return.
EDIT2:
Thanks for the responses so far.  Seems like there's no real consensus on methods to use, and everyone has a different opinion.  Seems like this should be a design pattern...
","How to cancel a deeply nested process
I have a class that is a ""manager"" sort of class.  One of it's functions is to signal that the long running process of the class should shut down.  It does this by setting a boolean called ""IsStopping"" in class.  
public class Foo
{
    bool isStoping

    void DoWork() {
        while (!isStopping)
        {
            // do work...
        }
    }
}

Now, DoWork() was a gigantic function, and I decided to refactor it out and as part of the process broke some of it into other classes.  The problem is, Some of these classes also have long running functions that need to check if isStopping is true.  
public class Foo
{
    bool isStoping

    void DoWork() {
        while (!isStopping)
        {
            MoreWork mw = new MoreWork()
            mw.DoMoreWork() // possibly long running
            // do work...
        }
    }
}

What are my options here?
I have considered passing isStopping by reference, which I don't really like because it requires there to be an outside object.  I would prefer to make the additional classes as stand alone and dependancy free as possible.
I have also considered making isStopping a property, and then then having it call an event that the inner classes could be subscribed to, but this seems overly complex.  
Another option was to create a ""Process Cancelation Token"" class, similar to what .net 4 Tasks use, then that token be passed to those classes.
How have you handled this situation?  
EDIT:
Also consider that MoreWork might have a EvenMoreWork object that it instantiates and calls a potentially long running method on... and so on.  I guess what i'm looking for is a way to be able to signal an arbitrary number of objects down a call tree to tell them to stop what they're doing and clean up and return.
EDIT2:
Thanks for the responses so far.  Seems like there's no real consensus on methods to use, and everyone has a different opinion.  Seems like this should be a design pattern...
",0,1,1,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,0,1,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
27554068,27554068,Faster way to copy C array with calculation between,"<p>I want to copy an C array data to another, but with a calculation between (i.e. not just copying the same content from one to another, but having a modification in the data):</p>

<pre><code>int aaa;
int src[ARRAY_SIZE];
int dest[ARRAY_SIZE];

//fill src with data

for (aaa = 0; aaa &lt; ARRAY_SIZE; aaa++)
{
    dest[aaa] = src[aaa] * 30;
}
</code></pre>

<p>This is done in buffers of size 520 or higher, so the for loop is considerable.</p>

<p>Is there any way to improve performance here in what comes to coding? </p>

<p>I did some research on the topic, but I couldn't find anything specific about this case, only about simple copy buffer to buffer (examples: <a href=""http://nadeausoftware.com/articles/2012/05/c_c_tip_how_copy_memory_quickly"" rel=""nofollow noreferrer"">here</a>, <a href=""https://stackoverflow.com/questions/1022379/copying-an-array-in-c"">here</a> and <a href=""https://stackoverflow.com/questions/17212847/faster-method-to-copy-arrays-after-typecasting-in-c"">here</a>).</p>

<p>Environment: GCC for ARM using Embedded Linux. The specific code above, though, is used inside a C project running inside a dedicated processor for DSP calculations. The general processor is an OMAP L138 (the DSP processor is included in the L138).</p>
","i want to copy an c array data to another but with a calculation between ie not just copying the same content from one to another but having a modification in the data
this is done in buffers of size 520 or higher so the for loop is considerable
is there any way to improve performance here in what comes to coding 
i did some research on the topic but i couldnt find anything specific about this case only about simple copy buffer to buffer examples here here and here
environment gcc for arm using embedded linux the specific code above though is used inside a c project running inside a dedicated processor for dsp calculations the general processor is an omap l138 the dsp processor is included in the l138
",0,"I want to copy an C array data to another, but with a calculation between (i.e. not just copying the same content from one to another, but having a modification in the data):
int aaa;
int src[ARRAY_SIZE];
int dest[ARRAY_SIZE];

//fill src with data

for (aaa = 0; aaa < ARRAY_SIZE; aaa++)
{
    dest[aaa] = src[aaa] * 30;
}

This is done in buffers of size 520 or higher, so the for loop is considerable.
Is there any way to improve performance here in what comes to coding? 
I did some research on the topic, but I couldn't find anything specific about this case, only about simple copy buffer to buffer (examples: here, here and here).
Environment: GCC for ARM using Embedded Linux. The specific code above, though, is used inside a C project running inside a dedicated processor for DSP calculations. The general processor is an OMAP L138 (the DSP processor is included in the L138).
","Faster way to copy C array with calculation between
I want to copy an C array data to another, but with a calculation between (i.e. not just copying the same content from one to another, but having a modification in the data):
int aaa;
int src[ARRAY_SIZE];
int dest[ARRAY_SIZE];

//fill src with data

for (aaa = 0; aaa < ARRAY_SIZE; aaa++)
{
    dest[aaa] = src[aaa] * 30;
}

This is done in buffers of size 520 or higher, so the for loop is considerable.
Is there any way to improve performance here in what comes to coding? 
I did some research on the topic, but I couldn't find anything specific about this case, only about simple copy buffer to buffer (examples: here, here and here).
Environment: GCC for ARM using Embedded Linux. The specific code above, though, is used inside a C project running inside a dedicated processor for DSP calculations. The general processor is an OMAP L138 (the DSP processor is included in the L138).
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
35843318,35843318,How to avoid memory leak at design-implementation level,"<p>
I saw an interview question a couple of days ago regarding memory leaks in c++.
The code was like (if I remember correctly):</p>

<pre class=""lang-cpp prettyprint-override""><code>#include &lt;iostream&gt;
using namespace std;

class super {
    int value;
    int arr[1000];
public:
    super() :value(0) {}
    super(int value) :value(value) {}
    virtual int getValue() const{
        return this-&gt;value;
    }
};

class sub : public super {
    int val;
    super sup;
    vector&lt;int&gt; v1;
public:
    sub() :val(0), sup(0) {}
    sub(int value) :val(value), sup(value), v1(10,0) {}
    int getValue() const{
        return this-&gt;val;
    }
};

int main() {
    sub* pt1 = new(sub);
    super* pt2 = pt1;

    pt1 = new(sub);

    delete pt2;     //memory leak ??

    //more code here...
    delete pt1;
    return 0;
}
</code></pre>

<p>The question was how to avoid this type of memory leaks at <strong>implementation-design level</strong>. I guess the question was more than simply answering ""do not use pointers like that"".</p>

<p>Does it have to do with implementing destructor as virtual or using dynamic cast? How can we implement destructors so that <code>delete pt2</code> wont create any memory leaks? Can anyone further analyze this example?</p>

<p>Thanks in advance.</p>
","
i saw an interview question a couple of days ago regarding memory leaks in c
the code was like if i remember correctly
include iostream
using namespace std

class super 
    int value
    int arr1000
public
    super value0 
    superint value valuevalue 
    virtual int getvalue const
        return thisvalue
    


class sub  public super 
    int val
    super sup
    vectorint v1
public
    sub val0 sup0 
    subint value valvalue supvalue v1100 
    int getvalue const
        return thisval
    


int main 
    sub pt1  newsub
    super pt2  pt1

    pt1  newsub

    delete pt2     memory leak 

    more code here
    delete pt1
    return 0


the question was how to avoid this type of memory leaks at implementationdesign level i guess the question was more than simply answering do not use pointers like that
does it have to do with implementing destructor as virtual or using dynamic cast how can we implement destructors so that delete pt2 wont create any memory leaks can anyone further analyze this example
thanks in advance
",1,"
I saw an interview question a couple of days ago regarding memory leaks in c++.
The code was like (if I remember correctly):
#include <iostream>
using namespace std;

class super {
    int value;
    int arr[1000];
public:
    super() :value(0) {}
    super(int value) :value(value) {}
    virtual int getValue() const{
        return this->value;
    }
};

class sub : public super {
    int val;
    super sup;
    vector<int> v1;
public:
    sub() :val(0), sup(0) {}
    sub(int value) :val(value), sup(value), v1(10,0) {}
    int getValue() const{
        return this->val;
    }
};

int main() {
    sub* pt1 = new(sub);
    super* pt2 = pt1;

    pt1 = new(sub);

    delete pt2;     //memory leak ??

    //more code here...
    delete pt1;
    return 0;
}

The question was how to avoid this type of memory leaks at implementation-design level. I guess the question was more than simply answering ""do not use pointers like that"".
Does it have to do with implementing destructor as virtual or using dynamic cast? How can we implement destructors so that delete pt2 wont create any memory leaks? Can anyone further analyze this example?
Thanks in advance.
","How to avoid memory leak at design-implementation level

I saw an interview question a couple of days ago regarding memory leaks in c++.
The code was like (if I remember correctly):
#include <iostream>
using namespace std;

class super {
    int value;
    int arr[1000];
public:
    super() :value(0) {}
    super(int value) :value(value) {}
    virtual int getValue() const{
        return this->value;
    }
};

class sub : public super {
    int val;
    super sup;
    vector<int> v1;
public:
    sub() :val(0), sup(0) {}
    sub(int value) :val(value), sup(value), v1(10,0) {}
    int getValue() const{
        return this->val;
    }
};

int main() {
    sub* pt1 = new(sub);
    super* pt2 = pt1;

    pt1 = new(sub);

    delete pt2;     //memory leak ??

    //more code here...
    delete pt1;
    return 0;
}

The question was how to avoid this type of memory leaks at implementation-design level. I guess the question was more than simply answering ""do not use pointers like that"".
Does it have to do with implementing destructor as virtual or using dynamic cast? How can we implement destructors so that delete pt2 wont create any memory leaks? Can anyone further analyze this example?
Thanks in advance.
",1,1,1,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,1,1,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
20808334,20808334,Only upper right part of image rendered when using octree,"<p>I'm currently implementing the paper of Revelles, Urena and Lastra <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.987"" rel=""nofollow noreferrer"">""An Efficient Parametric Algorithm for Octree Traversal""</a>. In <a href=""https://stackoverflow.com/questions/10228690/ray-octree-intersection-algorithms"">Ray - Octree intersection algorithms</a> someone implemented it and pasted his code. My implementation should be the same, except that I used some vectors for computation.
However using this Octree only the upper right part of the image is rendered, for the rest of the image the octree isn't traversed. The check wheter to traverse or not happens in the following method: </p>

<pre><code>bool Octnode::intersect( Ray r, SurfaceData *sd )
{
  unsigned int a = 0;
  v3d o = r.origin();
  v3d d = r.direction();

  if ( r.direction()[0] &lt; 0. ) {
    o[0] = _size[0] - r.origin()[0];
    d[0] = -r.direction()[0];
    a |= 4;
  }
  if ( r.direction()[1] &lt; 0. ) {
    o[1] = _size[1] - r.origin()[1];
    d[1] = -r.direction()[1];
    a |= 2;
  }
  if ( r.direction()[2] &lt; 0. ) {
    o[2] = _size[2] - r.origin()[2];
    d[2] = -r.direction()[2];
    a |= 1;
  }

  v3d t0 = ( _min - o ) / d;
  v3d t1 = ( _max - o ) / d;

  scalar t = std::numeric_limits&lt;double&gt;::max();

  // traversal -- if any -- starts here
  if ( t0.max() &lt; t1.min() ) {
    return processSubtree( t0, t1, r, &amp;t, sd, a );
  } else {
    return false;
  }
}
</code></pre>

<p><strong>[Edit]</strong> The above method implements the function</p>

<pre><code>void ray_parameter( octree *oct, ray r )
</code></pre>

<p>from the paper. As C. Urena pointed out there is an error in the paper that causes the traversal to be incorrect. Unfortunately traversal is skipped before this error could come into play. 
In the Google group that can be found follwing C. Urena's link it seems the size of an octree node is computed differently. I did:</p>

<pre><code>_size = _max - _min;
</code></pre>

<p>versus</p>

<pre><code>_size = ( _max - _min ) / 2.;
</code></pre>

<p>in the Google group. I'll test that and post another update. <strong>[/Edit]</strong></p>

<p><strong>[Edit 2]</strong> Applying the fix that Carlos mentioned and reducing the size by half brought me this far:</p>

<p><img src=""https://i.stack.imgur.com/dyzzD.png"" alt=""enter image description here""></p>

<p>The spheres should be completely rendered, but at least not all rays for the upper left quarter are rejected. <strong>[/Edit 2]</strong></p>

<p><strong>[Edit 3]</strong> Using different data sets I get seemingly better results, looks like I'll have to investigate some other parts of the code.</p>

<p><img src=""https://i.stack.imgur.com/UrAv9.png"" alt=""enter image description here""> <img src=""https://i.stack.imgur.com/Hkqf9.png"" alt=""enter image description here""></p>

<p><strong>[/Edit 3]</strong></p>
","im currently implementing the paper of revelles urena and lastra an efficient parametric algorithm for octree traversal in ray  octree intersection algorithms someone implemented it and pasted his code my implementation should be the same except that i used some vectors for computation
however using this octree only the upper right part of the image is rendered for the rest of the image the octree isnt traversed the check wheter to traverse or not happens in the following method 
edit the above method implements the function
from the paper as c urena pointed out there is an error in the paper that causes the traversal to be incorrect unfortunately traversal is skipped before this error could come into play 
in the google group that can be found follwing c urenas link it seems the size of an octree node is computed differently i did
versus
in the google group ill test that and post another update edit
edit 2 applying the fix that carlos mentioned and reducing the size by half brought me this far

the spheres should be completely rendered but at least not all rays for the upper left quarter are rejected edit 2
edit 3 using different data sets i get seemingly better results looks like ill have to investigate some other parts of the code
 
edit 3
",1,"I'm currently implementing the paper of Revelles, Urena and Lastra ""An Efficient Parametric Algorithm for Octree Traversal"". In Ray - Octree intersection algorithms someone implemented it and pasted his code. My implementation should be the same, except that I used some vectors for computation.
However using this Octree only the upper right part of the image is rendered, for the rest of the image the octree isn't traversed. The check wheter to traverse or not happens in the following method: 
bool Octnode::intersect( Ray r, SurfaceData *sd )
{
  unsigned int a = 0;
  v3d o = r.origin();
  v3d d = r.direction();

  if ( r.direction()[0] < 0. ) {
    o[0] = _size[0] - r.origin()[0];
    d[0] = -r.direction()[0];
    a |= 4;
  }
  if ( r.direction()[1] < 0. ) {
    o[1] = _size[1] - r.origin()[1];
    d[1] = -r.direction()[1];
    a |= 2;
  }
  if ( r.direction()[2] < 0. ) {
    o[2] = _size[2] - r.origin()[2];
    d[2] = -r.direction()[2];
    a |= 1;
  }

  v3d t0 = ( _min - o ) / d;
  v3d t1 = ( _max - o ) / d;

  scalar t = std::numeric_limits<double>::max();

  // traversal -- if any -- starts here
  if ( t0.max() < t1.min() ) {
    return processSubtree( t0, t1, r, &t, sd, a );
  } else {
    return false;
  }
}

[Edit] The above method implements the function
void ray_parameter( octree *oct, ray r )

from the paper. As C. Urena pointed out there is an error in the paper that causes the traversal to be incorrect. Unfortunately traversal is skipped before this error could come into play. 
In the Google group that can be found follwing C. Urena's link it seems the size of an octree node is computed differently. I did:
_size = _max - _min;

versus
_size = ( _max - _min ) / 2.;

in the Google group. I'll test that and post another update. [/Edit]
[Edit 2] Applying the fix that Carlos mentioned and reducing the size by half brought me this far:

The spheres should be completely rendered, but at least not all rays for the upper left quarter are rejected. [/Edit 2]
[Edit 3] Using different data sets I get seemingly better results, looks like I'll have to investigate some other parts of the code.
 
[/Edit 3]
","Only upper right part of image rendered when using octree
I'm currently implementing the paper of Revelles, Urena and Lastra ""An Efficient Parametric Algorithm for Octree Traversal"". In Ray - Octree intersection algorithms someone implemented it and pasted his code. My implementation should be the same, except that I used some vectors for computation.
However using this Octree only the upper right part of the image is rendered, for the rest of the image the octree isn't traversed. The check wheter to traverse or not happens in the following method: 
bool Octnode::intersect( Ray r, SurfaceData *sd )
{
  unsigned int a = 0;
  v3d o = r.origin();
  v3d d = r.direction();

  if ( r.direction()[0] < 0. ) {
    o[0] = _size[0] - r.origin()[0];
    d[0] = -r.direction()[0];
    a |= 4;
  }
  if ( r.direction()[1] < 0. ) {
    o[1] = _size[1] - r.origin()[1];
    d[1] = -r.direction()[1];
    a |= 2;
  }
  if ( r.direction()[2] < 0. ) {
    o[2] = _size[2] - r.origin()[2];
    d[2] = -r.direction()[2];
    a |= 1;
  }

  v3d t0 = ( _min - o ) / d;
  v3d t1 = ( _max - o ) / d;

  scalar t = std::numeric_limits<double>::max();

  // traversal -- if any -- starts here
  if ( t0.max() < t1.min() ) {
    return processSubtree( t0, t1, r, &t, sd, a );
  } else {
    return false;
  }
}

[Edit] The above method implements the function
void ray_parameter( octree *oct, ray r )

from the paper. As C. Urena pointed out there is an error in the paper that causes the traversal to be incorrect. Unfortunately traversal is skipped before this error could come into play. 
In the Google group that can be found follwing C. Urena's link it seems the size of an octree node is computed differently. I did:
_size = _max - _min;

versus
_size = ( _max - _min ) / 2.;

in the Google group. I'll test that and post another update. [/Edit]
[Edit 2] Applying the fix that Carlos mentioned and reducing the size by half brought me this far:

The spheres should be completely rendered, but at least not all rays for the upper left quarter are rejected. [/Edit 2]
[Edit 3] Using different data sets I get seemingly better results, looks like I'll have to investigate some other parts of the code.
 
[/Edit 3]
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
13556872,13556872,How to refactor this if statement to add or update to dictionary?,"<p>Here is my code:</p>

<pre><code>var messageHandlers = new Dictionary&lt;Type, Action&lt;T&gt;&gt;();

public static void Subscribe(Action&lt;T&gt; message)
{
    if (messageHandlers.ContainsKey(typeof(T)))
    {
        messageHandlers[typeof(T)] += message;
    }
    else
    {
        messageHandlers.Add(typeof(T), message);
    }
}
</code></pre>

<p>Is there any way to make this a one liner?</p>
","here is my code
is there any way to make this a one liner
",1,"Here is my code:
var messageHandlers = new Dictionary<Type, Action<T>>();

public static void Subscribe(Action<T> message)
{
    if (messageHandlers.ContainsKey(typeof(T)))
    {
        messageHandlers[typeof(T)] += message;
    }
    else
    {
        messageHandlers.Add(typeof(T), message);
    }
}

Is there any way to make this a one liner?
","How to refactor this if statement to add or update to dictionary?
Here is my code:
var messageHandlers = new Dictionary<Type, Action<T>>();

public static void Subscribe(Action<T> message)
{
    if (messageHandlers.ContainsKey(typeof(T)))
    {
        messageHandlers[typeof(T)] += message;
    }
    else
    {
        messageHandlers.Add(typeof(T), message);
    }
}

Is there any way to make this a one liner?
",1,1,1,1,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,1,1,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
38188173,38188173,Most efficient way of creating a List for an Android App,"<p>As of in the title, I am experimenting with two different ways of creating an <code>ArrayListView</code> in android.</p>

<p>The first one looks like this:</p>

<pre><code>list.add(new Obj(""smth"", ""Note: na, na, na"", image[0]));
list.add(new Obj(""smth"", ""Note: na, na, na"", image[1]));
list.add(new Obj(""smth"", ""Note: na, na, na"", image[2]));
</code></pre>

<p>and has a support Array for the images:</p>

<pre><code>private int[] images = {
        R.drawable.image1,
        R.drawable.image2,
        R.drawable.image3,
        ...}
</code></pre>

<p>the other way uses the getter and setter methods of the Obj.class. like this:</p>

<pre><code>ArrayList&lt;Obj&gt; objects = new ArrayList&lt;&gt;();

    Obj a=new Obj();
    a.setName(""smth"");
    a.setNote(""na,na,na"");
    a.setImage(R.drawable.image1);
    objects.add(a);

    a=new Obj();
    a.setName(""smth"");
    a.setNote(""na, na, na"");
    a.setImage(R.drawable.image2);
    objects.add(a);

    a=new Obj();
    a.setName(""smth"");
    a.setNote(""na,na,na"");
    a.setImage(R.drawable.image3);
    objects.add(a);
</code></pre>

<p><strong>Is one of them better than the other? maybe it increases performance in some way? what if this list will be crowded with hundreds of Objects?</strong></p>

<p><strong>UPDATE:</strong>
I have tried to run an App with many Objects in the List and performance seems slightly better using the second method. But I'm sure there are even better ones!</p>
","as of in the title i am experimenting with two different ways of creating an arraylistview in android
the first one looks like this
and has a support array for the images
the other way uses the getter and setter methods of the objclass like this
is one of them better than the other maybe it increases performance in some way what if this list will be crowded with hundreds of objects
update
i have tried to run an app with many objects in the list and performance seems slightly better using the second method but im sure there are even better ones
",1,"As of in the title, I am experimenting with two different ways of creating an ArrayListView in android.
The first one looks like this:
list.add(new Obj(""smth"", ""Note: na, na, na"", image[0]));
list.add(new Obj(""smth"", ""Note: na, na, na"", image[1]));
list.add(new Obj(""smth"", ""Note: na, na, na"", image[2]));

and has a support Array for the images:
private int[] images = {
        R.drawable.image1,
        R.drawable.image2,
        R.drawable.image3,
        ...}

the other way uses the getter and setter methods of the Obj.class. like this:
ArrayList<Obj> objects = new ArrayList<>();

    Obj a=new Obj();
    a.setName(""smth"");
    a.setNote(""na,na,na"");
    a.setImage(R.drawable.image1);
    objects.add(a);

    a=new Obj();
    a.setName(""smth"");
    a.setNote(""na, na, na"");
    a.setImage(R.drawable.image2);
    objects.add(a);

    a=new Obj();
    a.setName(""smth"");
    a.setNote(""na,na,na"");
    a.setImage(R.drawable.image3);
    objects.add(a);

Is one of them better than the other? maybe it increases performance in some way? what if this list will be crowded with hundreds of Objects?
UPDATE:
I have tried to run an App with many Objects in the List and performance seems slightly better using the second method. But I'm sure there are even better ones!
","Most efficient way of creating a List for an Android App
As of in the title, I am experimenting with two different ways of creating an ArrayListView in android.
The first one looks like this:
list.add(new Obj(""smth"", ""Note: na, na, na"", image[0]));
list.add(new Obj(""smth"", ""Note: na, na, na"", image[1]));
list.add(new Obj(""smth"", ""Note: na, na, na"", image[2]));

and has a support Array for the images:
private int[] images = {
        R.drawable.image1,
        R.drawable.image2,
        R.drawable.image3,
        ...}

the other way uses the getter and setter methods of the Obj.class. like this:
ArrayList<Obj> objects = new ArrayList<>();

    Obj a=new Obj();
    a.setName(""smth"");
    a.setNote(""na,na,na"");
    a.setImage(R.drawable.image1);
    objects.add(a);

    a=new Obj();
    a.setName(""smth"");
    a.setNote(""na, na, na"");
    a.setImage(R.drawable.image2);
    objects.add(a);

    a=new Obj();
    a.setName(""smth"");
    a.setNote(""na,na,na"");
    a.setImage(R.drawable.image3);
    objects.add(a);

Is one of them better than the other? maybe it increases performance in some way? what if this list will be crowded with hundreds of Objects?
UPDATE:
I have tried to run an App with many Objects in the List and performance seems slightly better using the second method. But I'm sure there are even better ones!
",0,1,0,1,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Code,Code,,Code,Very Mild,Very Mild,,Very Mild
26927462,26927462,Solver is too slow,"<p>I am still using the <code>OptaPlanner</code> to optimize a chained planning problem which is similar to the <code>VehicleRoutingExample</code>. My planning entities have a planning variable which is another planning entity. The scores are <code>HardSoftScores</code>, which are calculated with an <code>IncrementalScore</code>.</p>

<p>At the moment I have the problem that the time the solver needs is still too long to find an acceptable solution.</p>

<p>I want to optimize a problem with a fixed number of workers, who have to process a fixed number of orders with several time windows.</p>

<p>The orders are my chained entities and the workers are used as anchors for the chains.
I am calculating the start and end points of time for the orders by a listener. Another listener is saving the anchor of each chain in every entity (because it needs to much time to go through a chain which could be longer than 1000 entitiesâ€¦.)</p>

<p>My aim is to solve a problem with something about 3000 entities at the moment the solver needs more than 2 hours to get an acceptable solution. For a smaller problem with 400 entities it needs nearly 5min which is too long too. 10 min for the bigger problem 10min would be okay, for the small one 1min.</p>

<p>I have already worked with the benchmarker to find the best solver configâ€¦</p>

<p>Does anybody see some possibilities to make my solver faster?</p>

<p>P.S.: Is der an opportunity to use multithreadning?, or does the optaplanner already use it?</p>
","i am still using the optaplanner to optimize a chained planning problem which is similar to the vehicleroutingexample my planning entities have a planning variable which is another planning entity the scores are hardsoftscores which are calculated with an incrementalscore
at the moment i have the problem that the time the solver needs is still too long to find an acceptable solution
i want to optimize a problem with a fixed number of workers who have to process a fixed number of orders with several time windows
the orders are my chained entities and the workers are used as anchors for the chains
i am calculating the start and end points of time for the orders by a listener another listener is saving the anchor of each chain in every entity because it needs to much time to go through a chain which could be longer than 1000 entities
my aim is to solve a problem with something about 3000 entities at the moment the solver needs more than 2 hours to get an acceptable solution for a smaller problem with 400 entities it needs nearly 5min which is too long too 10 min for the bigger problem 10min would be okay for the small one 1min
i have already worked with the benchmarker to find the best solver config
does anybody see some possibilities to make my solver faster
ps is der an opportunity to use multithreadning or does the optaplanner already use it
",1,"I am still using the OptaPlanner to optimize a chained planning problem which is similar to the VehicleRoutingExample. My planning entities have a planning variable which is another planning entity. The scores are HardSoftScores, which are calculated with an IncrementalScore.
At the moment I have the problem that the time the solver needs is still too long to find an acceptable solution.
I want to optimize a problem with a fixed number of workers, who have to process a fixed number of orders with several time windows.
The orders are my chained entities and the workers are used as anchors for the chains.
I am calculating the start and end points of time for the orders by a listener. Another listener is saving the anchor of each chain in every entity (because it needs to much time to go through a chain which could be longer than 1000 entitiesâ€¦.)
My aim is to solve a problem with something about 3000 entities at the moment the solver needs more than 2 hours to get an acceptable solution. For a smaller problem with 400 entities it needs nearly 5min which is too long too. 10 min for the bigger problem 10min would be okay, for the small one 1min.
I have already worked with the benchmarker to find the best solver configâ€¦
Does anybody see some possibilities to make my solver faster?
P.S.: Is der an opportunity to use multithreadning?, or does the optaplanner already use it?
","Solver is too slow
I am still using the OptaPlanner to optimize a chained planning problem which is similar to the VehicleRoutingExample. My planning entities have a planning variable which is another planning entity. The scores are HardSoftScores, which are calculated with an IncrementalScore.
At the moment I have the problem that the time the solver needs is still too long to find an acceptable solution.
I want to optimize a problem with a fixed number of workers, who have to process a fixed number of orders with several time windows.
The orders are my chained entities and the workers are used as anchors for the chains.
I am calculating the start and end points of time for the orders by a listener. Another listener is saving the anchor of each chain in every entity (because it needs to much time to go through a chain which could be longer than 1000 entitiesâ€¦.)
My aim is to solve a problem with something about 3000 entities at the moment the solver needs more than 2 hours to get an acceptable solution. For a smaller problem with 400 entities it needs nearly 5min which is too long too. 10 min for the bigger problem 10min would be okay, for the small one 1min.
I have already worked with the benchmarker to find the best solver configâ€¦
Does anybody see some possibilities to make my solver faster?
P.S.: Is der an opportunity to use multithreadning?, or does the optaplanner already use it?
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
23716005,23716005,Parsing data fast from text file,"<p>i read in a full file into a string. This is very quick. (For a example 180Mb file - 2s)
Then i extract some values from the string using >> operator and create several arrays from it and insert the arrays into a struct and add each struct into a vector.</p>

<p>I'm trying to find the bottleneck, because this is very slow (but maybe you cant do anything)</p>

<ol>
<li><p>is the >> approach fast?</p>

<pre><code>string str; // gets filled with the file
struct A;
std::vector&lt;A&gt; b; // global variables
// in the function inside the loop 
str &gt;&gt; a.val
A a;
b.push_back(a);
</code></pre></li>
<li><p>Does the vector take ownership of the a or does it make a copy? Is a still on the stack? I have about 60.000 structs that get insert into the vector. Is this a fast approach or is there a better one. </p></li>
</ol>
","i read in a full file into a string this is very quick for a example 180mb file  2s
then i extract some values from the string using  operator and create several arrays from it and insert the arrays into a struct and add each struct into a vector
im trying to find the bottleneck because this is very slow but maybe you cant do anything

is the  approach fast

does the vector take ownership of the a or does it make a copy is a still on the stack i have about 60000 structs that get insert into the vector is this a fast approach or is there a better one 

",1,"i read in a full file into a string. This is very quick. (For a example 180Mb file - 2s)
Then i extract some values from the string using >> operator and create several arrays from it and insert the arrays into a struct and add each struct into a vector.
I'm trying to find the bottleneck, because this is very slow (but maybe you cant do anything)

is the >> approach fast?
string str; // gets filled with the file
struct A;
std::vector<A> b; // global variables
// in the function inside the loop 
str >> a.val
A a;
b.push_back(a);

Does the vector take ownership of the a or does it make a copy? Is a still on the stack? I have about 60.000 structs that get insert into the vector. Is this a fast approach or is there a better one. 

","Parsing data fast from text file
i read in a full file into a string. This is very quick. (For a example 180Mb file - 2s)
Then i extract some values from the string using >> operator and create several arrays from it and insert the arrays into a struct and add each struct into a vector.
I'm trying to find the bottleneck, because this is very slow (but maybe you cant do anything)

is the >> approach fast?
string str; // gets filled with the file
struct A;
std::vector<A> b; // global variables
// in the function inside the loop 
str >> a.val
A a;
b.push_back(a);

Does the vector take ownership of the a or does it make a copy? Is a still on the stack? I have about 60.000 structs that get insert into the vector. Is this a fast approach or is there a better one. 

",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
22669671,22669671,Traversing binary tree iterative or recursive - complexity analysis,"<p>I've heard some opinions that iterative lookup in Binary search tree is more efficient than the recursive way, is it true?<br>
(I know that in space terms the recusion is more expensive)</p>
","ive heard some opinions that iterative lookup in binary search tree is more efficient than the recursive way is it true
i know that in space terms the recusion is more expensive
",1,"I've heard some opinions that iterative lookup in Binary search tree is more efficient than the recursive way, is it true?
(I know that in space terms the recusion is more expensive)
","Traversing binary tree iterative or recursive - complexity analysis
I've heard some opinions that iterative lookup in Binary search tree is more efficient than the recursive way, is it true?
(I know that in space terms the recusion is more expensive)
",0,0,1,1,0,0,0,- Is a theoretical question,"Label: 1

Reasoning:

This question expresses technical debt because it implies that the current implementation of the recursive lookup in a Binary Search Tree (BST) may not be optimal in terms of efficiency, and the user is considering an alternative approach (iterative lookup) to improve performance. 

Although the user is not explicitly stating that their current implementation is slow or problematic, they are seeking to optimize their code by exploring a different approach that is perceived to be more efficient. The fact that they mention the space complexity of recursion being more expensive suggests that they are aware of potential performance issues and are looking to mitigate them.

The question implies that the user is open to refactoring their code to improve its efficiency, which is a characteristic of technical debt. Additionally, the comparison between iterative and recursive lookup methods indicates that the user is considering design alternatives, which may lead to a better implementation.

Overall, the question expresses technical debt because it reveals a suboptimal implementation (recursive lookup) that may require improvement to achieve better performance, and the user is actively seeking a more efficient solution.",Yes,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,0,1,1,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
76357201,76357201,Pyspark optimization with loops,"<p>I have a table with the columns items, date, and sales. I have the total sales by date and item. I need to find the combination of 3 items which minimizes the standard deviation. The table looks like this: e.g.</p>
<pre><code># df:
# +------------+----------+------------+
# |item        |      date|sales       |
# +------------+----------+------------+
# |         325|2021-05-01|     8524.64|
# |         400|2021-05-01|     9939.59|
# |         314|2021-05-03|      5466.3|
# |         267|2021-05-04|     6471.63|
# |         387|2021-05-04|     5406.85|
# +------------+----------+------------+
</code></pre>
<p>I need to find the group of items, that when grouped together the standard deviation is at its minimum. In order to do this I made a loop but it takes too long, this is how I thought about it:</p>
<pre><code>bestis=[]
besti=0
best_score=99999
t=1
list_items= [325,400,314,267,387] #all values of item column
while t&lt;=3:
    if besti in all_other_stores: all_other_stores.remove(besti)

for i in list_items:
    df_filtered = df.filter(col(&quot;item&quot;).isin(bestis+[i])
    stddev_sales = df_filtered.select(stddev(&quot;sales&quot;)).collect()[0][0]
    
    if stddev_sales&lt; best_stdev:
       best_stdev = stddev_sales
       besti=i 

bestis= bestis+[besti]
    t += 1 
</code></pre>
<p>At the end of the first loop, I will have the item with the min standard deviation. e.g: [325] At the end of the second loop, I should have the best two items that minimize the standard deviation. e.g. [325,400], at the end of third loop I should have [325,400,387] assuming that 387 is the additional item that minimizes the standard deviation. I got a working code, the only problem is that it takes too long because of the loop</p>
<p>How could I use a pySpark map function instead in this case? Or maybe is there some kind of optimization algorithm that will do this faster? like given the list, find me the best combo of 3 items that minimizes the standard deviation?</p>
","i have a table with the columns items date and sales i have the total sales by date and item i need to find the combination of 3 items which minimizes the standard deviation the table looks like this eg
i need to find the group of items that when grouped together the standard deviation is at its minimum in order to do this i made a loop but it takes too long this is how i thought about it
at the end of the first loop i will have the item with the min standard deviation eg 325 at the end of the second loop i should have the best two items that minimize the standard deviation eg 325400 at the end of third loop i should have 325400387 assuming that 387 is the additional item that minimizes the standard deviation i got a working code the only problem is that it takes too long because of the loop
how could i use a pyspark map function instead in this case or maybe is there some kind of optimization algorithm that will do this faster like given the list find me the best combo of 3 items that minimizes the standard deviation
",1,"I have a table with the columns items, date, and sales. I have the total sales by date and item. I need to find the combination of 3 items which minimizes the standard deviation. The table looks like this: e.g.
# df:
# +------------+----------+------------+
# |item        |      date|sales       |
# +------------+----------+------------+
# |         325|2021-05-01|     8524.64|
# |         400|2021-05-01|     9939.59|
# |         314|2021-05-03|      5466.3|
# |         267|2021-05-04|     6471.63|
# |         387|2021-05-04|     5406.85|
# +------------+----------+------------+

I need to find the group of items, that when grouped together the standard deviation is at its minimum. In order to do this I made a loop but it takes too long, this is how I thought about it:
bestis=[]
besti=0
best_score=99999
t=1
list_items= [325,400,314,267,387] #all values of item column
while t<=3:
    if besti in all_other_stores: all_other_stores.remove(besti)

for i in list_items:
    df_filtered = df.filter(col(""item"").isin(bestis+[i])
    stddev_sales = df_filtered.select(stddev(""sales"")).collect()[0][0]
    
    if stddev_sales< best_stdev:
       best_stdev = stddev_sales
       besti=i 

bestis= bestis+[besti]
    t += 1 

At the end of the first loop, I will have the item with the min standard deviation. e.g: [325] At the end of the second loop, I should have the best two items that minimize the standard deviation. e.g. [325,400], at the end of third loop I should have [325,400,387] assuming that 387 is the additional item that minimizes the standard deviation. I got a working code, the only problem is that it takes too long because of the loop
How could I use a pySpark map function instead in this case? Or maybe is there some kind of optimization algorithm that will do this faster? like given the list, find me the best combo of 3 items that minimizes the standard deviation?
","Pyspark optimization with loops
I have a table with the columns items, date, and sales. I have the total sales by date and item. I need to find the combination of 3 items which minimizes the standard deviation. The table looks like this: e.g.
# df:
# +------------+----------+------------+
# |item        |      date|sales       |
# +------------+----------+------------+
# |         325|2021-05-01|     8524.64|
# |         400|2021-05-01|     9939.59|
# |         314|2021-05-03|      5466.3|
# |         267|2021-05-04|     6471.63|
# |         387|2021-05-04|     5406.85|
# +------------+----------+------------+

I need to find the group of items, that when grouped together the standard deviation is at its minimum. In order to do this I made a loop but it takes too long, this is how I thought about it:
bestis=[]
besti=0
best_score=99999
t=1
list_items= [325,400,314,267,387] #all values of item column
while t<=3:
    if besti in all_other_stores: all_other_stores.remove(besti)

for i in list_items:
    df_filtered = df.filter(col(""item"").isin(bestis+[i])
    stddev_sales = df_filtered.select(stddev(""sales"")).collect()[0][0]
    
    if stddev_sales< best_stdev:
       best_stdev = stddev_sales
       besti=i 

bestis= bestis+[besti]
    t += 1 

At the end of the first loop, I will have the item with the min standard deviation. e.g: [325] At the end of the second loop, I should have the best two items that minimize the standard deviation. e.g. [325,400], at the end of third loop I should have [325,400,387] assuming that 387 is the additional item that minimizes the standard deviation. I got a working code, the only problem is that it takes too long because of the loop
How could I use a pySpark map function instead in this case? Or maybe is there some kind of optimization algorithm that will do this faster? like given the list, find me the best combo of 3 items that minimizes the standard deviation?
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Design,Design,,Design,Moderate,Moderate,,Moderate
51214193,51214193,Does Java's regular expression engine optimizes character class expressions internally?,"<p>We know we can come up with complex character classes such as:</p>

<pre><code>[A-F&amp;&amp;[^B]]     // A through F, excluding B
[G-L&amp;&amp;[^H]]     // G through L, excluding H
</code></pre>

<p>We can do intersections &amp;&amp; and unions (implicitly done):</p>

<pre><code>[[A-F&amp;&amp;[^B]][G-L&amp;&amp;[^H]]]      // union of both classes above
</code></pre>

<p>But the question is, will Java internally generate an optimized automaton so that the expression above is reduced to something like a simplified expression:</p>

<pre><code>[A-L&amp;&amp;[^BH]]  // union of both classes above, simplified
</code></pre>

<p>or not necessarily? I just want to determine whether it is worth coming up with some regular expression simplification routine to improve matching performance or whether Java is already doing simplification of character classes implicitly.</p>
","we know we can come up with complex character classes such as
we can do intersections  and unions implicitly done
but the question is will java internally generate an optimized automaton so that the expression above is reduced to something like a simplified expression
or not necessarily i just want to determine whether it is worth coming up with some regular expression simplification routine to improve matching performance or whether java is already doing simplification of character classes implicitly
",1,"We know we can come up with complex character classes such as:
[A-F&&[^B]]     // A through F, excluding B
[G-L&&[^H]]     // G through L, excluding H

We can do intersections && and unions (implicitly done):
[[A-F&&[^B]][G-L&&[^H]]]      // union of both classes above

But the question is, will Java internally generate an optimized automaton so that the expression above is reduced to something like a simplified expression:
[A-L&&[^BH]]  // union of both classes above, simplified

or not necessarily? I just want to determine whether it is worth coming up with some regular expression simplification routine to improve matching performance or whether Java is already doing simplification of character classes implicitly.
","Does Java's regular expression engine optimizes character class expressions internally?
We know we can come up with complex character classes such as:
[A-F&&[^B]]     // A through F, excluding B
[G-L&&[^H]]     // G through L, excluding H

We can do intersections && and unions (implicitly done):
[[A-F&&[^B]][G-L&&[^H]]]      // union of both classes above

But the question is, will Java internally generate an optimized automaton so that the expression above is reduced to something like a simplified expression:
[A-L&&[^BH]]  // union of both classes above, simplified

or not necessarily? I just want to determine whether it is worth coming up with some regular expression simplification routine to improve matching performance or whether Java is already doing simplification of character classes implicitly.
",0,0,1,1,0,0,0,- Is a theoretical question,"Label: 1

Reasoning:

This question expresses technical debt because the user is concerned about the potential performance impact of using complex regular expressions and wants to know if Java optimizes them internally. The user is considering implementing a regular expression simplification routine to improve matching performance, which suggests that they are aware of a potential performance issue.

The technical debt in this question is inherent, as the user is questioning the internal workings of Java's regular expression engine and its potential to optimize complex character classes. The user is essentially asking if they need to refactor their code to improve performance or if Java will handle it automatically.

The fact that the user is considering implementing a simplification routine implies that they think the current implementation might be suboptimal and could be improved. This indicates a potential design flaw or performance issue that needs to be addressed, which is a hallmark of technical debt.

Additionally, the user's concern about performance and their desire to simplify the regular expression suggest that they are thinking about maintainability and scalability, which are also important aspects of technical debt. Overall, this question expresses technical debt because it reveals a potential performance issue and a desire to improve the code's design and performance.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,1,0,0,,,Documentation,Documentation,,Documentation,Mild,Mild,,Mild
42017098,42017098,Using int field to improve read performance of byte array access in Java?,"<p>This is a question to better understand the Java JVM.</p>

<p>Let's say we have class with an array of 16 bytes. It could more or less than 16. Just 16 as an example.
Each byte is used as a collection of 8 flags. Lots of reads. Few writes.</p>

<p>What is the performance impact of creating 4 int fields by fusing 4 bytes into <code>ints</code>  fields. This removes the need for array bound checks when reading those flags. However you need to shift input flags to address the correct bits in the <code>int</code>.</p>

<p>Flag writes are done both on the byte array and the <code>ints</code> cache.
Reason might be the byte array is used elsewhere. We don't know. We just know we have those bytes and we have to access flags on it many many times.</p>

<p>Is it worth it to introduce the <code>int</code> cache in the class implementation?</p>

<p>EDIT: I realized writes by others will not be pushed to the <code>int</code> cache. So the cache will give false flags. But let's imagine, there is a system wide event for cache refresh.</p>
","this is a question to better understand the java jvm
lets say we have class with an array of 16 bytes it could more or less than 16 just 16 as an example
each byte is used as a collection of 8 flags lots of reads few writes
what is the performance impact of creating 4 int fields by fusing 4 bytes into ints  fields this removes the need for array bound checks when reading those flags however you need to shift input flags to address the correct bits in the int
flag writes are done both on the byte array and the ints cache
reason might be the byte array is used elsewhere we dont know we just know we have those bytes and we have to access flags on it many many times
is it worth it to introduce the int cache in the class implementation
edit i realized writes by others will not be pushed to the int cache so the cache will give false flags but lets imagine there is a system wide event for cache refresh
",1,"This is a question to better understand the Java JVM.
Let's say we have class with an array of 16 bytes. It could more or less than 16. Just 16 as an example.
Each byte is used as a collection of 8 flags. Lots of reads. Few writes.
What is the performance impact of creating 4 int fields by fusing 4 bytes into ints  fields. This removes the need for array bound checks when reading those flags. However you need to shift input flags to address the correct bits in the int.
Flag writes are done both on the byte array and the ints cache.
Reason might be the byte array is used elsewhere. We don't know. We just know we have those bytes and we have to access flags on it many many times.
Is it worth it to introduce the int cache in the class implementation?
EDIT: I realized writes by others will not be pushed to the int cache. So the cache will give false flags. But let's imagine, there is a system wide event for cache refresh.
","Using int field to improve read performance of byte array access in Java?
This is a question to better understand the Java JVM.
Let's say we have class with an array of 16 bytes. It could more or less than 16. Just 16 as an example.
Each byte is used as a collection of 8 flags. Lots of reads. Few writes.
What is the performance impact of creating 4 int fields by fusing 4 bytes into ints  fields. This removes the need for array bound checks when reading those flags. However you need to shift input flags to address the correct bits in the int.
Flag writes are done both on the byte array and the ints cache.
Reason might be the byte array is used elsewhere. We don't know. We just know we have those bytes and we have to access flags on it many many times.
Is it worth it to introduce the int cache in the class implementation?
EDIT: I realized writes by others will not be pushed to the int cache. So the cache will give false flags. But let's imagine, there is a system wide event for cache refresh.
",0,0,1,1,0,0,0,- Is a theoretical question,"Label: 1

Reasoning:

This question expresses technical debt because it discusses a potential optimization technique to improve performance, which implies that the current implementation may be suboptimal. The user is considering introducing an int cache to reduce array bound checks when reading flags, which suggests that the current implementation may be experiencing performance issues.

Several factors indicate technical debt:

1. The user mentions ""Lots of reads"" and ""many many times"" access to the flags, implying that the current implementation may be inefficient.
2. The proposed optimization aims to remove array bound checks, which could improve performance.
3. The user is weighing the benefits of introducing an int cache against the added complexity, suggesting that the current implementation may not be ideal.
4. The edit mentioning a system-wide event for cache refresh implies that the current implementation may not handle cache updates efficiently.

The user is not simply asking about a conceptual understanding of Java JVM or programming principles, but rather is seeking advice on optimizing a specific implementation. The discussion of trade-offs and potential performance improvements suggests that the current code may be in need of refactoring or optimization, which is a hallmark of technical debt.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,1,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
21565234,21565234,"ARRAYS DEBUGGING incorrect outputs, complex algorithm","<p>I made this algorithm, i was debugging it to see why it wasnt working, but then i started getting weird stuff while printing arrays at the end of each cycle to see where the problem first occurred.
At a first glance, it seemed my while cycles didn't take into consideration the last array value, but i dunno...
all info about algorithm and everything is in the source.
What i'd like to understand is, primarily, the answer to this question:
Why does the output change sometimes?? If i run the program, 60-70% of the time i get answer 14 (which should be wrong), but some other times i get weird stuff as the result...why??
how can i debug the code if i keep getting different results....plus, if i compile for release and not debug (running codeblocks under latest gcc available in debian sid here), i get most of the times 9 as result.</p>

<p>CODE:</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;vector&gt;
/*void print_array
{
    std::cout&lt;&lt;"" ( "";
    for (int i = 0; i &lt; n; i++) { std::cout&lt;&lt;array[i]&lt;&lt;"" ""; }
    std::cout&lt;&lt;"")""&lt;&lt;std::endl;
}*/



///this algorithm must take an array of elements and return the maximum achievable sum
///within any of the sub-arrays (or sub-segments) of the array (the sum must be composed of adjacent numbers within the array)

///it will squeeze the array ...(...positive numbers...)(...negative numbers...)(...positive numbers...)...
///into ...(positive number)(negative number)(positive number)...

///then it will 'remove' any negative numbers in case it would be convienent so that the sum between 2 positive numbers
///separated by 1 negative number would result in the highest achievable number, like this:
// -- (3,-4,4) if u do 'remove' the negative number in order to unite the positive ones, i will get 3-4+4=3. So it would
// be better not to remove the negative number, and let 4 be the highest number achievable, without any sums
// -- (3,-1,4) in this case removing -1 will result in 3-1+4=6, 6 is bigger than both 3 and 4, so it would be convienent to remove the
// negative number and sum all of the three up into one number
///so what this step does is shrink the array furthermore if it is possible to 'remove' any negatives in a smart way
///i also make it reiterate for as long as there is no more shrinking available, because if you think about it not always
///can the pc know if, after a shrinking has occured, there are more shrinkings to be done

///then, lastly, it will calculate which of the positive numbers left is highest, and it will choose that as remaining maximum sum :)

///expected result for the array of input, s[], would be (i think), 7

int main() {
const int n=4;
int s[n+1]={3,-2,4,-4,6};
int k[n+1]={0};
///PRINT ARRAY, FOR DEBUG
std::cout&lt;&lt;"" ( "";
    for (int i = 0; i &lt;= n; i++) { std::cout&lt;&lt;k[i]&lt;&lt;"" ""; }
    std::cout&lt;&lt;"")""&lt;&lt;std::endl;
int i=0, j=0;
// step 1: compress negative and postive subsegments of array s[] into single numbers within array k[]
/*while (i&lt;=n)
{
    while (s[i]&gt;=0)
    {
        k[j]+=s[i]; ++i;
    }
    ++j;
    while (s[i]&lt;0)
    {
        k[j]+=s[i]; ++i;
    }
    ++j;
}*/

while (i&lt;=n)
{

    while (s[i]&gt;=0)
    {
        if (i&gt;n) break;
        k[j]+=s[i]; ++i;
    }
    ++j;
    while (s[i]&lt;0)
    {
        if (i&gt;n) break;
        k[j]+=s[i]; ++i;
    }
    ++j;
}















std::cout&lt;&lt;""STEP 1 : "";
///PRINT ARRAY, FOR DEBUG
std::cout&lt;&lt;"" ( "";
    for (int i = 0; i &lt;= n; i++) { std::cout&lt;&lt;k[i]&lt;&lt;"" ""; }
    std::cout&lt;&lt;"")""&lt;&lt;std::endl;



j=0;
// step 2: remove negative numbers when handy
std::cout&lt;&lt;""checked WRONG! ""&lt;&lt;unsigned(k[3])&lt;&lt;std::endl;
int p=1;
while (p!=0)
{
    p=0;
    while (j&lt;=n)
    {
        std::cout&lt;&lt;""checked right! ""&lt;&lt;unsigned(k[j+1])&lt;&lt;std::endl;
        if (k[j]&lt;=0) { ++j; continue;}
        if ( k[j]&gt;unsigned(k[j+1]) &amp;&amp; k[j+2]&gt;unsigned(k[j+1]) )
        {
            std::cout&lt;&lt;""checked right!""&lt;&lt;std::endl;
            k[j+2]=k[j]+k[j+1]+k[j+2];
            k[j]=0; k[j+1]=0;
            ++p;
        }
        j+=2;
    }
}
std::cout&lt;&lt;""STEP 2 : "";
///PRINT ARRAY, FOR DEBUG
std::cout&lt;&lt;"" ( "";
    for (int i = 0; i &lt;= n; i++) { std::cout&lt;&lt;k[i]&lt;&lt;"" ""; }
    std::cout&lt;&lt;"")""&lt;&lt;std::endl;


j=0; i=0; //i will now use ""i"" and ""p"" variables for completely different purposes, as not to waste memory
// i will be final value that algorithm needed to find
// p will be a value to put within i if it is the biggest number found yet, it will keep changing as i go through the array....

// step 3: check which positive number is bigger: IT IS THE MAX ACHIEVABLE SUM!!
while (j&lt;=n)
{
    if(k[j]&lt;=0) { ++j; continue; }
    p=k[j]; if (p&gt;i) { std::swap(p,i); }
    j+=2;
}

std::cout&lt;&lt;std::endl&lt;&lt;""MAX ACHIEVABLE SUM WITHIN SUBSEGMENTS OF ARRAY : ""&lt;&lt;i&lt;&lt;std::endl;

return 0;
}
</code></pre>

<p>might there be problems because im not using vectors??
Thanks for your help!</p>

<p><strong>EDIT:</strong> i found both my algorithm bugs!
one is the one mentioned by user m24p, found in step 1 of the algorithm, which i fixed with a kinda-ugly get-around which ill get to cleaning up later...
the other is found in step2. it seems that in the while expression check, where i check something against unsigned values of the array, what is really checked is that something agains unsigned values of some weird numbers.
i tested it, with simple cout output:
IF i do unsigned(k[anyindexofk]) and the value contained in that spot is a positive number, i get the positive number of course which is unsigned
IF that number is negative though, the value won't be simply unsigned, but look very different, like i stepped over the array or something...i get this number ""4294967292"" when im instead expecting -2 to return as 2 or -4 to be 4.
(that number is for -4, -2 gives 4294967294)</p>

<p>I edited the sources with my new stuff, thanks for the help!</p>

<p><strong>EDIT 2:</strong> nvm i resolved with std::abs() using cmath libs of c++
would there have been any other ways without using abs?</p>
","i made this algorithm i was debugging it to see why it wasnt working but then i started getting weird stuff while printing arrays at the end of each cycle to see where the problem first occurred
at a first glance it seemed my while cycles didnt take into consideration the last array value but i dunno
all info about algorithm and everything is in the source
what id like to understand is primarily the answer to this question
why does the output change sometimes if i run the program 6070 of the time i get answer 14 which should be wrong but some other times i get weird stuff as the resultwhy
how can i debug the code if i keep getting different resultsplus if i compile for release and not debug running codeblocks under latest gcc available in debian sid here i get most of the times 9 as result
code
might there be problems because im not using vectors
thanks for your help
edit i found both my algorithm bugs
one is the one mentioned by user m24p found in step 1 of the algorithm which i fixed with a kindaugly getaround which ill get to cleaning up later
the other is found in step2 it seems that in the while expression check where i check something against unsigned values of the array what is really checked is that something agains unsigned values of some weird numbers
i tested it with simple cout output
if i do unsignedkanyindexofk and the value contained in that spot is a positive number i get the positive number of course which is unsigned
if that number is negative though the value wont be simply unsigned but look very different like i stepped over the array or somethingi get this number 4294967292 when im instead expecting 2 to return as 2 or 4 to be 4
that number is for 4 2 gives 4294967294
i edited the sources with my new stuff thanks for the help
edit 2 nvm i resolved with stdabs using cmath libs of c
would there have been any other ways without using abs
",1,"I made this algorithm, i was debugging it to see why it wasnt working, but then i started getting weird stuff while printing arrays at the end of each cycle to see where the problem first occurred.
At a first glance, it seemed my while cycles didn't take into consideration the last array value, but i dunno...
all info about algorithm and everything is in the source.
What i'd like to understand is, primarily, the answer to this question:
Why does the output change sometimes?? If i run the program, 60-70% of the time i get answer 14 (which should be wrong), but some other times i get weird stuff as the result...why??
how can i debug the code if i keep getting different results....plus, if i compile for release and not debug (running codeblocks under latest gcc available in debian sid here), i get most of the times 9 as result.
CODE:
#include <iostream>
#include <vector>
/*void print_array
{
    std::cout<<"" ( "";
    for (int i = 0; i < n; i++) { std::cout<<array[i]<<"" ""; }
    std::cout<<"")""<<std::endl;
}*/



///this algorithm must take an array of elements and return the maximum achievable sum
///within any of the sub-arrays (or sub-segments) of the array (the sum must be composed of adjacent numbers within the array)

///it will squeeze the array ...(...positive numbers...)(...negative numbers...)(...positive numbers...)...
///into ...(positive number)(negative number)(positive number)...

///then it will 'remove' any negative numbers in case it would be convienent so that the sum between 2 positive numbers
///separated by 1 negative number would result in the highest achievable number, like this:
// -- (3,-4,4) if u do 'remove' the negative number in order to unite the positive ones, i will get 3-4+4=3. So it would
// be better not to remove the negative number, and let 4 be the highest number achievable, without any sums
// -- (3,-1,4) in this case removing -1 will result in 3-1+4=6, 6 is bigger than both 3 and 4, so it would be convienent to remove the
// negative number and sum all of the three up into one number
///so what this step does is shrink the array furthermore if it is possible to 'remove' any negatives in a smart way
///i also make it reiterate for as long as there is no more shrinking available, because if you think about it not always
///can the pc know if, after a shrinking has occured, there are more shrinkings to be done

///then, lastly, it will calculate which of the positive numbers left is highest, and it will choose that as remaining maximum sum :)

///expected result for the array of input, s[], would be (i think), 7

int main() {
const int n=4;
int s[n+1]={3,-2,4,-4,6};
int k[n+1]={0};
///PRINT ARRAY, FOR DEBUG
std::cout<<"" ( "";
    for (int i = 0; i <= n; i++) { std::cout<<k[i]<<"" ""; }
    std::cout<<"")""<<std::endl;
int i=0, j=0;
// step 1: compress negative and postive subsegments of array s[] into single numbers within array k[]
/*while (i<=n)
{
    while (s[i]>=0)
    {
        k[j]+=s[i]; ++i;
    }
    ++j;
    while (s[i]<0)
    {
        k[j]+=s[i]; ++i;
    }
    ++j;
}*/

while (i<=n)
{

    while (s[i]>=0)
    {
        if (i>n) break;
        k[j]+=s[i]; ++i;
    }
    ++j;
    while (s[i]<0)
    {
        if (i>n) break;
        k[j]+=s[i]; ++i;
    }
    ++j;
}















std::cout<<""STEP 1 : "";
///PRINT ARRAY, FOR DEBUG
std::cout<<"" ( "";
    for (int i = 0; i <= n; i++) { std::cout<<k[i]<<"" ""; }
    std::cout<<"")""<<std::endl;



j=0;
// step 2: remove negative numbers when handy
std::cout<<""checked WRONG! ""<<unsigned(k[3])<<std::endl;
int p=1;
while (p!=0)
{
    p=0;
    while (j<=n)
    {
        std::cout<<""checked right! ""<<unsigned(k[j+1])<<std::endl;
        if (k[j]<=0) { ++j; continue;}
        if ( k[j]>unsigned(k[j+1]) && k[j+2]>unsigned(k[j+1]) )
        {
            std::cout<<""checked right!""<<std::endl;
            k[j+2]=k[j]+k[j+1]+k[j+2];
            k[j]=0; k[j+1]=0;
            ++p;
        }
        j+=2;
    }
}
std::cout<<""STEP 2 : "";
///PRINT ARRAY, FOR DEBUG
std::cout<<"" ( "";
    for (int i = 0; i <= n; i++) { std::cout<<k[i]<<"" ""; }
    std::cout<<"")""<<std::endl;


j=0; i=0; //i will now use ""i"" and ""p"" variables for completely different purposes, as not to waste memory
// i will be final value that algorithm needed to find
// p will be a value to put within i if it is the biggest number found yet, it will keep changing as i go through the array....

// step 3: check which positive number is bigger: IT IS THE MAX ACHIEVABLE SUM!!
while (j<=n)
{
    if(k[j]<=0) { ++j; continue; }
    p=k[j]; if (p>i) { std::swap(p,i); }
    j+=2;
}

std::cout<<std::endl<<""MAX ACHIEVABLE SUM WITHIN SUBSEGMENTS OF ARRAY : ""<<i<<std::endl;

return 0;
}

might there be problems because im not using vectors??
Thanks for your help!
EDIT: i found both my algorithm bugs!
one is the one mentioned by user m24p, found in step 1 of the algorithm, which i fixed with a kinda-ugly get-around which ill get to cleaning up later...
the other is found in step2. it seems that in the while expression check, where i check something against unsigned values of the array, what is really checked is that something agains unsigned values of some weird numbers.
i tested it, with simple cout output:
IF i do unsigned(k[anyindexofk]) and the value contained in that spot is a positive number, i get the positive number of course which is unsigned
IF that number is negative though, the value won't be simply unsigned, but look very different, like i stepped over the array or something...i get this number ""4294967292"" when im instead expecting -2 to return as 2 or -4 to be 4.
(that number is for -4, -2 gives 4294967294)
I edited the sources with my new stuff, thanks for the help!
EDIT 2: nvm i resolved with std::abs() using cmath libs of c++
would there have been any other ways without using abs?
","ARRAYS DEBUGGING incorrect outputs, complex algorithm
I made this algorithm, i was debugging it to see why it wasnt working, but then i started getting weird stuff while printing arrays at the end of each cycle to see where the problem first occurred.
At a first glance, it seemed my while cycles didn't take into consideration the last array value, but i dunno...
all info about algorithm and everything is in the source.
What i'd like to understand is, primarily, the answer to this question:
Why does the output change sometimes?? If i run the program, 60-70% of the time i get answer 14 (which should be wrong), but some other times i get weird stuff as the result...why??
how can i debug the code if i keep getting different results....plus, if i compile for release and not debug (running codeblocks under latest gcc available in debian sid here), i get most of the times 9 as result.
CODE:
#include <iostream>
#include <vector>
/*void print_array
{
    std::cout<<"" ( "";
    for (int i = 0; i < n; i++) { std::cout<<array[i]<<"" ""; }
    std::cout<<"")""<<std::endl;
}*/



///this algorithm must take an array of elements and return the maximum achievable sum
///within any of the sub-arrays (or sub-segments) of the array (the sum must be composed of adjacent numbers within the array)

///it will squeeze the array ...(...positive numbers...)(...negative numbers...)(...positive numbers...)...
///into ...(positive number)(negative number)(positive number)...

///then it will 'remove' any negative numbers in case it would be convienent so that the sum between 2 positive numbers
///separated by 1 negative number would result in the highest achievable number, like this:
// -- (3,-4,4) if u do 'remove' the negative number in order to unite the positive ones, i will get 3-4+4=3. So it would
// be better not to remove the negative number, and let 4 be the highest number achievable, without any sums
// -- (3,-1,4) in this case removing -1 will result in 3-1+4=6, 6 is bigger than both 3 and 4, so it would be convienent to remove the
// negative number and sum all of the three up into one number
///so what this step does is shrink the array furthermore if it is possible to 'remove' any negatives in a smart way
///i also make it reiterate for as long as there is no more shrinking available, because if you think about it not always
///can the pc know if, after a shrinking has occured, there are more shrinkings to be done

///then, lastly, it will calculate which of the positive numbers left is highest, and it will choose that as remaining maximum sum :)

///expected result for the array of input, s[], would be (i think), 7

int main() {
const int n=4;
int s[n+1]={3,-2,4,-4,6};
int k[n+1]={0};
///PRINT ARRAY, FOR DEBUG
std::cout<<"" ( "";
    for (int i = 0; i <= n; i++) { std::cout<<k[i]<<"" ""; }
    std::cout<<"")""<<std::endl;
int i=0, j=0;
// step 1: compress negative and postive subsegments of array s[] into single numbers within array k[]
/*while (i<=n)
{
    while (s[i]>=0)
    {
        k[j]+=s[i]; ++i;
    }
    ++j;
    while (s[i]<0)
    {
        k[j]+=s[i]; ++i;
    }
    ++j;
}*/

while (i<=n)
{

    while (s[i]>=0)
    {
        if (i>n) break;
        k[j]+=s[i]; ++i;
    }
    ++j;
    while (s[i]<0)
    {
        if (i>n) break;
        k[j]+=s[i]; ++i;
    }
    ++j;
}















std::cout<<""STEP 1 : "";
///PRINT ARRAY, FOR DEBUG
std::cout<<"" ( "";
    for (int i = 0; i <= n; i++) { std::cout<<k[i]<<"" ""; }
    std::cout<<"")""<<std::endl;



j=0;
// step 2: remove negative numbers when handy
std::cout<<""checked WRONG! ""<<unsigned(k[3])<<std::endl;
int p=1;
while (p!=0)
{
    p=0;
    while (j<=n)
    {
        std::cout<<""checked right! ""<<unsigned(k[j+1])<<std::endl;
        if (k[j]<=0) { ++j; continue;}
        if ( k[j]>unsigned(k[j+1]) && k[j+2]>unsigned(k[j+1]) )
        {
            std::cout<<""checked right!""<<std::endl;
            k[j+2]=k[j]+k[j+1]+k[j+2];
            k[j]=0; k[j+1]=0;
            ++p;
        }
        j+=2;
    }
}
std::cout<<""STEP 2 : "";
///PRINT ARRAY, FOR DEBUG
std::cout<<"" ( "";
    for (int i = 0; i <= n; i++) { std::cout<<k[i]<<"" ""; }
    std::cout<<"")""<<std::endl;


j=0; i=0; //i will now use ""i"" and ""p"" variables for completely different purposes, as not to waste memory
// i will be final value that algorithm needed to find
// p will be a value to put within i if it is the biggest number found yet, it will keep changing as i go through the array....

// step 3: check which positive number is bigger: IT IS THE MAX ACHIEVABLE SUM!!
while (j<=n)
{
    if(k[j]<=0) { ++j; continue; }
    p=k[j]; if (p>i) { std::swap(p,i); }
    j+=2;
}

std::cout<<std::endl<<""MAX ACHIEVABLE SUM WITHIN SUBSEGMENTS OF ARRAY : ""<<i<<std::endl;

return 0;
}

might there be problems because im not using vectors??
Thanks for your help!
EDIT: i found both my algorithm bugs!
one is the one mentioned by user m24p, found in step 1 of the algorithm, which i fixed with a kinda-ugly get-around which ill get to cleaning up later...
the other is found in step2. it seems that in the while expression check, where i check something against unsigned values of the array, what is really checked is that something agains unsigned values of some weird numbers.
i tested it, with simple cout output:
IF i do unsigned(k[anyindexofk]) and the value contained in that spot is a positive number, i get the positive number of course which is unsigned
IF that number is negative though, the value won't be simply unsigned, but look very different, like i stepped over the array or something...i get this number ""4294967292"" when im instead expecting -2 to return as 2 or -4 to be 4.
(that number is for -4, -2 gives 4294967294)
I edited the sources with my new stuff, thanks for the help!
EDIT 2: nvm i resolved with std::abs() using cmath libs of c++
would there have been any other ways without using abs?
",1,0,1,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,0,1,0,0,0,,,Test,Test,,Test,Mild,Mild,,Mild
47443122,47443122,"Python's scipy.optimize.minimize with SLSQP fails with ""Positive directional derivative for linesearch""","<p>I have a least squares minimization problem subject to inequality constraints which I am trying to solve using scipy.optimize.minimize.  It seems that there are two options for inequality constraints: COBYLA and SLSQP.  </p>

<p>I first tried SLSQP since it allow for explicit partial derivatives of the function to be minimized.  Depending on the scaling of the problem, it fails with error: </p>

<pre><code>Positive directional derivative for linesearch    (Exit mode 8)
</code></pre>

<p>whenever interval or more general inequality constraints are imposed. </p>

<p>This has been observed previously e.g., <a href=""https://stackoverflow.com/questions/11155721/positive-directional-derivative-for-linesearch"">here</a>.  Manual scaling of the function to be minimized (along with the associated partial derivatives) seems to get rid of the problem, but I cannot achieve the same effect by changing ftol in the options.</p>

<p>Overall, this whole thing is causing me to have doubts about the routine working in a robust manner.  Here's a simplified example:</p>

<pre><code>import numpy as np
import scipy.optimize as sp_optimize

def cost(x, A, y):

    e = y - A.dot(x)
    rss = np.sum(e ** 2)

    return rss

def cost_deriv(x, A, y):

    e = y - A.dot(x)
    deriv0 = -2 * e.dot(A[:,0])
    deriv1 = -2 * e.dot(A[:,1])

    deriv = np.array([deriv0, deriv1])

    return deriv


A = np.ones((10,2)); A[:,0] = np.linspace(-5,5, 10)
x_true = np.array([2, 2/20])
y = A.dot(x_true)
x_guess = x_true / 2

prm_bounds = ((0, 3), (0,1))

cons_SLSQP = ({'type': 'ineq', 'fun' : lambda x: np.array([x[0] - x[1]]),
               'jac' : lambda x: np.array([1.0, -1.0])})

# works correctly
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv, bounds=prm_bounds, method='SLSQP', constraints=cons_SLSQP, options={'disp': True})
print(min_res_SLSQP)

# fails
A = 100 * A
y = A.dot(x_true)
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv, bounds=prm_bounds, method='SLSQP', constraints=cons_SLSQP, options={'disp': True})
print(min_res_SLSQP)

# works if bounds and inequality constraints removed
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv,
method='SLSQP', options={'disp': True})
print(min_res_SLSQP)
</code></pre>

<p>How should ftol be set to avoid failure?  More generally, can a similar problem arise with COBYLA?   Is COBYLA a better choice for this type of inequality constrained least squares optimization problem?</p>

<p>Using a square root in the cost function was found to improve performance.  However, for a non-linear re-paramterization of the problem (simpler but closer to what I need to do in practice), it fails again.  Here are the details:</p>

<pre><code>import numpy as np
import scipy.optimize as sp_optimize


def cost(x, y, g):

    e = ((y - x[1]) / x[0]) - g

    rss = np.sqrt(np.sum(e ** 2))

    return rss


def cost_deriv(x, y, g):

    e = ((y- x[1]) / x[0]) - g

    factor = 0.5 / np.sqrt(e.dot(e))
    deriv0 = -2 * factor * e.dot(y - x[1]) / (x[0]**2)
    deriv1 = -2 * factor * np.sum(e) / x[0]

    deriv = np.array([deriv0, deriv1])

    return deriv


x_true = np.array([1/300, .1])
N = 20
t = 20 * np.arange(N)
g = 100 * np.cos(2 * np.pi * 1e-3 * (t - t[-1] / 2))
y = g * x_true[0] + x_true[1]

x_guess = x_true / 2
prm_bounds = ((1e-4, 1e-2), (0, .4))

# check derivatives
delta = 1e-9
C0 = cost(x_guess, y, g)
C1 = cost(x_guess + np.array([delta, 0]), y, g)
approx_deriv0 = (C1 - C0) / delta
C1 = cost(x_guess + np.array([0, delta]), y, g)
approx_deriv1 = (C1 - C0) / delta
approx_deriv = np.array([approx_deriv0, approx_deriv1])
deriv = cost_deriv(x_guess, y, g)

# fails
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(y, g), jac=cost_deriv,
bounds=prm_bounds, method='SLSQP', options={'disp': True})
print(min_res_SLSQP)
</code></pre>
","i have a least squares minimization problem subject to inequality constraints which i am trying to solve using scipyoptimizeminimize  it seems that there are two options for inequality constraints cobyla and slsqp  
i first tried slsqp since it allow for explicit partial derivatives of the function to be minimized  depending on the scaling of the problem it fails with error 
whenever interval or more general inequality constraints are imposed 
this has been observed previously eg here  manual scaling of the function to be minimized along with the associated partial derivatives seems to get rid of the problem but i cannot achieve the same effect by changing ftol in the options
overall this whole thing is causing me to have doubts about the routine working in a robust manner  heres a simplified example
how should ftol be set to avoid failure  more generally can a similar problem arise with cobyla   is cobyla a better choice for this type of inequality constrained least squares optimization problem
using a square root in the cost function was found to improve performance  however for a nonlinear reparamterization of the problem simpler but closer to what i need to do in practice it fails again  here are the details
",1,"I have a least squares minimization problem subject to inequality constraints which I am trying to solve using scipy.optimize.minimize.  It seems that there are two options for inequality constraints: COBYLA and SLSQP.  
I first tried SLSQP since it allow for explicit partial derivatives of the function to be minimized.  Depending on the scaling of the problem, it fails with error: 
Positive directional derivative for linesearch    (Exit mode 8)

whenever interval or more general inequality constraints are imposed. 
This has been observed previously e.g., here.  Manual scaling of the function to be minimized (along with the associated partial derivatives) seems to get rid of the problem, but I cannot achieve the same effect by changing ftol in the options.
Overall, this whole thing is causing me to have doubts about the routine working in a robust manner.  Here's a simplified example:
import numpy as np
import scipy.optimize as sp_optimize

def cost(x, A, y):

    e = y - A.dot(x)
    rss = np.sum(e ** 2)

    return rss

def cost_deriv(x, A, y):

    e = y - A.dot(x)
    deriv0 = -2 * e.dot(A[:,0])
    deriv1 = -2 * e.dot(A[:,1])

    deriv = np.array([deriv0, deriv1])

    return deriv


A = np.ones((10,2)); A[:,0] = np.linspace(-5,5, 10)
x_true = np.array([2, 2/20])
y = A.dot(x_true)
x_guess = x_true / 2

prm_bounds = ((0, 3), (0,1))

cons_SLSQP = ({'type': 'ineq', 'fun' : lambda x: np.array([x[0] - x[1]]),
               'jac' : lambda x: np.array([1.0, -1.0])})

# works correctly
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv, bounds=prm_bounds, method='SLSQP', constraints=cons_SLSQP, options={'disp': True})
print(min_res_SLSQP)

# fails
A = 100 * A
y = A.dot(x_true)
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv, bounds=prm_bounds, method='SLSQP', constraints=cons_SLSQP, options={'disp': True})
print(min_res_SLSQP)

# works if bounds and inequality constraints removed
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv,
method='SLSQP', options={'disp': True})
print(min_res_SLSQP)

How should ftol be set to avoid failure?  More generally, can a similar problem arise with COBYLA?   Is COBYLA a better choice for this type of inequality constrained least squares optimization problem?
Using a square root in the cost function was found to improve performance.  However, for a non-linear re-paramterization of the problem (simpler but closer to what I need to do in practice), it fails again.  Here are the details:
import numpy as np
import scipy.optimize as sp_optimize


def cost(x, y, g):

    e = ((y - x[1]) / x[0]) - g

    rss = np.sqrt(np.sum(e ** 2))

    return rss


def cost_deriv(x, y, g):

    e = ((y- x[1]) / x[0]) - g

    factor = 0.5 / np.sqrt(e.dot(e))
    deriv0 = -2 * factor * e.dot(y - x[1]) / (x[0]**2)
    deriv1 = -2 * factor * np.sum(e) / x[0]

    deriv = np.array([deriv0, deriv1])

    return deriv


x_true = np.array([1/300, .1])
N = 20
t = 20 * np.arange(N)
g = 100 * np.cos(2 * np.pi * 1e-3 * (t - t[-1] / 2))
y = g * x_true[0] + x_true[1]

x_guess = x_true / 2
prm_bounds = ((1e-4, 1e-2), (0, .4))

# check derivatives
delta = 1e-9
C0 = cost(x_guess, y, g)
C1 = cost(x_guess + np.array([delta, 0]), y, g)
approx_deriv0 = (C1 - C0) / delta
C1 = cost(x_guess + np.array([0, delta]), y, g)
approx_deriv1 = (C1 - C0) / delta
approx_deriv = np.array([approx_deriv0, approx_deriv1])
deriv = cost_deriv(x_guess, y, g)

# fails
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(y, g), jac=cost_deriv,
bounds=prm_bounds, method='SLSQP', options={'disp': True})
print(min_res_SLSQP)

","Python's scipy.optimize.minimize with SLSQP fails with ""Positive directional derivative for linesearch""
I have a least squares minimization problem subject to inequality constraints which I am trying to solve using scipy.optimize.minimize.  It seems that there are two options for inequality constraints: COBYLA and SLSQP.  
I first tried SLSQP since it allow for explicit partial derivatives of the function to be minimized.  Depending on the scaling of the problem, it fails with error: 
Positive directional derivative for linesearch    (Exit mode 8)

whenever interval or more general inequality constraints are imposed. 
This has been observed previously e.g., here.  Manual scaling of the function to be minimized (along with the associated partial derivatives) seems to get rid of the problem, but I cannot achieve the same effect by changing ftol in the options.
Overall, this whole thing is causing me to have doubts about the routine working in a robust manner.  Here's a simplified example:
import numpy as np
import scipy.optimize as sp_optimize

def cost(x, A, y):

    e = y - A.dot(x)
    rss = np.sum(e ** 2)

    return rss

def cost_deriv(x, A, y):

    e = y - A.dot(x)
    deriv0 = -2 * e.dot(A[:,0])
    deriv1 = -2 * e.dot(A[:,1])

    deriv = np.array([deriv0, deriv1])

    return deriv


A = np.ones((10,2)); A[:,0] = np.linspace(-5,5, 10)
x_true = np.array([2, 2/20])
y = A.dot(x_true)
x_guess = x_true / 2

prm_bounds = ((0, 3), (0,1))

cons_SLSQP = ({'type': 'ineq', 'fun' : lambda x: np.array([x[0] - x[1]]),
               'jac' : lambda x: np.array([1.0, -1.0])})

# works correctly
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv, bounds=prm_bounds, method='SLSQP', constraints=cons_SLSQP, options={'disp': True})
print(min_res_SLSQP)

# fails
A = 100 * A
y = A.dot(x_true)
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv, bounds=prm_bounds, method='SLSQP', constraints=cons_SLSQP, options={'disp': True})
print(min_res_SLSQP)

# works if bounds and inequality constraints removed
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(A, y), jac=cost_deriv,
method='SLSQP', options={'disp': True})
print(min_res_SLSQP)

How should ftol be set to avoid failure?  More generally, can a similar problem arise with COBYLA?   Is COBYLA a better choice for this type of inequality constrained least squares optimization problem?
Using a square root in the cost function was found to improve performance.  However, for a non-linear re-paramterization of the problem (simpler but closer to what I need to do in practice), it fails again.  Here are the details:
import numpy as np
import scipy.optimize as sp_optimize


def cost(x, y, g):

    e = ((y - x[1]) / x[0]) - g

    rss = np.sqrt(np.sum(e ** 2))

    return rss


def cost_deriv(x, y, g):

    e = ((y- x[1]) / x[0]) - g

    factor = 0.5 / np.sqrt(e.dot(e))
    deriv0 = -2 * factor * e.dot(y - x[1]) / (x[0]**2)
    deriv1 = -2 * factor * np.sum(e) / x[0]

    deriv = np.array([deriv0, deriv1])

    return deriv


x_true = np.array([1/300, .1])
N = 20
t = 20 * np.arange(N)
g = 100 * np.cos(2 * np.pi * 1e-3 * (t - t[-1] / 2))
y = g * x_true[0] + x_true[1]

x_guess = x_true / 2
prm_bounds = ((1e-4, 1e-2), (0, .4))

# check derivatives
delta = 1e-9
C0 = cost(x_guess, y, g)
C1 = cost(x_guess + np.array([delta, 0]), y, g)
approx_deriv0 = (C1 - C0) / delta
C1 = cost(x_guess + np.array([0, delta]), y, g)
approx_deriv1 = (C1 - C0) / delta
approx_deriv = np.array([approx_deriv0, approx_deriv1])
deriv = cost_deriv(x_guess, y, g)

# fails
min_res_SLSQP = sp_optimize.minimize(cost, x_guess, args=(y, g), jac=cost_deriv,
bounds=prm_bounds, method='SLSQP', options={'disp': True})
print(min_res_SLSQP)

",1,0,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,0,0,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
60473665,60473665,How do I reduce the cyclomatic complexity?,"<p>Thanks for reading my question.
I am currently taking a Java class on Coursera, and was asked to write a program on minesweeper for the assignment. My code creates the correct result, but my grade was deducted greatly because my code is â€excessively complex, with a cyclomatic complexity of 60â€œ according to the auto-grader. I understand that there are too many conditionals and loops, but I had a hard time trying to make it more simple.</p>

<p>Here is my code. It takes 3 integer command-line arguments m, n, and k to create an m-by-n grid with k mines in random locations. I use ""5"" to mark the mines instead of ""<em>"" because the highest a number in a tile can get is 4 (since a tile has 4 sides). If two mines are located side by side, extra values might be added to its marker of ""5"". So I make all the values >= 5 become ""</em>"" when I print them out. Each value is separated by two spaces.</p>

<pre><code>public class Minesweeper {
  public static void main(String[] args) {
    int m = Integer.parseInt(args[0]);
    int n = Integer.parseInt(args[1]);
    int k = Integer.parseInt(args[2]);
    int[][] mine = new int[m][n];
    //put the mines
    for(int z = 0; z &lt; k; z++) {
      int randomX = (int) (Math.random() * m);
      int randomY = (int) (Math.random() * n);
      mine[randomX][randomY] = 5; 
    }

    for(int y = 0; y &lt; n; y++) {
      for(int x = 0; x &lt; m; x++) {
        //first row of the grid
        if(y == 0) {
          //upper left corner
          if(x == 0) {
            if(mine[x + 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] &gt;= 5) {
              mine[x][y] += 1;
            }
          }
          //upper right corner
          else if(x == m - 1) {
            if(mine[x - 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] &gt;= 5) {
              mine[x][y] += 1;
            } 
          }
          //mid of first row
          else {
            if(mine[x - 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] &gt;= 5) {
              mine[x][y] += 1;
            } 
          }
        }
        //mid rows
        else if(y &gt; 0 &amp;&amp; y &lt; n - 1) {
          //left side
          if(x == 0) {
            if(mine[x][y - 1] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
          }
          //right side
          else if(x == m - 1) {
            if(mine[x][y - 1] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
          }
          //mid
          else {
            if(mine[x][y - 1] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
          } 
        }
        //bottom row
        else if(y == n - 1) {
          //bottom left corner
          if(x == 0) {
            if(mine[x + 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] &gt;= 5) {
              mine[x][y] += 1;
            }
          }
          //bottom right corner
          else if(x == m - 1) {
            if(mine[x - 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] &gt;= 5) {
              mine[x][y] += 1;
            }
          }
          //middle of the bottom row
          else {
            if(mine[x + 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] &gt;= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] &gt;= 5) {
              mine[x][y] += 1;
            }
          }
        }
      }
    }
  //print out the grid
    for(int y = 0; y &lt; n; y++) {
      for(int x = 0; x &lt; m; x++) {
        //println at the right edge of the grid
        if(x == m - 1) {
          if(mine[x][y] &gt;= 5) {
            System.out.println(""*"");
          }
          else {
            System.out.println(mine[x][y]);
          }
        }
        //other tiles, no need to switch lines
        else {
          if(mine[x][y] &gt;= 5) {
            System.out.print(""*  "");
          }
          else {
            System.out.print(mine[x][y] + ""  "");
          }
        }
      } 
    }
  }
}
</code></pre>

<p>Thank you for your time, and I'd really appreciate any suggestions.</p>
","thanks for reading my question
i am currently taking a java class on coursera and was asked to write a program on minesweeper for the assignment my code creates the correct result but my grade was deducted greatly because my code is excessively complex with a cyclomatic complexity of 60 according to the autograder i understand that there are too many conditionals and loops but i had a hard time trying to make it more simple
here is my code it takes 3 integer commandline arguments m n and k to create an mbyn grid with k mines in random locations i use 5 to mark the mines instead of  because the highest a number in a tile can get is 4 since a tile has 4 sides if two mines are located side by side extra values might be added to its marker of 5 so i make all the values  5 become  when i print them out each value is separated by two spaces
thank you for your time and id really appreciate any suggestions
",1,"Thanks for reading my question.
I am currently taking a Java class on Coursera, and was asked to write a program on minesweeper for the assignment. My code creates the correct result, but my grade was deducted greatly because my code is â€excessively complex, with a cyclomatic complexity of 60â€œ according to the auto-grader. I understand that there are too many conditionals and loops, but I had a hard time trying to make it more simple.
Here is my code. It takes 3 integer command-line arguments m, n, and k to create an m-by-n grid with k mines in random locations. I use ""5"" to mark the mines instead of """" because the highest a number in a tile can get is 4 (since a tile has 4 sides). If two mines are located side by side, extra values might be added to its marker of ""5"". So I make all the values >= 5 become """" when I print them out. Each value is separated by two spaces.
public class Minesweeper {
  public static void main(String[] args) {
    int m = Integer.parseInt(args[0]);
    int n = Integer.parseInt(args[1]);
    int k = Integer.parseInt(args[2]);
    int[][] mine = new int[m][n];
    //put the mines
    for(int z = 0; z < k; z++) {
      int randomX = (int) (Math.random() * m);
      int randomY = (int) (Math.random() * n);
      mine[randomX][randomY] = 5; 
    }

    for(int y = 0; y < n; y++) {
      for(int x = 0; x < m; x++) {
        //first row of the grid
        if(y == 0) {
          //upper left corner
          if(x == 0) {
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
          }
          //upper right corner
          else if(x == m - 1) {
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            } 
          }
          //mid of first row
          else {
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            } 
          }
        }
        //mid rows
        else if(y > 0 && y < n - 1) {
          //left side
          if(x == 0) {
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
          }
          //right side
          else if(x == m - 1) {
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
          }
          //mid
          else {
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
          } 
        }
        //bottom row
        else if(y == n - 1) {
          //bottom left corner
          if(x == 0) {
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
          }
          //bottom right corner
          else if(x == m - 1) {
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
          }
          //middle of the bottom row
          else {
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
          }
        }
      }
    }
  //print out the grid
    for(int y = 0; y < n; y++) {
      for(int x = 0; x < m; x++) {
        //println at the right edge of the grid
        if(x == m - 1) {
          if(mine[x][y] >= 5) {
            System.out.println(""*"");
          }
          else {
            System.out.println(mine[x][y]);
          }
        }
        //other tiles, no need to switch lines
        else {
          if(mine[x][y] >= 5) {
            System.out.print(""*  "");
          }
          else {
            System.out.print(mine[x][y] + ""  "");
          }
        }
      } 
    }
  }
}

Thank you for your time, and I'd really appreciate any suggestions.
","How do I reduce the cyclomatic complexity?
Thanks for reading my question.
I am currently taking a Java class on Coursera, and was asked to write a program on minesweeper for the assignment. My code creates the correct result, but my grade was deducted greatly because my code is â€excessively complex, with a cyclomatic complexity of 60â€œ according to the auto-grader. I understand that there are too many conditionals and loops, but I had a hard time trying to make it more simple.
Here is my code. It takes 3 integer command-line arguments m, n, and k to create an m-by-n grid with k mines in random locations. I use ""5"" to mark the mines instead of """" because the highest a number in a tile can get is 4 (since a tile has 4 sides). If two mines are located side by side, extra values might be added to its marker of ""5"". So I make all the values >= 5 become """" when I print them out. Each value is separated by two spaces.
public class Minesweeper {
  public static void main(String[] args) {
    int m = Integer.parseInt(args[0]);
    int n = Integer.parseInt(args[1]);
    int k = Integer.parseInt(args[2]);
    int[][] mine = new int[m][n];
    //put the mines
    for(int z = 0; z < k; z++) {
      int randomX = (int) (Math.random() * m);
      int randomY = (int) (Math.random() * n);
      mine[randomX][randomY] = 5; 
    }

    for(int y = 0; y < n; y++) {
      for(int x = 0; x < m; x++) {
        //first row of the grid
        if(y == 0) {
          //upper left corner
          if(x == 0) {
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
          }
          //upper right corner
          else if(x == m - 1) {
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            } 
          }
          //mid of first row
          else {
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            } 
          }
        }
        //mid rows
        else if(y > 0 && y < n - 1) {
          //left side
          if(x == 0) {
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
          }
          //right side
          else if(x == m - 1) {
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
          }
          //mid
          else {
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y + 1] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
          } 
        }
        //bottom row
        else if(y == n - 1) {
          //bottom left corner
          if(x == 0) {
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
          }
          //bottom right corner
          else if(x == m - 1) {
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
          }
          //middle of the bottom row
          else {
            if(mine[x + 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x - 1][y] >= 5) {
              mine[x][y] += 1;
            }
            if(mine[x][y - 1] >= 5) {
              mine[x][y] += 1;
            }
          }
        }
      }
    }
  //print out the grid
    for(int y = 0; y < n; y++) {
      for(int x = 0; x < m; x++) {
        //println at the right edge of the grid
        if(x == m - 1) {
          if(mine[x][y] >= 5) {
            System.out.println(""*"");
          }
          else {
            System.out.println(mine[x][y]);
          }
        }
        //other tiles, no need to switch lines
        else {
          if(mine[x][y] >= 5) {
            System.out.print(""*  "");
          }
          else {
            System.out.print(mine[x][y] + ""  "");
          }
        }
      } 
    }
  }
}

Thank you for your time, and I'd really appreciate any suggestions.
",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
76183069,76183069,Sync JavaScript timer with browser performance,"<p>Let's say I'm running a laggy app in the background of my computer.</p>
<p>This lag also affects the browser's performance (in this case Firefox).</p>
<p>And now, I build a JavaScript timer with this code:</p>
<pre class=""lang-js prettyprint-override""><code>let seconds = 10;
let countdown = setInterval(() =&gt; {
  if (seconds &lt;= 0) return clearInterval(countdown);
  document.querySelector(&quot;#timer&quot;).textContent = seconds;
  seconds--;
}, 1000);
</code></pre>
<p>This timer would work fine if my performance wasn't so bad.</p>
<p><img src=""https://i.stack.imgur.com/Iycoz.gif"" alt=""Delayed timer example"" /></p>
<p>Look closely at the number countdown, and you can see how the delay always changes from 1 second - 3 seconds.</p>
<p>How can I make this countdown <strong>sync with the performance</strong>, so it always decrements 1 second instead of delaying?</p>
","lets say im running a laggy app in the background of my computer
this lag also affects the browsers performance in this case firefox
and now i build a javascript timer with this code
let seconds  10
let countdown  setinterval  
  if seconds  0 return clearintervalcountdown
  documentqueryselectortimertextcontent  seconds
  seconds
 1000

this timer would work fine if my performance wasnt so bad

look closely at the number countdown and you can see how the delay always changes from 1 second  3 seconds
how can i make this countdown sync with the performance so it always decrements 1 second instead of delaying
",0,"Let's say I'm running a laggy app in the background of my computer.
This lag also affects the browser's performance (in this case Firefox).
And now, I build a JavaScript timer with this code:
let seconds = 10;
let countdown = setInterval(() => {
  if (seconds <= 0) return clearInterval(countdown);
  document.querySelector(""#timer"").textContent = seconds;
  seconds--;
}, 1000);

This timer would work fine if my performance wasn't so bad.

Look closely at the number countdown, and you can see how the delay always changes from 1 second - 3 seconds.
How can I make this countdown sync with the performance, so it always decrements 1 second instead of delaying?
","Sync JavaScript timer with browser performance
Let's say I'm running a laggy app in the background of my computer.
This lag also affects the browser's performance (in this case Firefox).
And now, I build a JavaScript timer with this code:
let seconds = 10;
let countdown = setInterval(() => {
  if (seconds <= 0) return clearInterval(countdown);
  document.querySelector(""#timer"").textContent = seconds;
  seconds--;
}, 1000);

This timer would work fine if my performance wasn't so bad.

Look closely at the number countdown, and you can see how the delay always changes from 1 second - 3 seconds.
How can I make this countdown sync with the performance, so it always decrements 1 second instead of delaying?
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
39053834,39053834,Which would be more correct and faster in C++11: switch-case or if( .. || .. || .. )?,"<p>I'm trying to build simple lexical analyzer - lexer. The part I'm working on now is tokenizer. I'm writing function which determines separators (whitespaces, tabs, newlines(CR, LF)) in the input sequence. So the question is which code is more correct:</p>
<p>The code with switch-case statement:</p>
<pre><code>bool isWhitespace(wchar_t &amp;symbol) {
    
    switch (symbol) {
        
        case L' ':
        case L'\t':
        case L'\r':
        case L'\n':
            return true;
        default:
            return false;
        
    }
    
}
</code></pre>
<p>Or the code with if(.. || .. || ..) statement:</p>
<pre><code>bool isWhitespace(wchar_t &amp;symbol) {
    
    if (symbol == L' ' ||
        symbol == L'\t' ||
        symbol == L'\r' ||
        symbol == L'\n') {

        return true;

    }
    
    return false;
    
}
</code></pre>
<p>And which one would be faster?</p>
<hr />
<p>UPD</p>
<p>Well, the result of assambler generated code and speed tests are:</p>
<p>For <code>switch-case</code>:</p>
<pre><code>__Z12isWhitespaceRw:
    pushl   %ebp
    movl    %esp, %ebp
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    movzwl  %ax, %eax
    cmpl    $13, %eax
    je  L18
    cmpl    $13, %eax
    jg  L19
    subl    $9, %eax
    cmpl    $1, %eax
    ja  L17
    jmp L18
L19:
    cmpl    $32, %eax
    jne L17
L18:
    movl    $1, %eax
    jmp L20
L17:
    movl    $0, %eax
L20:
    popl    %ebp
    ret
</code></pre>
<p>And for <code>if</code>:</p>
<pre><code>__Z12isWhitespaceRw:
    pushl   %ebp
    movl    %esp, %ebp
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $32, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $9, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $13, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $10, %ax
    jne L23
L22:
    movl    $1, %eax
    jmp L24
L23:
    movl    $0, %eax
L24:
    popl    %ebp
    ret
</code></pre>
<p>I'm not an assambler mega expert but looks like switch-case code has more jumps and less compare operations.</p>
<p>And speed measures:</p>
<h1>1</h1>
<p>Switch-case: <code>%time = 7.7</code>, <code>self = 0.01</code></p>
<p>If: <code>%time = 46.2</code>, <code>self = 0.06</code></p>
<h1>2</h1>
<p>Switch-case: <code>%time = 34.6</code>, <code>self = 0.03</code></p>
<p>If: <code>%time = 34.6</code>, <code>self = 0.03</code></p>
<hr />
<p>UPD #2</p>
<p>Yes, Unicode. Isn't it so obvious if looking at code?!</p>
","im trying to build simple lexical analyzer  lexer the part im working on now is tokenizer im writing function which determines separators whitespaces tabs newlinescr lf in the input sequence so the question is which code is more correct
the code with switchcase statement
or the code with if     statement
and which one would be faster

upd
well the result of assambler generated code and speed tests are
for switchcase
and for if
im not an assambler mega expert but looks like switchcase code has more jumps and less compare operations
and speed measures
1
switchcase time  77 self  001
if time  462 self  006
2
switchcase time  346 self  003
if time  346 self  003

upd 2
yes unicode isnt it so obvious if looking at code
",1,"I'm trying to build simple lexical analyzer - lexer. The part I'm working on now is tokenizer. I'm writing function which determines separators (whitespaces, tabs, newlines(CR, LF)) in the input sequence. So the question is which code is more correct:
The code with switch-case statement:
bool isWhitespace(wchar_t &symbol) {
    
    switch (symbol) {
        
        case L' ':
        case L'\t':
        case L'\r':
        case L'\n':
            return true;
        default:
            return false;
        
    }
    
}

Or the code with if(.. || .. || ..) statement:
bool isWhitespace(wchar_t &symbol) {
    
    if (symbol == L' ' ||
        symbol == L'\t' ||
        symbol == L'\r' ||
        symbol == L'\n') {

        return true;

    }
    
    return false;
    
}

And which one would be faster?

UPD
Well, the result of assambler generated code and speed tests are:
For switch-case:
__Z12isWhitespaceRw:
    pushl   %ebp
    movl    %esp, %ebp
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    movzwl  %ax, %eax
    cmpl    $13, %eax
    je  L18
    cmpl    $13, %eax
    jg  L19
    subl    $9, %eax
    cmpl    $1, %eax
    ja  L17
    jmp L18
L19:
    cmpl    $32, %eax
    jne L17
L18:
    movl    $1, %eax
    jmp L20
L17:
    movl    $0, %eax
L20:
    popl    %ebp
    ret

And for if:
__Z12isWhitespaceRw:
    pushl   %ebp
    movl    %esp, %ebp
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $32, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $9, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $13, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $10, %ax
    jne L23
L22:
    movl    $1, %eax
    jmp L24
L23:
    movl    $0, %eax
L24:
    popl    %ebp
    ret

I'm not an assambler mega expert but looks like switch-case code has more jumps and less compare operations.
And speed measures:
1
Switch-case: %time = 7.7, self = 0.01
If: %time = 46.2, self = 0.06
2
Switch-case: %time = 34.6, self = 0.03
If: %time = 34.6, self = 0.03

UPD #2
Yes, Unicode. Isn't it so obvious if looking at code?!
","Which would be more correct and faster in C++11: switch-case or if( .. || .. || .. )?
I'm trying to build simple lexical analyzer - lexer. The part I'm working on now is tokenizer. I'm writing function which determines separators (whitespaces, tabs, newlines(CR, LF)) in the input sequence. So the question is which code is more correct:
The code with switch-case statement:
bool isWhitespace(wchar_t &symbol) {
    
    switch (symbol) {
        
        case L' ':
        case L'\t':
        case L'\r':
        case L'\n':
            return true;
        default:
            return false;
        
    }
    
}

Or the code with if(.. || .. || ..) statement:
bool isWhitespace(wchar_t &symbol) {
    
    if (symbol == L' ' ||
        symbol == L'\t' ||
        symbol == L'\r' ||
        symbol == L'\n') {

        return true;

    }
    
    return false;
    
}

And which one would be faster?

UPD
Well, the result of assambler generated code and speed tests are:
For switch-case:
__Z12isWhitespaceRw:
    pushl   %ebp
    movl    %esp, %ebp
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    movzwl  %ax, %eax
    cmpl    $13, %eax
    je  L18
    cmpl    $13, %eax
    jg  L19
    subl    $9, %eax
    cmpl    $1, %eax
    ja  L17
    jmp L18
L19:
    cmpl    $32, %eax
    jne L17
L18:
    movl    $1, %eax
    jmp L20
L17:
    movl    $0, %eax
L20:
    popl    %ebp
    ret

And for if:
__Z12isWhitespaceRw:
    pushl   %ebp
    movl    %esp, %ebp
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $32, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $9, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $13, %ax
    je  L22
    movl    8(%ebp), %eax
    movzwl  (%eax), %eax
    cmpw    $10, %ax
    jne L23
L22:
    movl    $1, %eax
    jmp L24
L23:
    movl    $0, %eax
L24:
    popl    %ebp
    ret

I'm not an assambler mega expert but looks like switch-case code has more jumps and less compare operations.
And speed measures:
1
Switch-case: %time = 7.7, self = 0.01
If: %time = 46.2, self = 0.06
2
Switch-case: %time = 34.6, self = 0.03
If: %time = 34.6, self = 0.03

UPD #2
Yes, Unicode. Isn't it so obvious if looking at code?!
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
7862909,7862909,Procedural graph rendering optimisation: Draw only visible part?,"<h2>Background</h2>

<p>Let's say I have <strong>millions</strong> of interconnected <code>Node</code> instances which together form a graph. Each <code>Node</code> has a 2D position. A user has to be able to pan through a procedural render of this graph. Each <code>Node</code> has a <code>draw</code> method, but if I <code>draw</code> all <code>Node</code>s every frame, it's very slow.</p>

<p>As the user does not usually want to see the whole graph but instead is zoomed in, the optimisation is in <strong>not drawing the <code>Node</code>s that are off screen</strong>.</p>

<h2>My approach</h2>

<p>Divide the 2D world space into rectangular <code>Segment</code>s. Assign each <code>Node</code> to whatever <code>Segment</code> they're in. When drawing, find out first which set of <code>Segment</code>s the user's view intersects with and <code>draw</code> only the <code>Node</code>s in those <code>Segment</code>s.</p>

<h2>Now for my actual question:</h2>

<p><em><strong>How do I determine the optimal size of a <code>Segment</code>?</em></strong> (Make it too big and it's the same thing as drawing everything. Make it too small and there are again too many to iterate through.)</p>
","background
lets say i have millions of interconnected node instances which together form a graph each node has a 2d position a user has to be able to pan through a procedural render of this graph each node has a draw method but if i draw all nodes every frame its very slow
as the user does not usually want to see the whole graph but instead is zoomed in the optimisation is in not drawing the nodes that are off screen
my approach
divide the 2d world space into rectangular segments assign each node to whatever segment theyre in when drawing find out first which set of segments the users view intersects with and draw only the nodes in those segments
now for my actual question
how do i determine the optimal size of a segment make it too big and its the same thing as drawing everything make it too small and there are again too many to iterate through
",1,"Background
Let's say I have millions of interconnected Node instances which together form a graph. Each Node has a 2D position. A user has to be able to pan through a procedural render of this graph. Each Node has a draw method, but if I draw all Nodes every frame, it's very slow.
As the user does not usually want to see the whole graph but instead is zoomed in, the optimisation is in not drawing the Nodes that are off screen.
My approach
Divide the 2D world space into rectangular Segments. Assign each Node to whatever Segment they're in. When drawing, find out first which set of Segments the user's view intersects with and draw only the Nodes in those Segments.
Now for my actual question:
How do I determine the optimal size of a Segment? (Make it too big and it's the same thing as drawing everything. Make it too small and there are again too many to iterate through.)
","Procedural graph rendering optimisation: Draw only visible part?
Background
Let's say I have millions of interconnected Node instances which together form a graph. Each Node has a 2D position. A user has to be able to pan through a procedural render of this graph. Each Node has a draw method, but if I draw all Nodes every frame, it's very slow.
As the user does not usually want to see the whole graph but instead is zoomed in, the optimisation is in not drawing the Nodes that are off screen.
My approach
Divide the 2D world space into rectangular Segments. Assign each Node to whatever Segment they're in. When drawing, find out first which set of Segments the user's view intersects with and draw only the Nodes in those Segments.
Now for my actual question:
How do I determine the optimal size of a Segment? (Make it too big and it's the same thing as drawing everything. Make it too small and there are again too many to iterate through.)
",0,1,0,1,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
13317719,13317719,How to improve performance of map that loads new overlay images,"<p>I have inherited a website to maintain that uses a html map overlaying a real map to link specific countries to specific pages.</p>

<p>previously it loaded the default map image, then with some javascript it would change the image src to an image with that particular country in a different colour on mouseover and reset the image source back to the original image on mouse out</p>

<p>to make maintenance (adding new countries) easier i made the initial map a background image by utilising some CSS for the div tag, and then created new images for each country which only had that countries hightlight so that the images remain fairly small.</p>

<p>this works great but theres one issue which is particularly noticeable on slower internet connections</p>

<p>when you hover over a country if you dont have the image file in your browser cache or downloaded it wont load the image unless you hover over another country and then back onto the first country - i guess this is due to the image having to manually be downloaded on first hover.</p>

<p><strong>My question: is it possible to force the load of these extra images AFTER the page and all the other assets have finished loading so that this behaviour is all but eliminated?</strong></p>

<p>the html code for the MAP is as follows:</p>

<pre><code>    &lt;div class=""gtmap""&gt;&lt;img id=""Image-Maps_6200909211657061"" src=""&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png"" usemap=""#Image-Maps_6200909211657061"" alt=""We offer Guided Motorcycle Tours all around the world"" width=""615"" height=""296"" /&gt;
&lt;map id=""_Image-Maps_6200909211657061"" name=""Image-Maps_6200909211657061""&gt;
&lt;area shape=""poly"" coords=""511,134,532,107,542,113,520,141"" href=""/guided-motorcycle-tours-japan/"" alt=""Guided Japan Motorcycle Tours"" title=""Japan"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-japan.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;

&lt;area shape=""poly"" coords=""252,61,266,58,275,64,262,68"" href=""/guided-motorcycle-tour.php?iceland-motorcycle-adventure-39"" alt=""Guided Iceland Motorcycle Tours"" title=""Iceland""
onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-iceland.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;

&lt;area shape=""poly"" coords=""587,246,597,256,577,279,568,270"" href=""/guided-motorcycle-tour.php?new-zealand-south-island-adventure-10"" alt=""New Zealand Guided Motorcycle Tours"" title=""New Zealand"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-nz.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""418,133,412,145,412,154,421,178,430,180,430,166,443,154,443,145,438,144,433,142,430,138,431,130,430,129,425,128"" href=""/guided-motorcycle-tours-india/"" alt=""India Guided Motorcycle Tours"" title=""India"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-india.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""460,152,466,149,474,165,470,171,466,161"" href=""/guided-motorcycle-tours-laos/"" alt=""Laos Guided Motorcycle Tours"" title=""Laos"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-laos.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""468,179,475,166,468,152,475,152,482,169"" href=""/guided-motorcycle-tour.php?indochina-motorcycle-adventure-tour-32"" onClick=""javascript: pageTracker._trackPageview('/internal-links/guided-tours/map/vietnam');"" alt=""Vietnam Guided Motorcycle Tours"" title=""Vietnam"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-viet.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""330,239,337,235,347,226,352,233,351,243,344,250,335,253,327,255,323,249,322,242,323,241"" href=""/guided-motorcycle-tours-southafrica/"" alt=""South Africa Guided Motorcycle Tours"" title=""South Africa"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-sa.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""290,77,293,86,298,96,286,102,285,97,285,89,282,84,282,79"" href=""/guided-motorcycle-tour.php?great-britain-isle-of-man-scotland-wales-uk-18"" alt=""United Kingdom"" title=""United Kingdom Guided Motorcycle Tours"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-uk.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""357,118,368,118,369,126,345,129,338,125,338,117,342,115,348,116"" href=""/guided-motorcycle-tour.php?explore-turkey-adventure-45"" alt=""Turkey"" title=""Turkey Guided Motorcycle Tours"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-turkey.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""206,95,193,101,185,101,178,106,165,111,157,109,147,105,134,103,121,103,107,103,96,103,86,104,81,99,77,91,70,83,62,79,60,72,61,64,59,57,60,51,71,50,83,49,95,50,107,54,117,53,129,47,137,36,148,37,163,38,177,44,187,54,195,60,184,72,191,80,200,87"" href=""/guided-motorcycle-tours-canada/"" alt=""Guided Canada Motorcycle Tours"" title=""Canada"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-canada.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""61,75,60,62,60,55,59,44,51,44,43,43,36,42,28,43,23,48,17,51,15,62,19,74,27,79,19,83,16,93,35,83,43,77,50,75,55,75"" href=""/guided-motorcycle-tours-alaska/""  alt=""Guided Alaska Motorcycle Tours"" title=""Alaska"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-alaska.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""82,101,99,101,133,101,148,105,161,110,172,106,187,100,180,113,171,122,165,131,159,149,147,141,137,140,129,147,120,141,112,138,103,137,93,132,86,122,86,112,86,106"" href=""/guided-motorcycle-tours-usa/"" alt=""USA Guided Motorcycle Tours"" title=""USA"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-usa.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""178,225,180,214,175,208,174,204,178,198,174,193,167,192,157,199,158,204,164,211,167,218"" href=""/guided-motorcycle-tour.php?peru-machu-picchu-adventure-25"" alt=""Peru Guided Motorcycle Tours"" title=""Peru"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-peru.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""172,226,169,239,166,256,166,267,164,279,171,277,174,262,175,250,179,234,180,225,176,224"" href=""/guided-motorcycle-tours-chile/"" alt=""Guided Chile Motorcycle Tours"" title=""Chile"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-chile.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;area shape=""poly"" coords=""199,260,194,261,187,265,184,276,183,296,170,292,168,282,174,270,174,257,177,245,180,230,190,228,205,237,199,245"" href=""/guided-motorcycle-tours-argentina/"" alt=""Guided Argentina Motorcycle Tours"" title=""Argentina"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-arg.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='&lt;?php echo cdnhttpsCheck(); ?&gt;assets/wmap/a-guided-tours-map-blank.png';"" /&gt;
&lt;/map&gt;
&lt;/div&gt;
</code></pre>

<p>The <code>&lt;?php echo cdnhttpsCheck(); ?&gt;</code> is just a site specific function that gets the correct web domain/url from a config file to load resources from CDN where possible (eg all non HTTPS requests) 
We are loading Jquery at the bottom of the HTML if anybody wonders why it is missing from the code snippet</p>

<p>for reference, the page with the map in question is found here: <a href=""http://www.motoquest.com/guided-motorcycle-tours/"" rel=""nofollow"">http://www.motoquest.com/guided-motorcycle-tours/</a></p>
","i have inherited a website to maintain that uses a html map overlaying a real map to link specific countries to specific pages
previously it loaded the default map image then with some javascript it would change the image src to an image with that particular country in a different colour on mouseover and reset the image source back to the original image on mouse out
to make maintenance adding new countries easier i made the initial map a background image by utilising some css for the div tag and then created new images for each country which only had that countries hightlight so that the images remain fairly small
this works great but theres one issue which is particularly noticeable on slower internet connections
when you hover over a country if you dont have the image file in your browser cache or downloaded it wont load the image unless you hover over another country and then back onto the first country  i guess this is due to the image having to manually be downloaded on first hover
my question is it possible to force the load of these extra images after the page and all the other assets have finished loading so that this behaviour is all but eliminated
the html code for the map is as follows
the php echo cdnhttpscheck  is just a site specific function that gets the correct web domainurl from a config file to load resources from cdn where possible eg all non https requests 
we are loading jquery at the bottom of the html if anybody wonders why it is missing from the code snippet
for reference the page with the map in question is found here httpwwwmotoquestcomguidedmotorcycletours
",1,"I have inherited a website to maintain that uses a html map overlaying a real map to link specific countries to specific pages.
previously it loaded the default map image, then with some javascript it would change the image src to an image with that particular country in a different colour on mouseover and reset the image source back to the original image on mouse out
to make maintenance (adding new countries) easier i made the initial map a background image by utilising some CSS for the div tag, and then created new images for each country which only had that countries hightlight so that the images remain fairly small.
this works great but theres one issue which is particularly noticeable on slower internet connections
when you hover over a country if you dont have the image file in your browser cache or downloaded it wont load the image unless you hover over another country and then back onto the first country - i guess this is due to the image having to manually be downloaded on first hover.
My question: is it possible to force the load of these extra images AFTER the page and all the other assets have finished loading so that this behaviour is all but eliminated?
the html code for the MAP is as follows:
    <div class=""gtmap""><img id=""Image-Maps_6200909211657061"" src=""<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png"" usemap=""#Image-Maps_6200909211657061"" alt=""We offer Guided Motorcycle Tours all around the world"" width=""615"" height=""296"" />
<map id=""_Image-Maps_6200909211657061"" name=""Image-Maps_6200909211657061"">
<area shape=""poly"" coords=""511,134,532,107,542,113,520,141"" href=""/guided-motorcycle-tours-japan/"" alt=""Guided Japan Motorcycle Tours"" title=""Japan"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-japan.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />

<area shape=""poly"" coords=""252,61,266,58,275,64,262,68"" href=""/guided-motorcycle-tour.php?iceland-motorcycle-adventure-39"" alt=""Guided Iceland Motorcycle Tours"" title=""Iceland""
onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-iceland.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />

<area shape=""poly"" coords=""587,246,597,256,577,279,568,270"" href=""/guided-motorcycle-tour.php?new-zealand-south-island-adventure-10"" alt=""New Zealand Guided Motorcycle Tours"" title=""New Zealand"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-nz.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""418,133,412,145,412,154,421,178,430,180,430,166,443,154,443,145,438,144,433,142,430,138,431,130,430,129,425,128"" href=""/guided-motorcycle-tours-india/"" alt=""India Guided Motorcycle Tours"" title=""India"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-india.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""460,152,466,149,474,165,470,171,466,161"" href=""/guided-motorcycle-tours-laos/"" alt=""Laos Guided Motorcycle Tours"" title=""Laos"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-laos.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""468,179,475,166,468,152,475,152,482,169"" href=""/guided-motorcycle-tour.php?indochina-motorcycle-adventure-tour-32"" onClick=""javascript: pageTracker._trackPageview('/internal-links/guided-tours/map/vietnam');"" alt=""Vietnam Guided Motorcycle Tours"" title=""Vietnam"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-viet.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""330,239,337,235,347,226,352,233,351,243,344,250,335,253,327,255,323,249,322,242,323,241"" href=""/guided-motorcycle-tours-southafrica/"" alt=""South Africa Guided Motorcycle Tours"" title=""South Africa"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-sa.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""290,77,293,86,298,96,286,102,285,97,285,89,282,84,282,79"" href=""/guided-motorcycle-tour.php?great-britain-isle-of-man-scotland-wales-uk-18"" alt=""United Kingdom"" title=""United Kingdom Guided Motorcycle Tours"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-uk.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""357,118,368,118,369,126,345,129,338,125,338,117,342,115,348,116"" href=""/guided-motorcycle-tour.php?explore-turkey-adventure-45"" alt=""Turkey"" title=""Turkey Guided Motorcycle Tours"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-turkey.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""206,95,193,101,185,101,178,106,165,111,157,109,147,105,134,103,121,103,107,103,96,103,86,104,81,99,77,91,70,83,62,79,60,72,61,64,59,57,60,51,71,50,83,49,95,50,107,54,117,53,129,47,137,36,148,37,163,38,177,44,187,54,195,60,184,72,191,80,200,87"" href=""/guided-motorcycle-tours-canada/"" alt=""Guided Canada Motorcycle Tours"" title=""Canada"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-canada.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""61,75,60,62,60,55,59,44,51,44,43,43,36,42,28,43,23,48,17,51,15,62,19,74,27,79,19,83,16,93,35,83,43,77,50,75,55,75"" href=""/guided-motorcycle-tours-alaska/""  alt=""Guided Alaska Motorcycle Tours"" title=""Alaska"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-alaska.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""82,101,99,101,133,101,148,105,161,110,172,106,187,100,180,113,171,122,165,131,159,149,147,141,137,140,129,147,120,141,112,138,103,137,93,132,86,122,86,112,86,106"" href=""/guided-motorcycle-tours-usa/"" alt=""USA Guided Motorcycle Tours"" title=""USA"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-usa.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""178,225,180,214,175,208,174,204,178,198,174,193,167,192,157,199,158,204,164,211,167,218"" href=""/guided-motorcycle-tour.php?peru-machu-picchu-adventure-25"" alt=""Peru Guided Motorcycle Tours"" title=""Peru"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-peru.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""172,226,169,239,166,256,166,267,164,279,171,277,174,262,175,250,179,234,180,225,176,224"" href=""/guided-motorcycle-tours-chile/"" alt=""Guided Chile Motorcycle Tours"" title=""Chile"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-chile.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""199,260,194,261,187,265,184,276,183,296,170,292,168,282,174,270,174,257,177,245,180,230,190,228,205,237,199,245"" href=""/guided-motorcycle-tours-argentina/"" alt=""Guided Argentina Motorcycle Tours"" title=""Argentina"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-arg.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
</map>
</div>

The <?php echo cdnhttpsCheck(); ?> is just a site specific function that gets the correct web domain/url from a config file to load resources from CDN where possible (eg all non HTTPS requests) 
We are loading Jquery at the bottom of the HTML if anybody wonders why it is missing from the code snippet
for reference, the page with the map in question is found here: http://www.motoquest.com/guided-motorcycle-tours/
","How to improve performance of map that loads new overlay images
I have inherited a website to maintain that uses a html map overlaying a real map to link specific countries to specific pages.
previously it loaded the default map image, then with some javascript it would change the image src to an image with that particular country in a different colour on mouseover and reset the image source back to the original image on mouse out
to make maintenance (adding new countries) easier i made the initial map a background image by utilising some CSS for the div tag, and then created new images for each country which only had that countries hightlight so that the images remain fairly small.
this works great but theres one issue which is particularly noticeable on slower internet connections
when you hover over a country if you dont have the image file in your browser cache or downloaded it wont load the image unless you hover over another country and then back onto the first country - i guess this is due to the image having to manually be downloaded on first hover.
My question: is it possible to force the load of these extra images AFTER the page and all the other assets have finished loading so that this behaviour is all but eliminated?
the html code for the MAP is as follows:
    <div class=""gtmap""><img id=""Image-Maps_6200909211657061"" src=""<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png"" usemap=""#Image-Maps_6200909211657061"" alt=""We offer Guided Motorcycle Tours all around the world"" width=""615"" height=""296"" />
<map id=""_Image-Maps_6200909211657061"" name=""Image-Maps_6200909211657061"">
<area shape=""poly"" coords=""511,134,532,107,542,113,520,141"" href=""/guided-motorcycle-tours-japan/"" alt=""Guided Japan Motorcycle Tours"" title=""Japan"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-japan.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />

<area shape=""poly"" coords=""252,61,266,58,275,64,262,68"" href=""/guided-motorcycle-tour.php?iceland-motorcycle-adventure-39"" alt=""Guided Iceland Motorcycle Tours"" title=""Iceland""
onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-iceland.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />

<area shape=""poly"" coords=""587,246,597,256,577,279,568,270"" href=""/guided-motorcycle-tour.php?new-zealand-south-island-adventure-10"" alt=""New Zealand Guided Motorcycle Tours"" title=""New Zealand"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-nz.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""418,133,412,145,412,154,421,178,430,180,430,166,443,154,443,145,438,144,433,142,430,138,431,130,430,129,425,128"" href=""/guided-motorcycle-tours-india/"" alt=""India Guided Motorcycle Tours"" title=""India"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-india.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""460,152,466,149,474,165,470,171,466,161"" href=""/guided-motorcycle-tours-laos/"" alt=""Laos Guided Motorcycle Tours"" title=""Laos"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-laos.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""468,179,475,166,468,152,475,152,482,169"" href=""/guided-motorcycle-tour.php?indochina-motorcycle-adventure-tour-32"" onClick=""javascript: pageTracker._trackPageview('/internal-links/guided-tours/map/vietnam');"" alt=""Vietnam Guided Motorcycle Tours"" title=""Vietnam"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-viet.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""330,239,337,235,347,226,352,233,351,243,344,250,335,253,327,255,323,249,322,242,323,241"" href=""/guided-motorcycle-tours-southafrica/"" alt=""South Africa Guided Motorcycle Tours"" title=""South Africa"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-sa.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""290,77,293,86,298,96,286,102,285,97,285,89,282,84,282,79"" href=""/guided-motorcycle-tour.php?great-britain-isle-of-man-scotland-wales-uk-18"" alt=""United Kingdom"" title=""United Kingdom Guided Motorcycle Tours"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-uk.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""357,118,368,118,369,126,345,129,338,125,338,117,342,115,348,116"" href=""/guided-motorcycle-tour.php?explore-turkey-adventure-45"" alt=""Turkey"" title=""Turkey Guided Motorcycle Tours"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-turkey.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""206,95,193,101,185,101,178,106,165,111,157,109,147,105,134,103,121,103,107,103,96,103,86,104,81,99,77,91,70,83,62,79,60,72,61,64,59,57,60,51,71,50,83,49,95,50,107,54,117,53,129,47,137,36,148,37,163,38,177,44,187,54,195,60,184,72,191,80,200,87"" href=""/guided-motorcycle-tours-canada/"" alt=""Guided Canada Motorcycle Tours"" title=""Canada"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-canada.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""61,75,60,62,60,55,59,44,51,44,43,43,36,42,28,43,23,48,17,51,15,62,19,74,27,79,19,83,16,93,35,83,43,77,50,75,55,75"" href=""/guided-motorcycle-tours-alaska/""  alt=""Guided Alaska Motorcycle Tours"" title=""Alaska"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-alaska.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""82,101,99,101,133,101,148,105,161,110,172,106,187,100,180,113,171,122,165,131,159,149,147,141,137,140,129,147,120,141,112,138,103,137,93,132,86,122,86,112,86,106"" href=""/guided-motorcycle-tours-usa/"" alt=""USA Guided Motorcycle Tours"" title=""USA"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-usa.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""178,225,180,214,175,208,174,204,178,198,174,193,167,192,157,199,158,204,164,211,167,218"" href=""/guided-motorcycle-tour.php?peru-machu-picchu-adventure-25"" alt=""Peru Guided Motorcycle Tours"" title=""Peru"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-peru.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""172,226,169,239,166,256,166,267,164,279,171,277,174,262,175,250,179,234,180,225,176,224"" href=""/guided-motorcycle-tours-chile/"" alt=""Guided Chile Motorcycle Tours"" title=""Chile"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-chile.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
<area shape=""poly"" coords=""199,260,194,261,187,265,184,276,183,296,170,292,168,282,174,270,174,257,177,245,180,230,190,228,205,237,199,245"" href=""/guided-motorcycle-tours-argentina/"" alt=""Guided Argentina Motorcycle Tours"" title=""Argentina"" onmouseover=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-arg.png';"" onmouseout=""if(document.images) document.getElementById('Image-Maps_6200909211657061').src='<?php echo cdnhttpsCheck(); ?>assets/wmap/a-guided-tours-map-blank.png';"" />
</map>
</div>

The <?php echo cdnhttpsCheck(); ?> is just a site specific function that gets the correct web domain/url from a config file to load resources from CDN where possible (eg all non HTTPS requests) 
We are loading Jquery at the bottom of the HTML if anybody wonders why it is missing from the code snippet
for reference, the page with the map in question is found here: http://www.motoquest.com/guided-motorcycle-tours/
",0,1,1,0,0,0,1,,,,1,1,Web Performance,Web Performance,,Web Performance,0,1,1,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
6528214,6528214,Improving performance of raytracing hit function,"<p>I have a simple raytracer in python. rendering an image 200x200 takes 4 minutes, which is definitely too much for my taste. I want to improve the situation.</p>

<p>Some points: I shoot multiple rays per each pixel (to provide antialiasing) for a grand total of 16 rays per pixel. 200x200x16 is a grand total of 640000 rays. Each ray must be tested for impact on multiple Sphere objects in the scene. Ray is also a rather trivial object</p>

<pre><code>class Ray(object):
    def __init__(self, origin, direction):
        self.origin = numpy.array(origin)
        self.direction = numpy.array(direction)
</code></pre>

<p>Sphere is slightly more complex, and carries the logic for hit/nohit:</p>

<pre><code>class Sphere(object):
    def __init__(self, center, radius, color):
        self.center = numpy.array(center)
        self.radius = numpy.array(radius)
        self.color = color

    @profile 
    def hit(self, ray):
        temp = ray.origin - self.center
        a = numpy.dot(ray.direction, ray.direction)
        b = 2.0 * numpy.dot(temp, ray.direction)
        c = numpy.dot(temp, temp) - self.radius * self.radius
        disc = b * b - 4.0 * a * c

        if (disc &lt; 0.0):
            return None
        else:
            e = math.sqrt(disc)
            denom = 2.0 * a
            t = (-b - e) / denom 
            if (t &gt; 1.0e-7):
                normal = (temp + t * ray.direction) / self.radius
                hit_point = ray.origin + t * ray.direction
                return ShadeRecord.ShadeRecord(normal=normal, 
                                               hit_point=hit_point, 
                                               parameter=t, 
                                               color=self.color)           

            t = (-b + e) / denom

            if (t &gt; 1.0e-7):
                normal = (temp + t * ray.direction) / self.radius                hit_point = ray.origin + t * ray.direction
                return ShadeRecord.ShadeRecord(normal=normal, 
                                               hit_point=hit_point, 
                                               parameter=t, 
                                               color=self.color)       

        return None    
</code></pre>

<p>Now, I ran some profiling, and it appears that the longest processing time is in the hit() function</p>

<pre><code>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  2560000  118.831    0.000  152.701    0.000 raytrace/objects/Sphere.py:12(hit)
  1960020   42.989    0.000   42.989    0.000 {numpy.core.multiarray.array}
        1   34.566   34.566  285.829  285.829 raytrace/World.py:25(render)
  7680000   33.796    0.000   33.796    0.000 {numpy.core._dotblas.dot}
  2560000   11.124    0.000  163.825    0.000 raytrace/World.py:63(f)
   640000   10.132    0.000  189.411    0.000 raytrace/World.py:62(hit_bare_bones_object)
   640023    6.556    0.000  170.388    0.000 {map}
</code></pre>

<p>This does not surprise me, and I want to reduce this value as much as possible. I pass to line profiling, and the result is </p>

<pre><code>Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    12                                               @profile
    13                                               def hit(self, ray):
    14   2560000     27956358     10.9     19.2          temp = ray.origin - self.center
    15   2560000     17944912      7.0     12.3          a = numpy.dot(ray.direction, ray.direction)
    16   2560000     24132737      9.4     16.5          b = 2.0 * numpy.dot(temp, ray.direction)
    17   2560000     37113811     14.5     25.4          c = numpy.dot(temp, temp) - self.radius * self.radius
    18   2560000     20808930      8.1     14.3          disc = b * b - 4.0 * a * c
    19                                                   
    20   2560000     10963318      4.3      7.5          if (disc &lt; 0.0):
    21   2539908      5403624      2.1      3.7              return None
    22                                                   else:
    23     20092        75076      3.7      0.1              e = math.sqrt(disc)
    24     20092       104950      5.2      0.1              denom = 2.0 * a
    25     20092       115956      5.8      0.1              t = (-b - e) / denom
    26     20092        83382      4.2      0.1              if (t &gt; 1.0e-7):
    27     20092       525272     26.1      0.4                  normal = (temp + t * ray.direction) / self.radius
    28     20092       333879     16.6      0.2                  hit_point = ray.origin + t * ray.direction
    29     20092       299494     14.9      0.2                  return ShadeRecord.ShadeRecord(normal=normal, hit_point=hit_point, parameter=t, color=self.color)
</code></pre>

<p>So, it appears that most of the time is spent in this chunk of code:</p>

<pre><code>        temp = ray.origin - self.center
        a = numpy.dot(ray.direction, ray.direction)
        b = 2.0 * numpy.dot(temp, ray.direction)
        c = numpy.dot(temp, temp) - self.radius * self.radius
        disc = b * b - 4.0 * a * c
</code></pre>

<p>Where I don't really see a lot to optimize. Do you have any idea how to make this code more performant without going C ?</p>
","i have a simple raytracer in python rendering an image 200x200 takes 4 minutes which is definitely too much for my taste i want to improve the situation
some points i shoot multiple rays per each pixel to provide antialiasing for a grand total of 16 rays per pixel 200x200x16 is a grand total of 640000 rays each ray must be tested for impact on multiple sphere objects in the scene ray is also a rather trivial object
sphere is slightly more complex and carries the logic for hitnohit
now i ran some profiling and it appears that the longest processing time is in the hit function
this does not surprise me and i want to reduce this value as much as possible i pass to line profiling and the result is 
so it appears that most of the time is spent in this chunk of code
where i dont really see a lot to optimize do you have any idea how to make this code more performant without going c 
",0,"I have a simple raytracer in python. rendering an image 200x200 takes 4 minutes, which is definitely too much for my taste. I want to improve the situation.
Some points: I shoot multiple rays per each pixel (to provide antialiasing) for a grand total of 16 rays per pixel. 200x200x16 is a grand total of 640000 rays. Each ray must be tested for impact on multiple Sphere objects in the scene. Ray is also a rather trivial object
class Ray(object):
    def __init__(self, origin, direction):
        self.origin = numpy.array(origin)
        self.direction = numpy.array(direction)

Sphere is slightly more complex, and carries the logic for hit/nohit:
class Sphere(object):
    def __init__(self, center, radius, color):
        self.center = numpy.array(center)
        self.radius = numpy.array(radius)
        self.color = color

    @profile 
    def hit(self, ray):
        temp = ray.origin - self.center
        a = numpy.dot(ray.direction, ray.direction)
        b = 2.0 * numpy.dot(temp, ray.direction)
        c = numpy.dot(temp, temp) - self.radius * self.radius
        disc = b * b - 4.0 * a * c

        if (disc < 0.0):
            return None
        else:
            e = math.sqrt(disc)
            denom = 2.0 * a
            t = (-b - e) / denom 
            if (t > 1.0e-7):
                normal = (temp + t * ray.direction) / self.radius
                hit_point = ray.origin + t * ray.direction
                return ShadeRecord.ShadeRecord(normal=normal, 
                                               hit_point=hit_point, 
                                               parameter=t, 
                                               color=self.color)           

            t = (-b + e) / denom

            if (t > 1.0e-7):
                normal = (temp + t * ray.direction) / self.radius                hit_point = ray.origin + t * ray.direction
                return ShadeRecord.ShadeRecord(normal=normal, 
                                               hit_point=hit_point, 
                                               parameter=t, 
                                               color=self.color)       

        return None    

Now, I ran some profiling, and it appears that the longest processing time is in the hit() function
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  2560000  118.831    0.000  152.701    0.000 raytrace/objects/Sphere.py:12(hit)
  1960020   42.989    0.000   42.989    0.000 {numpy.core.multiarray.array}
        1   34.566   34.566  285.829  285.829 raytrace/World.py:25(render)
  7680000   33.796    0.000   33.796    0.000 {numpy.core._dotblas.dot}
  2560000   11.124    0.000  163.825    0.000 raytrace/World.py:63(f)
   640000   10.132    0.000  189.411    0.000 raytrace/World.py:62(hit_bare_bones_object)
   640023    6.556    0.000  170.388    0.000 {map}

This does not surprise me, and I want to reduce this value as much as possible. I pass to line profiling, and the result is 
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    12                                               @profile
    13                                               def hit(self, ray):
    14   2560000     27956358     10.9     19.2          temp = ray.origin - self.center
    15   2560000     17944912      7.0     12.3          a = numpy.dot(ray.direction, ray.direction)
    16   2560000     24132737      9.4     16.5          b = 2.0 * numpy.dot(temp, ray.direction)
    17   2560000     37113811     14.5     25.4          c = numpy.dot(temp, temp) - self.radius * self.radius
    18   2560000     20808930      8.1     14.3          disc = b * b - 4.0 * a * c
    19                                                   
    20   2560000     10963318      4.3      7.5          if (disc < 0.0):
    21   2539908      5403624      2.1      3.7              return None
    22                                                   else:
    23     20092        75076      3.7      0.1              e = math.sqrt(disc)
    24     20092       104950      5.2      0.1              denom = 2.0 * a
    25     20092       115956      5.8      0.1              t = (-b - e) / denom
    26     20092        83382      4.2      0.1              if (t > 1.0e-7):
    27     20092       525272     26.1      0.4                  normal = (temp + t * ray.direction) / self.radius
    28     20092       333879     16.6      0.2                  hit_point = ray.origin + t * ray.direction
    29     20092       299494     14.9      0.2                  return ShadeRecord.ShadeRecord(normal=normal, hit_point=hit_point, parameter=t, color=self.color)

So, it appears that most of the time is spent in this chunk of code:
        temp = ray.origin - self.center
        a = numpy.dot(ray.direction, ray.direction)
        b = 2.0 * numpy.dot(temp, ray.direction)
        c = numpy.dot(temp, temp) - self.radius * self.radius
        disc = b * b - 4.0 * a * c

Where I don't really see a lot to optimize. Do you have any idea how to make this code more performant without going C ?
","Improving performance of raytracing hit function
I have a simple raytracer in python. rendering an image 200x200 takes 4 minutes, which is definitely too much for my taste. I want to improve the situation.
Some points: I shoot multiple rays per each pixel (to provide antialiasing) for a grand total of 16 rays per pixel. 200x200x16 is a grand total of 640000 rays. Each ray must be tested for impact on multiple Sphere objects in the scene. Ray is also a rather trivial object
class Ray(object):
    def __init__(self, origin, direction):
        self.origin = numpy.array(origin)
        self.direction = numpy.array(direction)

Sphere is slightly more complex, and carries the logic for hit/nohit:
class Sphere(object):
    def __init__(self, center, radius, color):
        self.center = numpy.array(center)
        self.radius = numpy.array(radius)
        self.color = color

    @profile 
    def hit(self, ray):
        temp = ray.origin - self.center
        a = numpy.dot(ray.direction, ray.direction)
        b = 2.0 * numpy.dot(temp, ray.direction)
        c = numpy.dot(temp, temp) - self.radius * self.radius
        disc = b * b - 4.0 * a * c

        if (disc < 0.0):
            return None
        else:
            e = math.sqrt(disc)
            denom = 2.0 * a
            t = (-b - e) / denom 
            if (t > 1.0e-7):
                normal = (temp + t * ray.direction) / self.radius
                hit_point = ray.origin + t * ray.direction
                return ShadeRecord.ShadeRecord(normal=normal, 
                                               hit_point=hit_point, 
                                               parameter=t, 
                                               color=self.color)           

            t = (-b + e) / denom

            if (t > 1.0e-7):
                normal = (temp + t * ray.direction) / self.radius                hit_point = ray.origin + t * ray.direction
                return ShadeRecord.ShadeRecord(normal=normal, 
                                               hit_point=hit_point, 
                                               parameter=t, 
                                               color=self.color)       

        return None    

Now, I ran some profiling, and it appears that the longest processing time is in the hit() function
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  2560000  118.831    0.000  152.701    0.000 raytrace/objects/Sphere.py:12(hit)
  1960020   42.989    0.000   42.989    0.000 {numpy.core.multiarray.array}
        1   34.566   34.566  285.829  285.829 raytrace/World.py:25(render)
  7680000   33.796    0.000   33.796    0.000 {numpy.core._dotblas.dot}
  2560000   11.124    0.000  163.825    0.000 raytrace/World.py:63(f)
   640000   10.132    0.000  189.411    0.000 raytrace/World.py:62(hit_bare_bones_object)
   640023    6.556    0.000  170.388    0.000 {map}

This does not surprise me, and I want to reduce this value as much as possible. I pass to line profiling, and the result is 
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    12                                               @profile
    13                                               def hit(self, ray):
    14   2560000     27956358     10.9     19.2          temp = ray.origin - self.center
    15   2560000     17944912      7.0     12.3          a = numpy.dot(ray.direction, ray.direction)
    16   2560000     24132737      9.4     16.5          b = 2.0 * numpy.dot(temp, ray.direction)
    17   2560000     37113811     14.5     25.4          c = numpy.dot(temp, temp) - self.radius * self.radius
    18   2560000     20808930      8.1     14.3          disc = b * b - 4.0 * a * c
    19                                                   
    20   2560000     10963318      4.3      7.5          if (disc < 0.0):
    21   2539908      5403624      2.1      3.7              return None
    22                                                   else:
    23     20092        75076      3.7      0.1              e = math.sqrt(disc)
    24     20092       104950      5.2      0.1              denom = 2.0 * a
    25     20092       115956      5.8      0.1              t = (-b - e) / denom
    26     20092        83382      4.2      0.1              if (t > 1.0e-7):
    27     20092       525272     26.1      0.4                  normal = (temp + t * ray.direction) / self.radius
    28     20092       333879     16.6      0.2                  hit_point = ray.origin + t * ray.direction
    29     20092       299494     14.9      0.2                  return ShadeRecord.ShadeRecord(normal=normal, hit_point=hit_point, parameter=t, color=self.color)

So, it appears that most of the time is spent in this chunk of code:
        temp = ray.origin - self.center
        a = numpy.dot(ray.direction, ray.direction)
        b = 2.0 * numpy.dot(temp, ray.direction)
        c = numpy.dot(temp, temp) - self.radius * self.radius
        disc = b * b - 4.0 * a * c

Where I don't really see a lot to optimize. Do you have any idea how to make this code more performant without going C ?
",0,0,1,0,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,0,0,0,,,Code,Code,,Code,Very Severe,Very Severe,,Very Severe
14570513,14570513,"Which is faster, numpy transpose or flip indices?","<p>I have a dynamic programming algorithm (modified Needleman-Wunsch) which requires the same basic calculation twice, but the calculation is done in the orthogonal direction the second time. For instance, from a given cell <em>(i,j)</em> in matrix <em>scoreMatrix</em>, I want to both calculate a value from values ""up"" from <em>(i,j)</em>, as well as a value from values to the ""left"" of <em>(i,j)</em>. In order to reuse the code I have used a function in which in the first case I send in parameters <em>i,j,scoreMatrix</em>, and in the next case I send in <em>j,i,scoreMatrix.transpose()</em>. Here is a highly simplified version of that code:</p>

<pre><code>def calculateGapCost(i,j,scoreMatrix,gapcost):
  return scoreMatrix[i-1,j] - gapcost

...
gapLeft = calculateGapCost(i,j,scoreMatrix,gapcost)
gapUp = calculateGapCost(j,i,scoreMatrix.transpose(),gapcost)
...
</code></pre>

<p>I realized that I could alternatively send in a function that would in the one case pass through arguments <em>(i,j)</em> when retrieving a value from <em>scoreMatrix</em>, and in the other case reverse them to <em>(j,i)</em>, rather than transposing the matrix each time.</p>

<pre><code>def passThrough(i,j,matrix):
  return matrix[i,j]

def flipIndices(i,j,matrix):
  return matrix[j,i]

def calculateGapCost(i,j,scoreMatrix,gapcost,retrieveValue):
  return retrieveValue(i-1,j,scoreMatrix) - gapcost

...
gapLeft = calculateGapCost(i,j,scoreMatrix,gapcost,passThrough)
gapUp = calculateGapCost(j,i,scoreMatrix,gapcost,flipIndices)
...
</code></pre>

<p>However if numpy transpose uses some features I'm unaware of to do the transpose in just a few operations, it may be that transpose is in fact faster than my pass-through function idea. Can anyone tell me which would be faster (or if there is a better method I haven't thought of)?</p>

<p>The actual method would call <em>retrieveValue</em> 3 times, and involves 2 matrices that would be referenced (and thus transposed if using that approach).</p>
","i have a dynamic programming algorithm modified needlemanwunsch which requires the same basic calculation twice but the calculation is done in the orthogonal direction the second time for instance from a given cell ij in matrix scorematrix i want to both calculate a value from values up from ij as well as a value from values to the left of ij in order to reuse the code i have used a function in which in the first case i send in parameters ijscorematrix and in the next case i send in jiscorematrixtranspose here is a highly simplified version of that code
i realized that i could alternatively send in a function that would in the one case pass through arguments ij when retrieving a value from scorematrix and in the other case reverse them to ji rather than transposing the matrix each time
however if numpy transpose uses some features im unaware of to do the transpose in just a few operations it may be that transpose is in fact faster than my passthrough function idea can anyone tell me which would be faster or if there is a better method i havent thought of
the actual method would call retrievevalue 3 times and involves 2 matrices that would be referenced and thus transposed if using that approach
",1,"I have a dynamic programming algorithm (modified Needleman-Wunsch) which requires the same basic calculation twice, but the calculation is done in the orthogonal direction the second time. For instance, from a given cell (i,j) in matrix scoreMatrix, I want to both calculate a value from values ""up"" from (i,j), as well as a value from values to the ""left"" of (i,j). In order to reuse the code I have used a function in which in the first case I send in parameters i,j,scoreMatrix, and in the next case I send in j,i,scoreMatrix.transpose(). Here is a highly simplified version of that code:
def calculateGapCost(i,j,scoreMatrix,gapcost):
  return scoreMatrix[i-1,j] - gapcost

...
gapLeft = calculateGapCost(i,j,scoreMatrix,gapcost)
gapUp = calculateGapCost(j,i,scoreMatrix.transpose(),gapcost)
...

I realized that I could alternatively send in a function that would in the one case pass through arguments (i,j) when retrieving a value from scoreMatrix, and in the other case reverse them to (j,i), rather than transposing the matrix each time.
def passThrough(i,j,matrix):
  return matrix[i,j]

def flipIndices(i,j,matrix):
  return matrix[j,i]

def calculateGapCost(i,j,scoreMatrix,gapcost,retrieveValue):
  return retrieveValue(i-1,j,scoreMatrix) - gapcost

...
gapLeft = calculateGapCost(i,j,scoreMatrix,gapcost,passThrough)
gapUp = calculateGapCost(j,i,scoreMatrix,gapcost,flipIndices)
...

However if numpy transpose uses some features I'm unaware of to do the transpose in just a few operations, it may be that transpose is in fact faster than my pass-through function idea. Can anyone tell me which would be faster (or if there is a better method I haven't thought of)?
The actual method would call retrieveValue 3 times, and involves 2 matrices that would be referenced (and thus transposed if using that approach).
","Which is faster, numpy transpose or flip indices?
I have a dynamic programming algorithm (modified Needleman-Wunsch) which requires the same basic calculation twice, but the calculation is done in the orthogonal direction the second time. For instance, from a given cell (i,j) in matrix scoreMatrix, I want to both calculate a value from values ""up"" from (i,j), as well as a value from values to the ""left"" of (i,j). In order to reuse the code I have used a function in which in the first case I send in parameters i,j,scoreMatrix, and in the next case I send in j,i,scoreMatrix.transpose(). Here is a highly simplified version of that code:
def calculateGapCost(i,j,scoreMatrix,gapcost):
  return scoreMatrix[i-1,j] - gapcost

...
gapLeft = calculateGapCost(i,j,scoreMatrix,gapcost)
gapUp = calculateGapCost(j,i,scoreMatrix.transpose(),gapcost)
...

I realized that I could alternatively send in a function that would in the one case pass through arguments (i,j) when retrieving a value from scoreMatrix, and in the other case reverse them to (j,i), rather than transposing the matrix each time.
def passThrough(i,j,matrix):
  return matrix[i,j]

def flipIndices(i,j,matrix):
  return matrix[j,i]

def calculateGapCost(i,j,scoreMatrix,gapcost,retrieveValue):
  return retrieveValue(i-1,j,scoreMatrix) - gapcost

...
gapLeft = calculateGapCost(i,j,scoreMatrix,gapcost,passThrough)
gapUp = calculateGapCost(j,i,scoreMatrix,gapcost,flipIndices)
...

However if numpy transpose uses some features I'm unaware of to do the transpose in just a few operations, it may be that transpose is in fact faster than my pass-through function idea. Can anyone tell me which would be faster (or if there is a better method I haven't thought of)?
The actual method would call retrieveValue 3 times, and involves 2 matrices that would be referenced (and thus transposed if using that approach).
",0,1,0,1,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
21805308,21805308,Sharing common code across pages with Browserify,"<p>I have a fairly large multi-page javascript applications that uses requirejs to organize code. I am researching moving to browserify because I like the simplicity that it offers and I am already used to the node.js module system.</p>

<p>Currently on each page I have javascript that goes like this</p>

<pre><code>&lt;script data-main=""/scripts/config/common"" src=""/scripts/lib/require.js""&gt;
&lt;script data-main=""/scripts/config/page-specific-js"" src=""/scripts/lib/require.js""&gt;
</code></pre>

<p>and I have a common build step and a build for each page. This way the majority of the JS is cached for every page.</p>

<p>Is it possible to do something similar with browserify? Is caching like this even worth it, or is it better to bundle everything into one file across all pages (considering that maybe only one page can depend on a large external library)?</p>
","i have a fairly large multipage javascript applications that uses requirejs to organize code i am researching moving to browserify because i like the simplicity that it offers and i am already used to the nodejs module system
currently on each page i have javascript that goes like this
and i have a common build step and a build for each page this way the majority of the js is cached for every page
is it possible to do something similar with browserify is caching like this even worth it or is it better to bundle everything into one file across all pages considering that maybe only one page can depend on a large external library
",1,"I have a fairly large multi-page javascript applications that uses requirejs to organize code. I am researching moving to browserify because I like the simplicity that it offers and I am already used to the node.js module system.
Currently on each page I have javascript that goes like this
<script data-main=""/scripts/config/common"" src=""/scripts/lib/require.js"">
<script data-main=""/scripts/config/page-specific-js"" src=""/scripts/lib/require.js"">

and I have a common build step and a build for each page. This way the majority of the JS is cached for every page.
Is it possible to do something similar with browserify? Is caching like this even worth it, or is it better to bundle everything into one file across all pages (considering that maybe only one page can depend on a large external library)?
","Sharing common code across pages with Browserify
I have a fairly large multi-page javascript applications that uses requirejs to organize code. I am researching moving to browserify because I like the simplicity that it offers and I am already used to the node.js module system.
Currently on each page I have javascript that goes like this
<script data-main=""/scripts/config/common"" src=""/scripts/lib/require.js"">
<script data-main=""/scripts/config/page-specific-js"" src=""/scripts/lib/require.js"">

and I have a common build step and a build for each page. This way the majority of the JS is cached for every page.
Is it possible to do something similar with browserify? Is caching like this even worth it, or is it better to bundle everything into one file across all pages (considering that maybe only one page can depend on a large external library)?
",0,1,0,1,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Build,Build,,Build,Mild,Mild,,Mild
52756817,52756817,Poor performance when modifying appSettings,"<p>I have an application in which I create a TreeView from the values of a database and display it with checkboxes.
Now I want to write the selected values into the appSettings. I tried to do this with this code.
But the performance is so bad, that this probably can't be the right way.
How can I solve it better?</p>

<pre><code>public static void SearchAndSaveSelectedNodes(TreeNodeCollection nodes)
{
    foreach (TreeNode n in nodes)
    {
        DeleteSetting(n.Name);

        if (n.Checked)
        {
            UpdateSetting(n.Name, n.Name + ""@"" + n.FullPath);
        }
        SearchAndSaveSelectedNodes(n.Nodes);
    }
}

public static void DeleteSetting(string key)
{
    System.Configuration.Configuration config = `ConfigurationManager.OpenExeConfiguration(ConfigurationUserLevel.None);`
    config.AppSettings.Settings.Remove(key);
    config.Save(ConfigurationSaveMode.Modified);
    ConfigurationManager.RefreshSection(""appSettings"");
    ConfigurationManager.RefreshSection(config.AppSettings.SectionInformation.Name);
}

public static void UpdateSetting(string key, string value)
{
    System.Configuration.Configuration config = ConfigurationManager.OpenExeConfiguration(ConfigurationUserLevel.None);
    config.AppSettings.Settings.Remove(key);
    config.AppSettings.Settings.Add(key, value);
    config.Save(ConfigurationSaveMode.Modified);
    ConfigurationManager.RefreshSection(""appSettings"");
    ConfigurationManager.RefreshSection(config.AppSettings.SectionInformation.Name);
}
</code></pre>
","i have an application in which i create a treeview from the values of a database and display it with checkboxes
now i want to write the selected values into the appsettings i tried to do this with this code
but the performance is so bad that this probably cant be the right way
how can i solve it better
",1,"I have an application in which I create a TreeView from the values of a database and display it with checkboxes.
Now I want to write the selected values into the appSettings. I tried to do this with this code.
But the performance is so bad, that this probably can't be the right way.
How can I solve it better?
public static void SearchAndSaveSelectedNodes(TreeNodeCollection nodes)
{
    foreach (TreeNode n in nodes)
    {
        DeleteSetting(n.Name);

        if (n.Checked)
        {
            UpdateSetting(n.Name, n.Name + ""@"" + n.FullPath);
        }
        SearchAndSaveSelectedNodes(n.Nodes);
    }
}

public static void DeleteSetting(string key)
{
    System.Configuration.Configuration config = `ConfigurationManager.OpenExeConfiguration(ConfigurationUserLevel.None);`
    config.AppSettings.Settings.Remove(key);
    config.Save(ConfigurationSaveMode.Modified);
    ConfigurationManager.RefreshSection(""appSettings"");
    ConfigurationManager.RefreshSection(config.AppSettings.SectionInformation.Name);
}

public static void UpdateSetting(string key, string value)
{
    System.Configuration.Configuration config = ConfigurationManager.OpenExeConfiguration(ConfigurationUserLevel.None);
    config.AppSettings.Settings.Remove(key);
    config.AppSettings.Settings.Add(key, value);
    config.Save(ConfigurationSaveMode.Modified);
    ConfigurationManager.RefreshSection(""appSettings"");
    ConfigurationManager.RefreshSection(config.AppSettings.SectionInformation.Name);
}

","Poor performance when modifying appSettings
I have an application in which I create a TreeView from the values of a database and display it with checkboxes.
Now I want to write the selected values into the appSettings. I tried to do this with this code.
But the performance is so bad, that this probably can't be the right way.
How can I solve it better?
public static void SearchAndSaveSelectedNodes(TreeNodeCollection nodes)
{
    foreach (TreeNode n in nodes)
    {
        DeleteSetting(n.Name);

        if (n.Checked)
        {
            UpdateSetting(n.Name, n.Name + ""@"" + n.FullPath);
        }
        SearchAndSaveSelectedNodes(n.Nodes);
    }
}

public static void DeleteSetting(string key)
{
    System.Configuration.Configuration config = `ConfigurationManager.OpenExeConfiguration(ConfigurationUserLevel.None);`
    config.AppSettings.Settings.Remove(key);
    config.Save(ConfigurationSaveMode.Modified);
    ConfigurationManager.RefreshSection(""appSettings"");
    ConfigurationManager.RefreshSection(config.AppSettings.SectionInformation.Name);
}

public static void UpdateSetting(string key, string value)
{
    System.Configuration.Configuration config = ConfigurationManager.OpenExeConfiguration(ConfigurationUserLevel.None);
    config.AppSettings.Settings.Remove(key);
    config.AppSettings.Settings.Add(key, value);
    config.Save(ConfigurationSaveMode.Modified);
    ConfigurationManager.RefreshSection(""appSettings"");
    ConfigurationManager.RefreshSection(config.AppSettings.SectionInformation.Name);
}

",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Code,Design,"The code is works but is slow, pointing more towards an underlying scalability issue as a result of the current design",Code,Moderate,Moderate,,Moderate
34229498,34229498,How to efficiently a number from a list of numbers til 0,"<p>I have an list of PTO object that has two properties AllowedHours and TakenHours.
I need to subtract PTO from the Accrued Banks until they hit 0 and then move to the next bank. </p>

<p>This code works but it looks off is there a way to do it more efficiently? Maybe a way to do it just through linq?</p>

<pre><code>var requestedHours = requestDetail.Hours;
while (requestedHours &gt; 0)
{
    var accrualBank = employeeAccruals.First(e =&gt; (e.AllowedHours - e.TakenHours) &gt; 0);
    var hoursAvailable = accrualBank.AllowedHours - accrualBank.TakenHours;

    if (hoursAvailable &gt; requestedHours)
    {
        accrualBank.TakenHours += requestedHours;
        requestedHours = 0;
    }
    else
    {
        accrualBank.TakenHours = accrualBank.AllowedHours;
        requestedHours -= hoursAvailable;
    }
}
</code></pre>
","i have an list of pto object that has two properties allowedhours and takenhours
i need to subtract pto from the accrued banks until they hit 0 and then move to the next bank 
this code works but it looks off is there a way to do it more efficiently maybe a way to do it just through linq
",0,"I have an list of PTO object that has two properties AllowedHours and TakenHours.
I need to subtract PTO from the Accrued Banks until they hit 0 and then move to the next bank. 
This code works but it looks off is there a way to do it more efficiently? Maybe a way to do it just through linq?
var requestedHours = requestDetail.Hours;
while (requestedHours > 0)
{
    var accrualBank = employeeAccruals.First(e => (e.AllowedHours - e.TakenHours) > 0);
    var hoursAvailable = accrualBank.AllowedHours - accrualBank.TakenHours;

    if (hoursAvailable > requestedHours)
    {
        accrualBank.TakenHours += requestedHours;
        requestedHours = 0;
    }
    else
    {
        accrualBank.TakenHours = accrualBank.AllowedHours;
        requestedHours -= hoursAvailable;
    }
}

","How to efficiently a number from a list of numbers til 0
I have an list of PTO object that has two properties AllowedHours and TakenHours.
I need to subtract PTO from the Accrued Banks until they hit 0 and then move to the next bank. 
This code works but it looks off is there a way to do it more efficiently? Maybe a way to do it just through linq?
var requestedHours = requestDetail.Hours;
while (requestedHours > 0)
{
    var accrualBank = employeeAccruals.First(e => (e.AllowedHours - e.TakenHours) > 0);
    var hoursAvailable = accrualBank.AllowedHours - accrualBank.TakenHours;

    if (hoursAvailable > requestedHours)
    {
        accrualBank.TakenHours += requestedHours;
        requestedHours = 0;
    }
    else
    {
        accrualBank.TakenHours = accrualBank.AllowedHours;
        requestedHours -= hoursAvailable;
    }
}

",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
42302987,42302987,scipy minimize inequality constraint function,"<p>I need to constrain my loss, so that the prediction is always positive.
So I have:</p>

<pre><code>x = [1.0,0.64,0.36,0.3,0.2]
y = [1.0,0.5,0.4,-0.1,-0.2]
alpha = 0

def loss(w, x, y, alpha):
    loss = 0.0
    for y_i,x_i in zip(y,x):
        loss += ((y_i - np.dot(w,x_i)) ** 2)
    return loss + alpha * math.sqrt(np.dot(w,w))

res = minimize(loss_new_scipy, 0.0, args=(x, y, alpha))
</code></pre>

<p>Now i want to add the constraints, but i found mostly constraints that x is in between bounds, not <code>np.dot(w,x)&gt;= 0</code>
How would such a constraint look like?</p>

<p>EDIT:
I want to use the constraints parameter in the scipy.optimize.minimize function, so I think it should look somehow like this:</p>

<pre><code>def con(w,x):
    loss = 0.0
    for i_x in x:
         loss += (np.dot(w, i_x))
    return loss


cons = ({'type': 'ineq', 'fun': con})
res = minimize(loss_new_scipy, 0.0, args=(x, y, alpha), constraints=cons)
</code></pre>

<p>also i removed the second constraint for simplicity</p>

<p>EDIT2:
I changed my problem to the following: constraint is w*x has to be greater than 1, and also changed the targets to all negatives. I also changed the args, so it runs now:</p>

<pre><code>x = np.array([1.0,0.64,0.36,0.3,0.2])
y = [-1.0,-0.5,-0.4,-0.1,-0.2]
alpha = 0

def con(w,x,y,alpha):
    print np.array(w*x)
    return np.array((w*x)-1).sum()


cons = ({'type': 'ineq', 'fun': con,'args':(x,y,alpha)})

def loss_new_scipy(w, x, y, alpha):
    loss = 0.0
    for y_i,x_i in zip(y,x):
        loss += ((y_i - np.dot(w,x_i)) ** 2)
    return loss + alpha * math.sqrt(np.dot(w,w))

res = minimize(loss_new_scipy, np.array([1.0]), args=(x, y, alpha),constraints=cons)
print res
</code></pre>

<p>But unfortunately the result for w is 2.0, which indeed is positive and looks like the constraint helped, since it is far away from fitting the function to the targets, but the predictions w*x are not all above 1.0</p>

<p>EDIT3:
i just realized that the sum of my predictions - 1 is equal to 0 now, but i want each prediction to be greater than 1.0
So with w = 2.0, </p>

<pre><code>w*x = [ 2.00000001  1.28000001  0.72        0.6         0.4       ] 
</code></pre>

<p>and </p>

<pre><code>(w*x) - 1 = [ 1.00000001  0.28000001 -0.28       -0.4        -0.6       ]
</code></pre>

<p>which sum is equal to 0.0, but i want all predictions <code>w*x</code> to be greater than 1.0, so all 5 values in <code>w*x</code> should be at least 1.0</p>
","i need to constrain my loss so that the prediction is always positive
so i have
now i want to add the constraints but i found mostly constraints that x is in between bounds not npdotwx 0
how would such a constraint look like
edit
i want to use the constraints parameter in the scipyoptimizeminimize function so i think it should look somehow like this
also i removed the second constraint for simplicity
edit2
i changed my problem to the following constraint is wx has to be greater than 1 and also changed the targets to all negatives i also changed the args so it runs now
but unfortunately the result for w is 20 which indeed is positive and looks like the constraint helped since it is far away from fitting the function to the targets but the predictions wx are not all above 10
edit3
i just realized that the sum of my predictions  1 is equal to 0 now but i want each prediction to be greater than 10
so with w  20 
and 
which sum is equal to 00 but i want all predictions wx to be greater than 10 so all 5 values in wx should be at least 10
",1,"I need to constrain my loss, so that the prediction is always positive.
So I have:
x = [1.0,0.64,0.36,0.3,0.2]
y = [1.0,0.5,0.4,-0.1,-0.2]
alpha = 0

def loss(w, x, y, alpha):
    loss = 0.0
    for y_i,x_i in zip(y,x):
        loss += ((y_i - np.dot(w,x_i)) ** 2)
    return loss + alpha * math.sqrt(np.dot(w,w))

res = minimize(loss_new_scipy, 0.0, args=(x, y, alpha))

Now i want to add the constraints, but i found mostly constraints that x is in between bounds, not np.dot(w,x)>= 0
How would such a constraint look like?
EDIT:
I want to use the constraints parameter in the scipy.optimize.minimize function, so I think it should look somehow like this:
def con(w,x):
    loss = 0.0
    for i_x in x:
         loss += (np.dot(w, i_x))
    return loss


cons = ({'type': 'ineq', 'fun': con})
res = minimize(loss_new_scipy, 0.0, args=(x, y, alpha), constraints=cons)

also i removed the second constraint for simplicity
EDIT2:
I changed my problem to the following: constraint is w*x has to be greater than 1, and also changed the targets to all negatives. I also changed the args, so it runs now:
x = np.array([1.0,0.64,0.36,0.3,0.2])
y = [-1.0,-0.5,-0.4,-0.1,-0.2]
alpha = 0

def con(w,x,y,alpha):
    print np.array(w*x)
    return np.array((w*x)-1).sum()


cons = ({'type': 'ineq', 'fun': con,'args':(x,y,alpha)})

def loss_new_scipy(w, x, y, alpha):
    loss = 0.0
    for y_i,x_i in zip(y,x):
        loss += ((y_i - np.dot(w,x_i)) ** 2)
    return loss + alpha * math.sqrt(np.dot(w,w))

res = minimize(loss_new_scipy, np.array([1.0]), args=(x, y, alpha),constraints=cons)
print res

But unfortunately the result for w is 2.0, which indeed is positive and looks like the constraint helped, since it is far away from fitting the function to the targets, but the predictions w*x are not all above 1.0
EDIT3:
i just realized that the sum of my predictions - 1 is equal to 0 now, but i want each prediction to be greater than 1.0
So with w = 2.0, 
w*x = [ 2.00000001  1.28000001  0.72        0.6         0.4       ] 

and 
(w*x) - 1 = [ 1.00000001  0.28000001 -0.28       -0.4        -0.6       ]

which sum is equal to 0.0, but i want all predictions w*x to be greater than 1.0, so all 5 values in w*x should be at least 1.0
","scipy minimize inequality constraint function
I need to constrain my loss, so that the prediction is always positive.
So I have:
x = [1.0,0.64,0.36,0.3,0.2]
y = [1.0,0.5,0.4,-0.1,-0.2]
alpha = 0

def loss(w, x, y, alpha):
    loss = 0.0
    for y_i,x_i in zip(y,x):
        loss += ((y_i - np.dot(w,x_i)) ** 2)
    return loss + alpha * math.sqrt(np.dot(w,w))

res = minimize(loss_new_scipy, 0.0, args=(x, y, alpha))

Now i want to add the constraints, but i found mostly constraints that x is in between bounds, not np.dot(w,x)>= 0
How would such a constraint look like?
EDIT:
I want to use the constraints parameter in the scipy.optimize.minimize function, so I think it should look somehow like this:
def con(w,x):
    loss = 0.0
    for i_x in x:
         loss += (np.dot(w, i_x))
    return loss


cons = ({'type': 'ineq', 'fun': con})
res = minimize(loss_new_scipy, 0.0, args=(x, y, alpha), constraints=cons)

also i removed the second constraint for simplicity
EDIT2:
I changed my problem to the following: constraint is w*x has to be greater than 1, and also changed the targets to all negatives. I also changed the args, so it runs now:
x = np.array([1.0,0.64,0.36,0.3,0.2])
y = [-1.0,-0.5,-0.4,-0.1,-0.2]
alpha = 0

def con(w,x,y,alpha):
    print np.array(w*x)
    return np.array((w*x)-1).sum()


cons = ({'type': 'ineq', 'fun': con,'args':(x,y,alpha)})

def loss_new_scipy(w, x, y, alpha):
    loss = 0.0
    for y_i,x_i in zip(y,x):
        loss += ((y_i - np.dot(w,x_i)) ** 2)
    return loss + alpha * math.sqrt(np.dot(w,w))

res = minimize(loss_new_scipy, np.array([1.0]), args=(x, y, alpha),constraints=cons)
print res

But unfortunately the result for w is 2.0, which indeed is positive and looks like the constraint helped, since it is far away from fitting the function to the targets, but the predictions w*x are not all above 1.0
EDIT3:
i just realized that the sum of my predictions - 1 is equal to 0 now, but i want each prediction to be greater than 1.0
So with w = 2.0, 
w*x = [ 2.00000001  1.28000001  0.72        0.6         0.4       ] 

and 
(w*x) - 1 = [ 1.00000001  0.28000001 -0.28       -0.4        -0.6       ]

which sum is equal to 0.0, but i want all predictions w*x to be greater than 1.0, so all 5 values in w*x should be at least 1.0
",0,1,1,0,0,0,0,- User just doesn't know how to approach a problem,"Label: 1

Reasoning: This question expresses technical debt because the user is trying to optimize their loss function with constraints, but they are struggling to implement the correct constraints to achieve the desired outcome. The user has attempted to add a constraint to ensure that the prediction `w*x` is always positive, but their initial attempts have not been successful. They have also realized that their current implementation is not meeting their requirements, as the predictions `w*x` are not all above 1.0.

The question shows signs of technical debt in several ways:

1. **Design flaws**: The user's initial implementation of the constraint is not correct, and they have had to iterate through multiple versions to try to achieve the desired outcome.
2. **Suboptimal code**: The user's code is not efficient, and they are struggling to implement the correct constraints to optimize their loss function.
3. **Performance issues**: Although not explicitly stated, the user's goal is to optimize their loss function, which implies that they are concerned about performance.
4. **Need for refactoring**: The user's code requires refactoring to correctly implement the constraints and achieve the desired outcome.

Overall, this question expresses technical debt because the user is struggling to implement a critical aspect of their code, and their current implementation is not meeting their requirements. The question implies a need for improvement, refactoring, and optimization to achieve the desired outcome.",Yes,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,1,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
34753101,34753101,Update map with values from another map,"<p>Let's assume that I have two maps of the same type and the set of keys of the second map is a subset of the keys of the first map. I want to update the first map values with values from the second map (only for the keys that the second map contains). </p>

<p>I have written a simple loop to do this, but I was wondering if there is a better way to write it using STL algorithms.</p>

<p>Code sample:</p>

<pre><code>using PersonAgeMap = std::map&lt;std::string, int&gt;;

PersonAgeMap map1;
map1.insert(std::make_pair(""John"", 23));
map1.insert(std::make_pair(""Liza"", 19));
map1.insert(std::make_pair(""Dad"", 45));
map1.insert(std::make_pair(""Granny"", 77));

PersonAgeMap map2;
map2.insert(std::make_pair(""John"", 24));
map2.insert(std::make_pair(""Liza"", 20));

//simple cycle way
for (const auto&amp; person: map2)
{
    map1[person.first] = person.second;
}

//is there some another way of doing this using STL algorithms???

for (const auto&amp; person: map1)
{
    std::cout &lt;&lt; person.first &lt;&lt; "" "" &lt;&lt; person.second &lt;&lt; std::endl;
}
</code></pre>

<p>Output:</p>

<pre><code>  Dad 45
  Granny 77
  John 24
  Liza 20
</code></pre>
","lets assume that i have two maps of the same type and the set of keys of the second map is a subset of the keys of the first map i want to update the first map values with values from the second map only for the keys that the second map contains 
i have written a simple loop to do this but i was wondering if there is a better way to write it using stl algorithms
code sample
output
",1,"Let's assume that I have two maps of the same type and the set of keys of the second map is a subset of the keys of the first map. I want to update the first map values with values from the second map (only for the keys that the second map contains). 
I have written a simple loop to do this, but I was wondering if there is a better way to write it using STL algorithms.
Code sample:
using PersonAgeMap = std::map<std::string, int>;

PersonAgeMap map1;
map1.insert(std::make_pair(""John"", 23));
map1.insert(std::make_pair(""Liza"", 19));
map1.insert(std::make_pair(""Dad"", 45));
map1.insert(std::make_pair(""Granny"", 77));

PersonAgeMap map2;
map2.insert(std::make_pair(""John"", 24));
map2.insert(std::make_pair(""Liza"", 20));

//simple cycle way
for (const auto& person: map2)
{
    map1[person.first] = person.second;
}

//is there some another way of doing this using STL algorithms???

for (const auto& person: map1)
{
    std::cout << person.first << "" "" << person.second << std::endl;
}

Output:
  Dad 45
  Granny 77
  John 24
  Liza 20

","Update map with values from another map
Let's assume that I have two maps of the same type and the set of keys of the second map is a subset of the keys of the first map. I want to update the first map values with values from the second map (only for the keys that the second map contains). 
I have written a simple loop to do this, but I was wondering if there is a better way to write it using STL algorithms.
Code sample:
using PersonAgeMap = std::map<std::string, int>;

PersonAgeMap map1;
map1.insert(std::make_pair(""John"", 23));
map1.insert(std::make_pair(""Liza"", 19));
map1.insert(std::make_pair(""Dad"", 45));
map1.insert(std::make_pair(""Granny"", 77));

PersonAgeMap map2;
map2.insert(std::make_pair(""John"", 24));
map2.insert(std::make_pair(""Liza"", 20));

//simple cycle way
for (const auto& person: map2)
{
    map1[person.first] = person.second;
}

//is there some another way of doing this using STL algorithms???

for (const auto& person: map1)
{
    std::cout << person.first << "" "" << person.second << std::endl;
}

Output:
  Dad 45
  Granny 77
  John 24
  Liza 20

",0,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
55083891,55083891,"understanding javascript advanced syntax, and object notation","<p>So I can across this neat helper function and was getting confused with the syntax. There is a variable (bool declared as true, which appears to be an array. Its using the bracket object notation, but then going to compare if bool[j] or [i] is true yet, there is nothing adding [i] or [j] to the object map. </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const helper = (word, words) =&gt; {

    let bool = [true]; 
   //if you console.log(typeof bool) returns object ?? 




    //This comes out as an Obj at first glance I thought it was an arr, but its bracket notation
    for (var i = 1; i &lt;= word.length; i++) {
     
        for (var j = 0; j &lt;= i; j++) {
          //how is bool[j] being evaluated? or i if its an obj? 
            if (bool[j] === true &amp;&amp; words[word.substring(j, i)] === true) {
                bool[i] = true;
                break;
            } else {
                bool[i] = false;
            }

        }
    }
    return console.log(bool[word.length] ? true : false);
}


helper('aa', ['aa', 'aabb', 'someotherword']);</code></pre>
</div>
</div>
</p>
","so i can across this neat helper function and was getting confused with the syntax there is a variable bool declared as true which appears to be an array its using the bracket object notation but then going to compare if boolj or i is true yet there is nothing adding i or j to the object map 


const helper  word words  

    let bool  true 
   if you consolelogtypeof bool returns object  




    this comes out as an obj at first glance i thought it was an arr but its bracket notation
    for var i  1 i  wordlength i 
     
        for var j  0 j  i j 
          how is boolj being evaluated or i if its an obj 
            if boolj  true  wordswordsubstringj i  true 
                booli  true
                break
             else 
                booli  false
            

        
    
    return consolelogboolwordlength  true  false



helperaa aa aabb someotherword



",1,"So I can across this neat helper function and was getting confused with the syntax. There is a variable (bool declared as true, which appears to be an array. Its using the bracket object notation, but then going to compare if bool[j] or [i] is true yet, there is nothing adding [i] or [j] to the object map. 


const helper = (word, words) => {

    let bool = [true]; 
   //if you console.log(typeof bool) returns object ?? 




    //This comes out as an Obj at first glance I thought it was an arr, but its bracket notation
    for (var i = 1; i <= word.length; i++) {
     
        for (var j = 0; j <= i; j++) {
          //how is bool[j] being evaluated? or i if its an obj? 
            if (bool[j] === true && words[word.substring(j, i)] === true) {
                bool[i] = true;
                break;
            } else {
                bool[i] = false;
            }

        }
    }
    return console.log(bool[word.length] ? true : false);
}


helper('aa', ['aa', 'aabb', 'someotherword']);



","understanding javascript advanced syntax, and object notation
So I can across this neat helper function and was getting confused with the syntax. There is a variable (bool declared as true, which appears to be an array. Its using the bracket object notation, but then going to compare if bool[j] or [i] is true yet, there is nothing adding [i] or [j] to the object map. 


const helper = (word, words) => {

    let bool = [true]; 
   //if you console.log(typeof bool) returns object ?? 




    //This comes out as an Obj at first glance I thought it was an arr, but its bracket notation
    for (var i = 1; i <= word.length; i++) {
     
        for (var j = 0; j <= i; j++) {
          //how is bool[j] being evaluated? or i if its an obj? 
            if (bool[j] === true && words[word.substring(j, i)] === true) {
                bool[i] = true;
                break;
            } else {
                bool[i] = false;
            }

        }
    }
    return console.log(bool[word.length] ? true : false);
}


helper('aa', ['aa', 'aabb', 'someotherword']);



",0,0,1,0,0,0,0,- Is a theoretical question,"Label: 1

Reasoning: This question expresses technical debt due to several reasons. Firstly, the author of the question is confused about the syntax of the code, specifically the use of bracket notation with an array declared as a boolean. This indicates that the code may not be easily maintainable or understandable, which is a characteristic of technical debt.

Secondly, the author is questioning how the `bool` array is being evaluated, which suggests that the code may not be following best practices or may be using unconventional techniques. This could lead to difficulties in debugging or extending the code in the future.

Additionally, the code is using `var` instead of `let` or `const`, which may indicate a lack of awareness of modern JavaScript best practices. The use of `console.log` as a return statement is also unconventional and may not be suitable for production code.

Lastly, the code is doing some complex logic with nested loops and conditional statements, which may be hard to understand or modify in the future. This complexity could be simplified or refactored to improve the code's maintainability and readability.

Overall, this question expresses technical debt because it highlights issues with code quality, maintainability, and potential performance problems, indicating a need for improvement or refactoring.",Yes,1,1,Compiler and Compilation Strategies,Compiler and Compilation Strategies,,Compiler and Compilation Strategies,0,0,1,0,0,0,,,Documentation,Documentation,,Documentation,Mild,Mild,,Mild
38597511,38597511,Javascript - Does accessing objects through a reference incur much of a performance hit?,"<pre><code>function Obj()
{
    this.prop = { a: 1, b: 2 };

    this.access();
    access(this.prop);
}

Obj.prototype.access = function()
{
    // accessing 'this.prop' directly
}

function access(property)
{
    // accessing 'this.prop' through local reference 'property'
}
</code></pre>

<p>How much of a performance hit is incurred when accessing <code>Obj.prop</code> indirectly through an object reference (as seen in the global <code>access()</code> function) vs directly (as seen in <code>Obj</code>'s <code>access()</code> method), and why?</p>
","
how much of a performance hit is incurred when accessing objprop indirectly through an object reference as seen in the global access function vs directly as seen in objs access method and why
",1,"function Obj()
{
    this.prop = { a: 1, b: 2 };

    this.access();
    access(this.prop);
}

Obj.prototype.access = function()
{
    // accessing 'this.prop' directly
}

function access(property)
{
    // accessing 'this.prop' through local reference 'property'
}

How much of a performance hit is incurred when accessing Obj.prop indirectly through an object reference (as seen in the global access() function) vs directly (as seen in Obj's access() method), and why?
","Javascript - Does accessing objects through a reference incur much of a performance hit?
function Obj()
{
    this.prop = { a: 1, b: 2 };

    this.access();
    access(this.prop);
}

Obj.prototype.access = function()
{
    // accessing 'this.prop' directly
}

function access(property)
{
    // accessing 'this.prop' through local reference 'property'
}

How much of a performance hit is incurred when accessing Obj.prop indirectly through an object reference (as seen in the global access() function) vs directly (as seen in Obj's access() method), and why?
",0,0,1,0,0,0,0,- Is a theoretical question,"Label: 1

Reasoning: This question expresses technical debt because it highlights a potential performance issue due to indirect access of an object property. The user is comparing two different approaches to accessing the same property (`Obj.prop`) and seeking to understand the performance implications of each. This suggests that the current implementation may be suboptimal and warrants investigation into potential optimizations.

The fact that the user is asking about the performance hit of indirect access implies that they are concerned about the efficiency of their code, which may be affecting the overall performance of their application. By asking about the underlying reasons for any performance differences, the user is indicating a desire to improve the design or architecture of their code to achieve better performance.

Furthermore, the presence of two different access patterns (direct access within the object's method vs. indirect access through a global function) may indicate a lack of consistency in the code, which can contribute to technical debt. This inconsistency could lead to maintainability issues, as developers may need to understand and adapt to different access patterns throughout the codebase.

Overall, the question expresses technical debt because it reveals a potential performance issue and a desire to improve the design and efficiency of the code, which may require future refactoring to address.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
3471559,3471559,memory size of Python data structure,"<p>How do I find out the memory size of a Python data structure? I'm looking for something like:</p>

<pre><code>sizeof({1:'hello', 2:'world'})
</code></pre>

<p>It is great if it counts every thing recursively. But even a basic non-recursive result helps. Basically I want to get a sense of various implementation options like tuple v.s. list v.s. class in terms of memory footprint. It matters because I'm planning to have millions of object instantiated.</p>

<p>My current dev platform is CPython 2.6.</p>
","how do i find out the memory size of a python data structure im looking for something like
it is great if it counts every thing recursively but even a basic nonrecursive result helps basically i want to get a sense of various implementation options like tuple vs list vs class in terms of memory footprint it matters because im planning to have millions of object instantiated
my current dev platform is cpython 26
",1,"How do I find out the memory size of a Python data structure? I'm looking for something like:
sizeof({1:'hello', 2:'world'})

It is great if it counts every thing recursively. But even a basic non-recursive result helps. Basically I want to get a sense of various implementation options like tuple v.s. list v.s. class in terms of memory footprint. It matters because I'm planning to have millions of object instantiated.
My current dev platform is CPython 2.6.
","memory size of Python data structure
How do I find out the memory size of a Python data structure? I'm looking for something like:
sizeof({1:'hello', 2:'world'})

It is great if it counts every thing recursively. But even a basic non-recursive result helps. Basically I want to get a sense of various implementation options like tuple v.s. list v.s. class in terms of memory footprint. It matters because I'm planning to have millions of object instantiated.
My current dev platform is CPython 2.6.
",0,1,1,0,0,0,0,- User just doesn't know how to approach a problem,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,1,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
24427117,24427117,Tree building infinite recursion detection,"<p>I am working on <code>JQuery</code> mechanism that is building tree, it has to be as fast as possible. Volume of data is quite large so master record column is used to be able to 'grab' all relevant nodes in one select from webSQL db.<br>
Whole mechanism but one part is done, when assembling tree it must check that there is no infinite recursion being created. What mechanism does at the moment if Record A is Master and Parent of record B, and Record B is Master and Parent of record A then structure like <code>A.Children[0] = B</code> and <code>B.Children[0] = A</code> is being built. It all would work fine, but it has do be knockout.js bound and displayed as expandable list to users, which results in overflow when trying to display the tree.<br>
Requirement is to detect such loops and do not create tree relationships.  </p>

<p>Mechanism that would check if item is already in the tree I came up is:</p>

<pre><code>function InTree(master, item) {
    return $.inArray(item, $.map(master, function recurs(n) {
        return ($.isArray(n) ? $.map(n, recurs) : n);
    })) != -1;
}

if(!InTree(tree, record))
{
//attach 
} else {
// get next record
}
</code></pre>

<p><strong>Is there anything faster than <code>InTree()</code> function that would get me if item is in the tree?</strong></p>

<p>Whole tree building algorithm is below (not that I think it is relevant but rather to avoid the comments 'show the code')</p>

<pre><code>$(document).on('OnPreQuery', onTheMove.PageDataRoles, function (e, options) {
    var isChildAttachmentQueued = true;
    var knockoutContextName = options.knockoutContextName;
    if (TreeEnabled(knockoutContextName)) {
        var isModelReadyToAttachChildren = function () {
            var isReady = false;
            if (PageObj[knockoutContextName] != undefined) {
                isReady = (PageObj[knockoutContextName]().length &gt; 0) &amp;&amp; isChildAttachmentQueued;
            }

            return isReady;
        };
        var businessComponent = eval(knockoutContextName);
            var treeSettings = businessComponent.Tree;
            treeSettings.knockoutContextName = knockoutContextName;
            $(businessComponent).on('OnPreUIUpdate', function (e, options) {
                if (isModelReadyToAttachChildren()) {
                    getChildrenForMasterRecordList({
                        parentTable: businessComponent.primaryTableName,
                        knockoutContextName: treeSettings.knockoutContextName,
                        parentIdColumn: treeSettings.ParentIdColumn,
                        masterIdColumn: treeSettings.MasterIdColumn
                    });
                    isChildAttachmentQueued = false;
                }
            });
    }
});

function TreeEnabled(knockoutContextName) {
    var isTreeEnabled = false;
    try {
        eval(knockoutContextName);
    } catch (e) {
        return isTreeEnabled;
    }
    var treeSettings = eval(knockoutContextName).Tree;
    if (treeSettings &amp;&amp; treeSettings.IncludeChildren) {
        isTreeEnabled = true;
    }

    return isTreeEnabled;
}

function ComposeRecordsToTreeStructure(results, tableArray, columnArray, options) {
    if (results.rows.length &gt; 0) {
        if (options.parentLayerIdList == undefined) {
            options.parentLayerIdList = options.masterIdList;
        }

        if (options.orphans == undefined) {
            options.orphans = [];
        }
        var knockoutContextName = options.knockoutContextName;
        var childRecordIdArray = [];
        if (options.runningOnOrphans) {
            if (options.orphans.length &gt; 0) {
                for (var j = 0; j &lt; options.orphans.length; j++) {
                    var rowRecord = options.orphans[j];
                    var rowRecordParentId = rowRecord[options.parentIdColumn];
                    var result = EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                    if (result.hasEstablishedConnection) {
                        childRecordIdArray = result.childRecordIdArray;
                    }
                }

                options.orphans = $.grep(options.orphans, function (item) {
                    return $.inArray(item['Id'], childRecordIdArray) == -1;
                });
            }
        } else {
            for (var i = 0; i &lt; results.rows.length; i++) {
                var rowRecord = results.rows.item(i);
                var rowRecordParentId = rowRecord[options.parentIdColumn];
                if (rowRecord[options.parentIdColumn] == '' || rowRecord[options.masterIdColumn] == '' || rowRecord[options.masterIdColumn] == rowRecord['Id']) {
                    rowRecord.isInvalid = true;
                } else if ($.inArray(rowRecord['Id'], options.masterIdList) != -1) {
                    masterRecordClone = $.grep(PageObj[knockoutContextName](), function (item) { return item.Fields.Id() == rowRecord['Id'] })[0];
                    if (masterRecordClone != undefined &amp;&amp; masterRecordClone.Children) {
                        rowRecord.Children = masterRecordClone.Children;
                     }
                }

                if (rowRecord.isInvalid == true) {
                    if (rowRecord[options.masterIdColumn] != rowRecord['Id']) {
                        var result = EstablishParentChildConnection(rowRecord, rowRecord[options.masterIdColumn], options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                        if (result.hasEstablishedConnection) {
                            childRecordIdArray = result.childRecordIdArray;
                            EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                        }
                    }
                } else {
                    var result = EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                    if (result.hasEstablishedConnection) {
                        childRecordIdArray = result.childRecordIdArray;
                    } else {
                        var recordObject = AddIsExpandedProperty(rowRecord);
                        options.orphans.push(recordObject);
                        options.runningOnOrphans = true;
                    }
                }
            }
        }

        if (options.orphans.length &gt; 0 &amp;&amp; childRecordIdArray.length &gt; 0) {
            options.parentLayerIdList = childRecordIdArray;
            ComposeRecordsToTreeStructure(results, tableArray, columnArray, options);
        }
    }

    onTheMove.seleniumHelper.markPageAsLoaded();
}

function EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray, options) {
    var result = EstablishParentChildConnection(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray);
    if (result.hasEstablishedConnection) {
        childRecordIdArray = result.childRecordIdArray;
    } else {
        var result = EstablishParentChildConnection(rowRecord, rowRecordParentId, childRecordIdArray, knockoutContextName, childRecordIdArray);
        if (result.hasEstablishedConnection) {
            childRecordIdArray = result.childRecordIdArray;
        } else {
            var matchingOrphans = $.grep(options.orphans, function (item) {
                return item['Id'] == rowRecordParentId;
            });
            if (matchingOrphans.length &gt; 0) {
                AttachPassedChildRecord(rowRecord, matchingOrphans);
                var result = {
                    hasEstablishedConnection: true
                };
            }
        }
    }

    return {
        childRecordIdArray: childRecordIdArray,
        hasEstablishedConnection: result.hasEstablishedConnection
    };
}

function EstablishParentChildConnection(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray) {
    var hasEstablishedConnection = false;
    var parentPosition = $.inArray(rowRecordParentId, parentLayerIdList);
    if (parentPosition != -1) {
        AttachChildRecordsToParents(rowRecord, parentLayerIdList[parentPosition], knockoutContextName);
        childRecordIdArray = AddChildRecordsToNextParentList(rowRecord, childRecordIdArray);
        childRecordIdArray.push(rowRecord['Id']);
        hasEstablishedConnection = true;
    }

    return {
        childRecordIdArray: childRecordIdArray,
        hasEstablishedConnection: hasEstablishedConnection
    };
}

function AddChildRecordsToNextParentList(childRecord, childRecordIdArray) {
    if (childRecord.Children != undefined) {
        for (var i = 0; i &lt; childRecord.Children.length; i++) {
            childRecordIdArray.push(childRecord.Children[i]['Id']);
            if (childRecord.Children[i].Children != undefined) {
                AddChildRecordsToNextParentList(childRecord.Children[i], childRecordIdArray);
            }
        }
    }

    return childRecordIdArray;
}

function RowsToListDataStructure(results) {
    var array = [];
    for (var i = 0; i &lt; results.rows.length; i++) {
        array.push(results.rows.item(i));
    }

    return array;
}

function AttachChildRecordsToParents(recordRow, id, knockoutContextName) {
    var childTreeOptions = {
        id: id,
        knockoutContextName: knockoutContextName,
        results: []
    };
    findObjectsInChildTreeById(childTreeOptions);
    if (childTreeOptions.results.length &gt; 0) {
        AttachPassedChildRecord(recordRow, childTreeOptions.results);
    }
}

function AttachPassedChildRecord(recordObject, pageObjParentResults) {
    for (var i = 0; i &lt; pageObjParentResults.length; i++) {
        if (pageObjParentResults[i].Children == undefined) {
            pageObjParentResults[i].Children = [];
        }
        if ($.grep(pageObjParentResults[i].Children, function (children) {
            return children['Id'] == recordObject['Id'];
        }).length == 0) {
            recordObject = AddIsExpandedProperty(recordObject);
            pageObjParentResults[i].Children.push(recordObject);
        }
    }
}

function AddIsExpandedProperty(recordObject) {
    recordObject.IsExpanded = ko.observable(false);
    return recordObject;
}

function findObjectsInChildTreeById(options) {
    if (options.item == undefined) {
        if (typeof PageObj[options.knockoutContextName] != 'undefined') {
            for (var item in PageObj[options.knockoutContextName]()) {
                findObjectsInChildTreeById({
                    item: PageObj[options.knockoutContextName]()[item],
                    id: options.id,
                    results: options.results
                });
            }
        }
    } else {
        if (typeof options.item.Fields != 'undefined') {
            if (options.item.Fields['Id']() == options.id)
                options.results.push(options.item);
        } else {
            if (options.item['Id'] == options.id)
                options.results.push(options.item);
        }

        if (options.item.Children != undefined) {
            for (var item in options.item.Children) {
                findObjectsInChildTreeById({
                    item: options.item.Children[item],
                    id: options.id,
                    results: options.results
                });
            }
        }
    }
}

function getChildrenForMasterRecordList(options) {
    var parentTable = options.parentTable,
        masterIdColumn = options.masterIdColumn,
        parentIdColumn = options.parentIdColumn,
        knockoutContextName = options.knockoutContextName,
        masterIds = getParentIdsAndMastersOfParentsFromPageObj(knockoutContextName, masterIdColumn);
    for (var item in PageObj[options.knockoutContextName]()) {
        AddIsExpandedProperty(PageObj[knockoutContextName]()[item]);
    }

    var dbManager = new OnTheMoveDatabaseManager();
    dbManager.queryDatabase({
        statement: {
            Tables: [{
                Alias: parentTable,
                JoinSpec: null,
                JoinType: """",
                Name: parentTable
            }, {
                Alias: ""Record"",
                JoinSpec: ""Record.Id = "" + parentTable + "".Id"",
                JoinType: ""INNER"",
                Name: ""Record""
            }],
            WhereClause: parentTable + ""."" + masterIdColumn + "" IN ('"" + masterIds.join(""','"") + ""') AND Record.RecordType ='"" + parentTable + ""'"",
            SelectFields: [{
                IsAggregate: false,
                Name: ""*""
            }],
            DisablePaging: true,
            OrderClause: ""Record.Id""
        },
        knockoutContextName: knockoutContextName,
        isObservable: false,
        masterIdColumn: masterIdColumn,
        masterIdList: masterIds,
        parentIdColumn: parentIdColumn,
        parentTable: options.parentTable,
        success: function (results, tableArray, columnArray, options) {
            ComposeRecordsToTreeStructure(results, tableArray, columnArray, options);
        }
    });
}

function getParentIdsAndMastersOfParentsFromPageObj(knockoutContextName, masterColumnName) {
    var list = [];
    if (typeof PageObj[knockoutContextName] != 'undefined') {
        for (var item in PageObj[knockoutContextName]()) {
            if ($.inArray(PageObj[knockoutContextName]()[item].Fields['Id'](), list) == -1) {
                list.push(PageObj[knockoutContextName]()[item].Fields['Id']());
            }
            if (PageObj[knockoutContextName]()[item].Fields[masterColumnName]() != '' &amp;&amp; $.inArray(PageObj[knockoutContextName]()[item].Fields[masterColumnName](), list) == -1) {
                list.push(PageObj[knockoutContextName]()[item].Fields[masterColumnName]());
             }
        }
    }

    return list
}

function InTree(master, item) {
    return $.inArray(item, $.map(master, function recurs(n) {
        return ($.isArray(n) ? $.map(n, recurs) : n);
    })) != -1;
}
</code></pre>
","i am working on jquery mechanism that is building tree it has to be as fast as possible volume of data is quite large so master record column is used to be able to grab all relevant nodes in one select from websql db
whole mechanism but one part is done when assembling tree it must check that there is no infinite recursion being created what mechanism does at the moment if record a is master and parent of record b and record b is master and parent of record a then structure like achildren0  b and bchildren0  a is being built it all would work fine but it has do be knockoutjs bound and displayed as expandable list to users which results in overflow when trying to display the tree
requirement is to detect such loops and do not create tree relationships  
mechanism that would check if item is already in the tree i came up is
is there anything faster than intree function that would get me if item is in the tree
whole tree building algorithm is below not that i think it is relevant but rather to avoid the comments show the code
",1,"I am working on JQuery mechanism that is building tree, it has to be as fast as possible. Volume of data is quite large so master record column is used to be able to 'grab' all relevant nodes in one select from webSQL db.
Whole mechanism but one part is done, when assembling tree it must check that there is no infinite recursion being created. What mechanism does at the moment if Record A is Master and Parent of record B, and Record B is Master and Parent of record A then structure like A.Children[0] = B and B.Children[0] = A is being built. It all would work fine, but it has do be knockout.js bound and displayed as expandable list to users, which results in overflow when trying to display the tree.
Requirement is to detect such loops and do not create tree relationships.  
Mechanism that would check if item is already in the tree I came up is:
function InTree(master, item) {
    return $.inArray(item, $.map(master, function recurs(n) {
        return ($.isArray(n) ? $.map(n, recurs) : n);
    })) != -1;
}

if(!InTree(tree, record))
{
//attach 
} else {
// get next record
}

Is there anything faster than InTree() function that would get me if item is in the tree?
Whole tree building algorithm is below (not that I think it is relevant but rather to avoid the comments 'show the code')
$(document).on('OnPreQuery', onTheMove.PageDataRoles, function (e, options) {
    var isChildAttachmentQueued = true;
    var knockoutContextName = options.knockoutContextName;
    if (TreeEnabled(knockoutContextName)) {
        var isModelReadyToAttachChildren = function () {
            var isReady = false;
            if (PageObj[knockoutContextName] != undefined) {
                isReady = (PageObj[knockoutContextName]().length > 0) && isChildAttachmentQueued;
            }

            return isReady;
        };
        var businessComponent = eval(knockoutContextName);
            var treeSettings = businessComponent.Tree;
            treeSettings.knockoutContextName = knockoutContextName;
            $(businessComponent).on('OnPreUIUpdate', function (e, options) {
                if (isModelReadyToAttachChildren()) {
                    getChildrenForMasterRecordList({
                        parentTable: businessComponent.primaryTableName,
                        knockoutContextName: treeSettings.knockoutContextName,
                        parentIdColumn: treeSettings.ParentIdColumn,
                        masterIdColumn: treeSettings.MasterIdColumn
                    });
                    isChildAttachmentQueued = false;
                }
            });
    }
});

function TreeEnabled(knockoutContextName) {
    var isTreeEnabled = false;
    try {
        eval(knockoutContextName);
    } catch (e) {
        return isTreeEnabled;
    }
    var treeSettings = eval(knockoutContextName).Tree;
    if (treeSettings && treeSettings.IncludeChildren) {
        isTreeEnabled = true;
    }

    return isTreeEnabled;
}

function ComposeRecordsToTreeStructure(results, tableArray, columnArray, options) {
    if (results.rows.length > 0) {
        if (options.parentLayerIdList == undefined) {
            options.parentLayerIdList = options.masterIdList;
        }

        if (options.orphans == undefined) {
            options.orphans = [];
        }
        var knockoutContextName = options.knockoutContextName;
        var childRecordIdArray = [];
        if (options.runningOnOrphans) {
            if (options.orphans.length > 0) {
                for (var j = 0; j < options.orphans.length; j++) {
                    var rowRecord = options.orphans[j];
                    var rowRecordParentId = rowRecord[options.parentIdColumn];
                    var result = EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                    if (result.hasEstablishedConnection) {
                        childRecordIdArray = result.childRecordIdArray;
                    }
                }

                options.orphans = $.grep(options.orphans, function (item) {
                    return $.inArray(item['Id'], childRecordIdArray) == -1;
                });
            }
        } else {
            for (var i = 0; i < results.rows.length; i++) {
                var rowRecord = results.rows.item(i);
                var rowRecordParentId = rowRecord[options.parentIdColumn];
                if (rowRecord[options.parentIdColumn] == '' || rowRecord[options.masterIdColumn] == '' || rowRecord[options.masterIdColumn] == rowRecord['Id']) {
                    rowRecord.isInvalid = true;
                } else if ($.inArray(rowRecord['Id'], options.masterIdList) != -1) {
                    masterRecordClone = $.grep(PageObj[knockoutContextName](), function (item) { return item.Fields.Id() == rowRecord['Id'] })[0];
                    if (masterRecordClone != undefined && masterRecordClone.Children) {
                        rowRecord.Children = masterRecordClone.Children;
                     }
                }

                if (rowRecord.isInvalid == true) {
                    if (rowRecord[options.masterIdColumn] != rowRecord['Id']) {
                        var result = EstablishParentChildConnection(rowRecord, rowRecord[options.masterIdColumn], options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                        if (result.hasEstablishedConnection) {
                            childRecordIdArray = result.childRecordIdArray;
                            EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                        }
                    }
                } else {
                    var result = EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                    if (result.hasEstablishedConnection) {
                        childRecordIdArray = result.childRecordIdArray;
                    } else {
                        var recordObject = AddIsExpandedProperty(rowRecord);
                        options.orphans.push(recordObject);
                        options.runningOnOrphans = true;
                    }
                }
            }
        }

        if (options.orphans.length > 0 && childRecordIdArray.length > 0) {
            options.parentLayerIdList = childRecordIdArray;
            ComposeRecordsToTreeStructure(results, tableArray, columnArray, options);
        }
    }

    onTheMove.seleniumHelper.markPageAsLoaded();
}

function EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray, options) {
    var result = EstablishParentChildConnection(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray);
    if (result.hasEstablishedConnection) {
        childRecordIdArray = result.childRecordIdArray;
    } else {
        var result = EstablishParentChildConnection(rowRecord, rowRecordParentId, childRecordIdArray, knockoutContextName, childRecordIdArray);
        if (result.hasEstablishedConnection) {
            childRecordIdArray = result.childRecordIdArray;
        } else {
            var matchingOrphans = $.grep(options.orphans, function (item) {
                return item['Id'] == rowRecordParentId;
            });
            if (matchingOrphans.length > 0) {
                AttachPassedChildRecord(rowRecord, matchingOrphans);
                var result = {
                    hasEstablishedConnection: true
                };
            }
        }
    }

    return {
        childRecordIdArray: childRecordIdArray,
        hasEstablishedConnection: result.hasEstablishedConnection
    };
}

function EstablishParentChildConnection(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray) {
    var hasEstablishedConnection = false;
    var parentPosition = $.inArray(rowRecordParentId, parentLayerIdList);
    if (parentPosition != -1) {
        AttachChildRecordsToParents(rowRecord, parentLayerIdList[parentPosition], knockoutContextName);
        childRecordIdArray = AddChildRecordsToNextParentList(rowRecord, childRecordIdArray);
        childRecordIdArray.push(rowRecord['Id']);
        hasEstablishedConnection = true;
    }

    return {
        childRecordIdArray: childRecordIdArray,
        hasEstablishedConnection: hasEstablishedConnection
    };
}

function AddChildRecordsToNextParentList(childRecord, childRecordIdArray) {
    if (childRecord.Children != undefined) {
        for (var i = 0; i < childRecord.Children.length; i++) {
            childRecordIdArray.push(childRecord.Children[i]['Id']);
            if (childRecord.Children[i].Children != undefined) {
                AddChildRecordsToNextParentList(childRecord.Children[i], childRecordIdArray);
            }
        }
    }

    return childRecordIdArray;
}

function RowsToListDataStructure(results) {
    var array = [];
    for (var i = 0; i < results.rows.length; i++) {
        array.push(results.rows.item(i));
    }

    return array;
}

function AttachChildRecordsToParents(recordRow, id, knockoutContextName) {
    var childTreeOptions = {
        id: id,
        knockoutContextName: knockoutContextName,
        results: []
    };
    findObjectsInChildTreeById(childTreeOptions);
    if (childTreeOptions.results.length > 0) {
        AttachPassedChildRecord(recordRow, childTreeOptions.results);
    }
}

function AttachPassedChildRecord(recordObject, pageObjParentResults) {
    for (var i = 0; i < pageObjParentResults.length; i++) {
        if (pageObjParentResults[i].Children == undefined) {
            pageObjParentResults[i].Children = [];
        }
        if ($.grep(pageObjParentResults[i].Children, function (children) {
            return children['Id'] == recordObject['Id'];
        }).length == 0) {
            recordObject = AddIsExpandedProperty(recordObject);
            pageObjParentResults[i].Children.push(recordObject);
        }
    }
}

function AddIsExpandedProperty(recordObject) {
    recordObject.IsExpanded = ko.observable(false);
    return recordObject;
}

function findObjectsInChildTreeById(options) {
    if (options.item == undefined) {
        if (typeof PageObj[options.knockoutContextName] != 'undefined') {
            for (var item in PageObj[options.knockoutContextName]()) {
                findObjectsInChildTreeById({
                    item: PageObj[options.knockoutContextName]()[item],
                    id: options.id,
                    results: options.results
                });
            }
        }
    } else {
        if (typeof options.item.Fields != 'undefined') {
            if (options.item.Fields['Id']() == options.id)
                options.results.push(options.item);
        } else {
            if (options.item['Id'] == options.id)
                options.results.push(options.item);
        }

        if (options.item.Children != undefined) {
            for (var item in options.item.Children) {
                findObjectsInChildTreeById({
                    item: options.item.Children[item],
                    id: options.id,
                    results: options.results
                });
            }
        }
    }
}

function getChildrenForMasterRecordList(options) {
    var parentTable = options.parentTable,
        masterIdColumn = options.masterIdColumn,
        parentIdColumn = options.parentIdColumn,
        knockoutContextName = options.knockoutContextName,
        masterIds = getParentIdsAndMastersOfParentsFromPageObj(knockoutContextName, masterIdColumn);
    for (var item in PageObj[options.knockoutContextName]()) {
        AddIsExpandedProperty(PageObj[knockoutContextName]()[item]);
    }

    var dbManager = new OnTheMoveDatabaseManager();
    dbManager.queryDatabase({
        statement: {
            Tables: [{
                Alias: parentTable,
                JoinSpec: null,
                JoinType: """",
                Name: parentTable
            }, {
                Alias: ""Record"",
                JoinSpec: ""Record.Id = "" + parentTable + "".Id"",
                JoinType: ""INNER"",
                Name: ""Record""
            }],
            WhereClause: parentTable + ""."" + masterIdColumn + "" IN ('"" + masterIds.join(""','"") + ""') AND Record.RecordType ='"" + parentTable + ""'"",
            SelectFields: [{
                IsAggregate: false,
                Name: ""*""
            }],
            DisablePaging: true,
            OrderClause: ""Record.Id""
        },
        knockoutContextName: knockoutContextName,
        isObservable: false,
        masterIdColumn: masterIdColumn,
        masterIdList: masterIds,
        parentIdColumn: parentIdColumn,
        parentTable: options.parentTable,
        success: function (results, tableArray, columnArray, options) {
            ComposeRecordsToTreeStructure(results, tableArray, columnArray, options);
        }
    });
}

function getParentIdsAndMastersOfParentsFromPageObj(knockoutContextName, masterColumnName) {
    var list = [];
    if (typeof PageObj[knockoutContextName] != 'undefined') {
        for (var item in PageObj[knockoutContextName]()) {
            if ($.inArray(PageObj[knockoutContextName]()[item].Fields['Id'](), list) == -1) {
                list.push(PageObj[knockoutContextName]()[item].Fields['Id']());
            }
            if (PageObj[knockoutContextName]()[item].Fields[masterColumnName]() != '' && $.inArray(PageObj[knockoutContextName]()[item].Fields[masterColumnName](), list) == -1) {
                list.push(PageObj[knockoutContextName]()[item].Fields[masterColumnName]());
             }
        }
    }

    return list
}

function InTree(master, item) {
    return $.inArray(item, $.map(master, function recurs(n) {
        return ($.isArray(n) ? $.map(n, recurs) : n);
    })) != -1;
}

","Tree building infinite recursion detection
I am working on JQuery mechanism that is building tree, it has to be as fast as possible. Volume of data is quite large so master record column is used to be able to 'grab' all relevant nodes in one select from webSQL db.
Whole mechanism but one part is done, when assembling tree it must check that there is no infinite recursion being created. What mechanism does at the moment if Record A is Master and Parent of record B, and Record B is Master and Parent of record A then structure like A.Children[0] = B and B.Children[0] = A is being built. It all would work fine, but it has do be knockout.js bound and displayed as expandable list to users, which results in overflow when trying to display the tree.
Requirement is to detect such loops and do not create tree relationships.  
Mechanism that would check if item is already in the tree I came up is:
function InTree(master, item) {
    return $.inArray(item, $.map(master, function recurs(n) {
        return ($.isArray(n) ? $.map(n, recurs) : n);
    })) != -1;
}

if(!InTree(tree, record))
{
//attach 
} else {
// get next record
}

Is there anything faster than InTree() function that would get me if item is in the tree?
Whole tree building algorithm is below (not that I think it is relevant but rather to avoid the comments 'show the code')
$(document).on('OnPreQuery', onTheMove.PageDataRoles, function (e, options) {
    var isChildAttachmentQueued = true;
    var knockoutContextName = options.knockoutContextName;
    if (TreeEnabled(knockoutContextName)) {
        var isModelReadyToAttachChildren = function () {
            var isReady = false;
            if (PageObj[knockoutContextName] != undefined) {
                isReady = (PageObj[knockoutContextName]().length > 0) && isChildAttachmentQueued;
            }

            return isReady;
        };
        var businessComponent = eval(knockoutContextName);
            var treeSettings = businessComponent.Tree;
            treeSettings.knockoutContextName = knockoutContextName;
            $(businessComponent).on('OnPreUIUpdate', function (e, options) {
                if (isModelReadyToAttachChildren()) {
                    getChildrenForMasterRecordList({
                        parentTable: businessComponent.primaryTableName,
                        knockoutContextName: treeSettings.knockoutContextName,
                        parentIdColumn: treeSettings.ParentIdColumn,
                        masterIdColumn: treeSettings.MasterIdColumn
                    });
                    isChildAttachmentQueued = false;
                }
            });
    }
});

function TreeEnabled(knockoutContextName) {
    var isTreeEnabled = false;
    try {
        eval(knockoutContextName);
    } catch (e) {
        return isTreeEnabled;
    }
    var treeSettings = eval(knockoutContextName).Tree;
    if (treeSettings && treeSettings.IncludeChildren) {
        isTreeEnabled = true;
    }

    return isTreeEnabled;
}

function ComposeRecordsToTreeStructure(results, tableArray, columnArray, options) {
    if (results.rows.length > 0) {
        if (options.parentLayerIdList == undefined) {
            options.parentLayerIdList = options.masterIdList;
        }

        if (options.orphans == undefined) {
            options.orphans = [];
        }
        var knockoutContextName = options.knockoutContextName;
        var childRecordIdArray = [];
        if (options.runningOnOrphans) {
            if (options.orphans.length > 0) {
                for (var j = 0; j < options.orphans.length; j++) {
                    var rowRecord = options.orphans[j];
                    var rowRecordParentId = rowRecord[options.parentIdColumn];
                    var result = EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                    if (result.hasEstablishedConnection) {
                        childRecordIdArray = result.childRecordIdArray;
                    }
                }

                options.orphans = $.grep(options.orphans, function (item) {
                    return $.inArray(item['Id'], childRecordIdArray) == -1;
                });
            }
        } else {
            for (var i = 0; i < results.rows.length; i++) {
                var rowRecord = results.rows.item(i);
                var rowRecordParentId = rowRecord[options.parentIdColumn];
                if (rowRecord[options.parentIdColumn] == '' || rowRecord[options.masterIdColumn] == '' || rowRecord[options.masterIdColumn] == rowRecord['Id']) {
                    rowRecord.isInvalid = true;
                } else if ($.inArray(rowRecord['Id'], options.masterIdList) != -1) {
                    masterRecordClone = $.grep(PageObj[knockoutContextName](), function (item) { return item.Fields.Id() == rowRecord['Id'] })[0];
                    if (masterRecordClone != undefined && masterRecordClone.Children) {
                        rowRecord.Children = masterRecordClone.Children;
                     }
                }

                if (rowRecord.isInvalid == true) {
                    if (rowRecord[options.masterIdColumn] != rowRecord['Id']) {
                        var result = EstablishParentChildConnection(rowRecord, rowRecord[options.masterIdColumn], options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                        if (result.hasEstablishedConnection) {
                            childRecordIdArray = result.childRecordIdArray;
                            EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                        }
                    }
                } else {
                    var result = EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, options.parentLayerIdList, knockoutContextName, childRecordIdArray, options);
                    if (result.hasEstablishedConnection) {
                        childRecordIdArray = result.childRecordIdArray;
                    } else {
                        var recordObject = AddIsExpandedProperty(rowRecord);
                        options.orphans.push(recordObject);
                        options.runningOnOrphans = true;
                    }
                }
            }
        }

        if (options.orphans.length > 0 && childRecordIdArray.length > 0) {
            options.parentLayerIdList = childRecordIdArray;
            ComposeRecordsToTreeStructure(results, tableArray, columnArray, options);
        }
    }

    onTheMove.seleniumHelper.markPageAsLoaded();
}

function EstablishParentChildConnectionOnAlreadyProcessedNodes(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray, options) {
    var result = EstablishParentChildConnection(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray);
    if (result.hasEstablishedConnection) {
        childRecordIdArray = result.childRecordIdArray;
    } else {
        var result = EstablishParentChildConnection(rowRecord, rowRecordParentId, childRecordIdArray, knockoutContextName, childRecordIdArray);
        if (result.hasEstablishedConnection) {
            childRecordIdArray = result.childRecordIdArray;
        } else {
            var matchingOrphans = $.grep(options.orphans, function (item) {
                return item['Id'] == rowRecordParentId;
            });
            if (matchingOrphans.length > 0) {
                AttachPassedChildRecord(rowRecord, matchingOrphans);
                var result = {
                    hasEstablishedConnection: true
                };
            }
        }
    }

    return {
        childRecordIdArray: childRecordIdArray,
        hasEstablishedConnection: result.hasEstablishedConnection
    };
}

function EstablishParentChildConnection(rowRecord, rowRecordParentId, parentLayerIdList, knockoutContextName, childRecordIdArray) {
    var hasEstablishedConnection = false;
    var parentPosition = $.inArray(rowRecordParentId, parentLayerIdList);
    if (parentPosition != -1) {
        AttachChildRecordsToParents(rowRecord, parentLayerIdList[parentPosition], knockoutContextName);
        childRecordIdArray = AddChildRecordsToNextParentList(rowRecord, childRecordIdArray);
        childRecordIdArray.push(rowRecord['Id']);
        hasEstablishedConnection = true;
    }

    return {
        childRecordIdArray: childRecordIdArray,
        hasEstablishedConnection: hasEstablishedConnection
    };
}

function AddChildRecordsToNextParentList(childRecord, childRecordIdArray) {
    if (childRecord.Children != undefined) {
        for (var i = 0; i < childRecord.Children.length; i++) {
            childRecordIdArray.push(childRecord.Children[i]['Id']);
            if (childRecord.Children[i].Children != undefined) {
                AddChildRecordsToNextParentList(childRecord.Children[i], childRecordIdArray);
            }
        }
    }

    return childRecordIdArray;
}

function RowsToListDataStructure(results) {
    var array = [];
    for (var i = 0; i < results.rows.length; i++) {
        array.push(results.rows.item(i));
    }

    return array;
}

function AttachChildRecordsToParents(recordRow, id, knockoutContextName) {
    var childTreeOptions = {
        id: id,
        knockoutContextName: knockoutContextName,
        results: []
    };
    findObjectsInChildTreeById(childTreeOptions);
    if (childTreeOptions.results.length > 0) {
        AttachPassedChildRecord(recordRow, childTreeOptions.results);
    }
}

function AttachPassedChildRecord(recordObject, pageObjParentResults) {
    for (var i = 0; i < pageObjParentResults.length; i++) {
        if (pageObjParentResults[i].Children == undefined) {
            pageObjParentResults[i].Children = [];
        }
        if ($.grep(pageObjParentResults[i].Children, function (children) {
            return children['Id'] == recordObject['Id'];
        }).length == 0) {
            recordObject = AddIsExpandedProperty(recordObject);
            pageObjParentResults[i].Children.push(recordObject);
        }
    }
}

function AddIsExpandedProperty(recordObject) {
    recordObject.IsExpanded = ko.observable(false);
    return recordObject;
}

function findObjectsInChildTreeById(options) {
    if (options.item == undefined) {
        if (typeof PageObj[options.knockoutContextName] != 'undefined') {
            for (var item in PageObj[options.knockoutContextName]()) {
                findObjectsInChildTreeById({
                    item: PageObj[options.knockoutContextName]()[item],
                    id: options.id,
                    results: options.results
                });
            }
        }
    } else {
        if (typeof options.item.Fields != 'undefined') {
            if (options.item.Fields['Id']() == options.id)
                options.results.push(options.item);
        } else {
            if (options.item['Id'] == options.id)
                options.results.push(options.item);
        }

        if (options.item.Children != undefined) {
            for (var item in options.item.Children) {
                findObjectsInChildTreeById({
                    item: options.item.Children[item],
                    id: options.id,
                    results: options.results
                });
            }
        }
    }
}

function getChildrenForMasterRecordList(options) {
    var parentTable = options.parentTable,
        masterIdColumn = options.masterIdColumn,
        parentIdColumn = options.parentIdColumn,
        knockoutContextName = options.knockoutContextName,
        masterIds = getParentIdsAndMastersOfParentsFromPageObj(knockoutContextName, masterIdColumn);
    for (var item in PageObj[options.knockoutContextName]()) {
        AddIsExpandedProperty(PageObj[knockoutContextName]()[item]);
    }

    var dbManager = new OnTheMoveDatabaseManager();
    dbManager.queryDatabase({
        statement: {
            Tables: [{
                Alias: parentTable,
                JoinSpec: null,
                JoinType: """",
                Name: parentTable
            }, {
                Alias: ""Record"",
                JoinSpec: ""Record.Id = "" + parentTable + "".Id"",
                JoinType: ""INNER"",
                Name: ""Record""
            }],
            WhereClause: parentTable + ""."" + masterIdColumn + "" IN ('"" + masterIds.join(""','"") + ""') AND Record.RecordType ='"" + parentTable + ""'"",
            SelectFields: [{
                IsAggregate: false,
                Name: ""*""
            }],
            DisablePaging: true,
            OrderClause: ""Record.Id""
        },
        knockoutContextName: knockoutContextName,
        isObservable: false,
        masterIdColumn: masterIdColumn,
        masterIdList: masterIds,
        parentIdColumn: parentIdColumn,
        parentTable: options.parentTable,
        success: function (results, tableArray, columnArray, options) {
            ComposeRecordsToTreeStructure(results, tableArray, columnArray, options);
        }
    });
}

function getParentIdsAndMastersOfParentsFromPageObj(knockoutContextName, masterColumnName) {
    var list = [];
    if (typeof PageObj[knockoutContextName] != 'undefined') {
        for (var item in PageObj[knockoutContextName]()) {
            if ($.inArray(PageObj[knockoutContextName]()[item].Fields['Id'](), list) == -1) {
                list.push(PageObj[knockoutContextName]()[item].Fields['Id']());
            }
            if (PageObj[knockoutContextName]()[item].Fields[masterColumnName]() != '' && $.inArray(PageObj[knockoutContextName]()[item].Fields[masterColumnName](), list) == -1) {
                list.push(PageObj[knockoutContextName]()[item].Fields[masterColumnName]());
             }
        }
    }

    return list
}

function InTree(master, item) {
    return $.inArray(item, $.map(master, function recurs(n) {
        return ($.isArray(n) ? $.map(n, recurs) : n);
    })) != -1;
}

",1,1,0,0,0,0,0,- User just doesn't know how to approach a problem,"Label: 1

Reasoning:

This question expresses technical debt because the current implementation of the tree-building algorithm is suboptimal and affects performance. The user is seeking a faster alternative to the `InTree` function, which suggests that the current implementation is not efficient. Additionally, the requirement to detect infinite recursion loops and prevent the creation of tree relationships indicates that the code is prone to errors and needs improvement.

Furthermore, the provided code is complex and contains multiple nested functions, making it difficult to maintain and debug. The use of `eval` statements and global variables (e.g., `PageObj`) also raises concerns about code quality and maintainability.

The user's request for optimization and the complexity of the code suggest that there may be opportunities for refactoring and improvement, indicating technical debt.",Yes,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Very Severe,Very Severe,,Very Severe
45229677,45229677,Cannot obtain values while parallelizing 2 for loops,"<p>I am trying to run the following snippet which appends data to lists 'tests1' and 'tests2'. But when I print 'tests1' and 'tests2', the displayed list is empty. Anything incorrect here?      </p>

<pre><code>tests1 = []
tests2 = []

def func1():
    for i in range(25,26):
        tests1.append(test_loader.get_tests(test_prefix=new_paths[i],tags=params.get('tags', None),
                                    exclude=params.get('exclude', False)))

def func2():
    for i in range(26,27):
        tests2.append(test_loader.get_tests(test_prefix=new_paths[i],tags=params.get('tags', None),
                                    exclude=params.get('exclude', False)))


p1 = mp.Process(target=func1)
p2 = mp.Process(target=func2)

p1.start()
p2.start()
p1.join()
p2.join()

print tests1
print tests2
</code></pre>
","i am trying to run the following snippet which appends data to lists tests1 and tests2 but when i print tests1 and tests2 the displayed list is empty anything incorrect here      
",0,"I am trying to run the following snippet which appends data to lists 'tests1' and 'tests2'. But when I print 'tests1' and 'tests2', the displayed list is empty. Anything incorrect here?      
tests1 = []
tests2 = []

def func1():
    for i in range(25,26):
        tests1.append(test_loader.get_tests(test_prefix=new_paths[i],tags=params.get('tags', None),
                                    exclude=params.get('exclude', False)))

def func2():
    for i in range(26,27):
        tests2.append(test_loader.get_tests(test_prefix=new_paths[i],tags=params.get('tags', None),
                                    exclude=params.get('exclude', False)))


p1 = mp.Process(target=func1)
p2 = mp.Process(target=func2)

p1.start()
p2.start()
p1.join()
p2.join()

print tests1
print tests2

","Cannot obtain values while parallelizing 2 for loops
I am trying to run the following snippet which appends data to lists 'tests1' and 'tests2'. But when I print 'tests1' and 'tests2', the displayed list is empty. Anything incorrect here?      
tests1 = []
tests2 = []

def func1():
    for i in range(25,26):
        tests1.append(test_loader.get_tests(test_prefix=new_paths[i],tags=params.get('tags', None),
                                    exclude=params.get('exclude', False)))

def func2():
    for i in range(26,27):
        tests2.append(test_loader.get_tests(test_prefix=new_paths[i],tags=params.get('tags', None),
                                    exclude=params.get('exclude', False)))


p1 = mp.Process(target=func1)
p2 = mp.Process(target=func2)

p1.start()
p2.start()
p1.join()
p2.join()

print tests1
print tests2

",1,0,0,0,0,0,1,- Code not working as intended,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,1,0,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
3717420,3717420,Is there a benefit to using a return statement that returns nothing?,"<p>I'm refactoring a large javascript document that I picked up from an open source project.  A number of functions use inconsistent return statements.  Here's a simple example of what I mean:</p>

<pre><code>var func = function(param) {
    if (!param) {
        return;
    }
    // do stuff
    return true;
}
</code></pre>

<p>Sometimes the functions return boolean, sometimes strings or other things.  Usually they are inconsistently paired with a simple <code>return;</code> statement inside of a conditional.</p>

<p>The problem is that the code is complex.  It is a parser that uses a multitude of unique RegEx matches, creates and destroys DOM nodes on the fly, etc.  Preliminary testing shows that, in the above example, I could change the <code>return;</code> statement to become <code>return false;</code>, but I'm concerned that I may not realize that it had a negative impact (i.e. some feature stopped working) on the script until much later.</p>

<p>So my questions:  Is there a benefit to using a blank return statement?  Could this have been intentionally coded this way or was it just lazy? Can I change them all to <code>return false;</code>, or <code>return null;</code> or do I need to dig through every call and find out what they are doing with the results of those functions?</p>
","im refactoring a large javascript document that i picked up from an open source project  a number of functions use inconsistent return statements  heres a simple example of what i mean
sometimes the functions return boolean sometimes strings or other things  usually they are inconsistently paired with a simple return statement inside of a conditional
the problem is that the code is complex  it is a parser that uses a multitude of unique regex matches creates and destroys dom nodes on the fly etc  preliminary testing shows that in the above example i could change the return statement to become return false but im concerned that i may not realize that it had a negative impact ie some feature stopped working on the script until much later
so my questions  is there a benefit to using a blank return statement  could this have been intentionally coded this way or was it just lazy can i change them all to return false or return null or do i need to dig through every call and find out what they are doing with the results of those functions
",1,"I'm refactoring a large javascript document that I picked up from an open source project.  A number of functions use inconsistent return statements.  Here's a simple example of what I mean:
var func = function(param) {
    if (!param) {
        return;
    }
    // do stuff
    return true;
}

Sometimes the functions return boolean, sometimes strings or other things.  Usually they are inconsistently paired with a simple return; statement inside of a conditional.
The problem is that the code is complex.  It is a parser that uses a multitude of unique RegEx matches, creates and destroys DOM nodes on the fly, etc.  Preliminary testing shows that, in the above example, I could change the return; statement to become return false;, but I'm concerned that I may not realize that it had a negative impact (i.e. some feature stopped working) on the script until much later.
So my questions:  Is there a benefit to using a blank return statement?  Could this have been intentionally coded this way or was it just lazy? Can I change them all to return false;, or return null; or do I need to dig through every call and find out what they are doing with the results of those functions?
","Is there a benefit to using a return statement that returns nothing?
I'm refactoring a large javascript document that I picked up from an open source project.  A number of functions use inconsistent return statements.  Here's a simple example of what I mean:
var func = function(param) {
    if (!param) {
        return;
    }
    // do stuff
    return true;
}

Sometimes the functions return boolean, sometimes strings or other things.  Usually they are inconsistently paired with a simple return; statement inside of a conditional.
The problem is that the code is complex.  It is a parser that uses a multitude of unique RegEx matches, creates and destroys DOM nodes on the fly, etc.  Preliminary testing shows that, in the above example, I could change the return; statement to become return false;, but I'm concerned that I may not realize that it had a negative impact (i.e. some feature stopped working) on the script until much later.
So my questions:  Is there a benefit to using a blank return statement?  Could this have been intentionally coded this way or was it just lazy? Can I change them all to return false;, or return null; or do I need to dig through every call and find out what they are doing with the results of those functions?
",1,0,1,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,0,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
25505186,25505186,how do I debug client side browser/HTML-structure performance issues?,"<p>I have developed my first web application. It is running, and doing what it is supposed to, but some pages have performance issues. I have tuned the application in several areas, but I am completely lost with one issue:</p>

<p>I have a calendar with events, that toggles invisible or visible when a user toggles options. The whole thing is updated client side, with the use of javascript, and JQuery functions. The update is just a little too slow.</p>

<p>I had no experience in debugging this, so I took a guess based on previous reading, that nested tables might be the culprit. I worked for several hours on untangling these as <a href=""http://webdesign.about.com/od/speed/a/dont-use-nested-tables.htm"" rel=""nofollow"">this article</a> describes. Then it was the issue of making the css work again, and then testing. The performance was unfortunately even worse than before.</p>

<p><strong>I would like to know if there is a better way to approach my problem, as trial and error will get old in the long run. Can I somehow time the browsers rendering of a <code>&lt;table&gt;</code>? How else can I debug performance issues due to client side issues/html structure?</strong> I am hoping for a cross-browser solution, but my main concern is IE8 and Google Chrome. Take a guess on which one is the slow one...</p>

<p>I tried to make a JSFiddle of my specific issue, but it ended up being 20.000 lines of code, and still not working properly. I guess a size performance issue isn't really the best fit for JSFiddles format. </p>
","i have developed my first web application it is running and doing what it is supposed to but some pages have performance issues i have tuned the application in several areas but i am completely lost with one issue
i have a calendar with events that toggles invisible or visible when a user toggles options the whole thing is updated client side with the use of javascript and jquery functions the update is just a little too slow
i had no experience in debugging this so i took a guess based on previous reading that nested tables might be the culprit i worked for several hours on untangling these as this article describes then it was the issue of making the css work again and then testing the performance was unfortunately even worse than before
i would like to know if there is a better way to approach my problem as trial and error will get old in the long run can i somehow time the browsers rendering of a table how else can i debug performance issues due to client side issueshtml structure i am hoping for a crossbrowser solution but my main concern is ie8 and google chrome take a guess on which one is the slow one
i tried to make a jsfiddle of my specific issue but it ended up being 20000 lines of code and still not working properly i guess a size performance issue isnt really the best fit for jsfiddles format 
",1,"I have developed my first web application. It is running, and doing what it is supposed to, but some pages have performance issues. I have tuned the application in several areas, but I am completely lost with one issue:
I have a calendar with events, that toggles invisible or visible when a user toggles options. The whole thing is updated client side, with the use of javascript, and JQuery functions. The update is just a little too slow.
I had no experience in debugging this, so I took a guess based on previous reading, that nested tables might be the culprit. I worked for several hours on untangling these as this article describes. Then it was the issue of making the css work again, and then testing. The performance was unfortunately even worse than before.
I would like to know if there is a better way to approach my problem, as trial and error will get old in the long run. Can I somehow time the browsers rendering of a <table>? How else can I debug performance issues due to client side issues/html structure? I am hoping for a cross-browser solution, but my main concern is IE8 and Google Chrome. Take a guess on which one is the slow one...
I tried to make a JSFiddle of my specific issue, but it ended up being 20.000 lines of code, and still not working properly. I guess a size performance issue isn't really the best fit for JSFiddles format. 
","how do I debug client side browser/HTML-structure performance issues?
I have developed my first web application. It is running, and doing what it is supposed to, but some pages have performance issues. I have tuned the application in several areas, but I am completely lost with one issue:
I have a calendar with events, that toggles invisible or visible when a user toggles options. The whole thing is updated client side, with the use of javascript, and JQuery functions. The update is just a little too slow.
I had no experience in debugging this, so I took a guess based on previous reading, that nested tables might be the culprit. I worked for several hours on untangling these as this article describes. Then it was the issue of making the css work again, and then testing. The performance was unfortunately even worse than before.
I would like to know if there is a better way to approach my problem, as trial and error will get old in the long run. Can I somehow time the browsers rendering of a <table>? How else can I debug performance issues due to client side issues/html structure? I am hoping for a cross-browser solution, but my main concern is IE8 and Google Chrome. Take a guess on which one is the slow one...
I tried to make a JSFiddle of my specific issue, but it ended up being 20.000 lines of code, and still not working properly. I guess a size performance issue isn't really the best fit for JSFiddles format. 
",1,0,1,0,0,0,1,,,,1,1,Web Performance,Web Performance,,Web Performance,1,0,1,0,0,0,,,Test,Test,,Test,Mild,Mild,,Mild
356846,356846,Will using 'var' affect performance?,"<p>Earlier I asked a question about <a href=""https://stackoverflow.com/questions/335682/mvc-examples-use-of-var"">why I see so many examples use the <code>var</code>keyword</a> and got the answer that while it is only necessary for anonymous types, that it is used nonetheless to make writing code 'quicker'/easier and 'just because'.</p>

<p>Following <a href=""http://www.interact-sw.co.uk/iangblog/2005/09/23/varisntobject"" rel=""noreferrer"">this link (""C# 3.0 - Var Isn't Objec"")</a> I saw that <code>var</code> gets compiled down to the correct type in the IL (you will see it about midway down article).</p>

<p>My question is how much more, if any, IL code does using the <code>var</code> keyword take, and would it be even close to having a measurable level on the performance of the code if it was used everywhere?</p>
","earlier i asked a question about why i see so many examples use the varkeyword and got the answer that while it is only necessary for anonymous types that it is used nonetheless to make writing code quickereasier and just because
following this link c 30  var isnt objec i saw that var gets compiled down to the correct type in the il you will see it about midway down article
my question is how much more if any il code does using the var keyword take and would it be even close to having a measurable level on the performance of the code if it was used everywhere
",1,"Earlier I asked a question about why I see so many examples use the varkeyword and got the answer that while it is only necessary for anonymous types, that it is used nonetheless to make writing code 'quicker'/easier and 'just because'.
Following this link (""C# 3.0 - Var Isn't Objec"") I saw that var gets compiled down to the correct type in the IL (you will see it about midway down article).
My question is how much more, if any, IL code does using the var keyword take, and would it be even close to having a measurable level on the performance of the code if it was used everywhere?
","Will using 'var' affect performance?
Earlier I asked a question about why I see so many examples use the varkeyword and got the answer that while it is only necessary for anonymous types, that it is used nonetheless to make writing code 'quicker'/easier and 'just because'.
Following this link (""C# 3.0 - Var Isn't Objec"") I saw that var gets compiled down to the correct type in the IL (you will see it about midway down article).
My question is how much more, if any, IL code does using the var keyword take, and would it be even close to having a measurable level on the performance of the code if it was used everywhere?
",1,0,1,0,0,0,0,- Is a theoretical question,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,0,1,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
4239371,4239371,Suggestions on how to speed up a distance calculation,"<p>Consider the following class:</p>

<pre><code>class SquareErrorDistance(object):
    def __init__(self, dataSample):
        variance = var(list(dataSample))
        if variance == 0:
            self._norm = 1.0
        else:
            self._norm = 1.0 / (2 * variance)

    def __call__(self, u, v): # u and v are floats
        return (u - v) ** 2 * self._norm
</code></pre>

<p>I use it to calculate the distance between two elements of a vector. I basically create one instance of that class for every dimension of the vector that uses this distance measure (there are dimensions that use other distance measures). Profiling reveals that the <code>__call__</code> function of this class accounts for 90% of the running-time of my knn-implementation (who would have thought). I do not think there is any pure-Python way to speed this up, but maybe if I implement it in C? </p>

<p>If I run a simple C program that just calculates distances for random values using the formula above, it is orders of magnitude faster than Python. So I tried using <a href=""http://docs.python.org/library/ctypes.html"" rel=""nofollow"">ctypes</a> and call a C function that does the computation, but apparently the conversion of the parameters and return-values is far to expensive, because the resulting code is much slower.</p>

<p>I could of course implement the entire knn in C and just call that, but the problem is that, like I described, I use different distance functions for some dimension of the vectors, and translating these to C would be too much work.</p>

<p>So what are my alternatives? Will writing the C-function using the <a href=""http://docs.python.org/extending/extending.html"" rel=""nofollow"">Python C-API</a> get rid of the overhead? Are there any other ways to speed this calculation up?</p>
","consider the following class
i use it to calculate the distance between two elements of a vector i basically create one instance of that class for every dimension of the vector that uses this distance measure there are dimensions that use other distance measures profiling reveals that the __call__ function of this class accounts for 90 of the runningtime of my knnimplementation who would have thought i do not think there is any purepython way to speed this up but maybe if i implement it in c 
if i run a simple c program that just calculates distances for random values using the formula above it is orders of magnitude faster than python so i tried using ctypes and call a c function that does the computation but apparently the conversion of the parameters and returnvalues is far to expensive because the resulting code is much slower
i could of course implement the entire knn in c and just call that but the problem is that like i described i use different distance functions for some dimension of the vectors and translating these to c would be too much work
so what are my alternatives will writing the cfunction using the python capi get rid of the overhead are there any other ways to speed this calculation up
",1,"Consider the following class:
class SquareErrorDistance(object):
    def __init__(self, dataSample):
        variance = var(list(dataSample))
        if variance == 0:
            self._norm = 1.0
        else:
            self._norm = 1.0 / (2 * variance)

    def __call__(self, u, v): # u and v are floats
        return (u - v) ** 2 * self._norm

I use it to calculate the distance between two elements of a vector. I basically create one instance of that class for every dimension of the vector that uses this distance measure (there are dimensions that use other distance measures). Profiling reveals that the __call__ function of this class accounts for 90% of the running-time of my knn-implementation (who would have thought). I do not think there is any pure-Python way to speed this up, but maybe if I implement it in C? 
If I run a simple C program that just calculates distances for random values using the formula above, it is orders of magnitude faster than Python. So I tried using ctypes and call a C function that does the computation, but apparently the conversion of the parameters and return-values is far to expensive, because the resulting code is much slower.
I could of course implement the entire knn in C and just call that, but the problem is that, like I described, I use different distance functions for some dimension of the vectors, and translating these to C would be too much work.
So what are my alternatives? Will writing the C-function using the Python C-API get rid of the overhead? Are there any other ways to speed this calculation up?
","Suggestions on how to speed up a distance calculation
Consider the following class:
class SquareErrorDistance(object):
    def __init__(self, dataSample):
        variance = var(list(dataSample))
        if variance == 0:
            self._norm = 1.0
        else:
            self._norm = 1.0 / (2 * variance)

    def __call__(self, u, v): # u and v are floats
        return (u - v) ** 2 * self._norm

I use it to calculate the distance between two elements of a vector. I basically create one instance of that class for every dimension of the vector that uses this distance measure (there are dimensions that use other distance measures). Profiling reveals that the __call__ function of this class accounts for 90% of the running-time of my knn-implementation (who would have thought). I do not think there is any pure-Python way to speed this up, but maybe if I implement it in C? 
If I run a simple C program that just calculates distances for random values using the formula above, it is orders of magnitude faster than Python. So I tried using ctypes and call a C function that does the computation, but apparently the conversion of the parameters and return-values is far to expensive, because the resulting code is much slower.
I could of course implement the entire knn in C and just call that, but the problem is that, like I described, I use different distance functions for some dimension of the vectors, and translating these to C would be too much work.
So what are my alternatives? Will writing the C-function using the Python C-API get rid of the overhead? Are there any other ways to speed this calculation up?
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
76023347,76023347,What types of cycle/recursive dependency problems can occur with dependency injection systems?,"<p>I am implementing a dependency injection library, and so far I finished the library as far as running applications without recursive dependency problems.<br />
I am looking to implement protections against cycles so the app could be stopped as soon as a problem is detected, ideally with a helpful message on which classes cause the cycle.</p>
<p>A typical contract for most DI libraries is this:<br />
For each type you can request a <code>Provider&lt;T&gt;</code> with a <code>get()</code> method.<br />
(all constructor injection etc. is just a syntax sugar on top if this contract)</p>
<p>They should be thread-safe and support concurrency at all times.</p>
<p>Two types of bindings, one is a &quot;factory&quot; creating objects on demand, and the second is &quot;singleton&quot; which can only be initialized once.</p>
<p>If I am correct there are two separate kind of problems to handle:</p>
<pre class=""lang-kotlin prettyprint-override""><code>@Singleton 
class A(val dep: B)
@Singleton
class B(val dep: A)
</code></pre>
<p>As singleton construction is synchronized on the binding object, that should result in a classic deadlock.</p>
<pre class=""lang-kotlin prettyprint-override""><code>class A(val dep: B)
class B(val dep: A)
</code></pre>
<p>That to me should result in a StackOverflowError.</p>
<p>What's the best way to detect these cycles and provide useful exceptions pointing to source of the cycle?
Do you think there's a 3rd type of recursive problem to handle that cannot be generalized to these 2 cases?</p>
","i am implementing a dependency injection library and so far i finished the library as far as running applications without recursive dependency problems
i am looking to implement protections against cycles so the app could be stopped as soon as a problem is detected ideally with a helpful message on which classes cause the cycle
a typical contract for most di libraries is this
for each type you can request a providert with a get method
all constructor injection etc is just a syntax sugar on top if this contract
they should be threadsafe and support concurrency at all times
two types of bindings one is a factory creating objects on demand and the second is singleton which can only be initialized once
if i am correct there are two separate kind of problems to handle
singleton 
class aval dep b
singleton
class bval dep a

as singleton construction is synchronized on the binding object that should result in a classic deadlock
class aval dep b
class bval dep a

that to me should result in a stackoverflowerror
whats the best way to detect these cycles and provide useful exceptions pointing to source of the cycle
do you think theres a 3rd type of recursive problem to handle that cannot be generalized to these 2 cases
",1,"I am implementing a dependency injection library, and so far I finished the library as far as running applications without recursive dependency problems.
I am looking to implement protections against cycles so the app could be stopped as soon as a problem is detected, ideally with a helpful message on which classes cause the cycle.
A typical contract for most DI libraries is this:
For each type you can request a Provider<T> with a get() method.
(all constructor injection etc. is just a syntax sugar on top if this contract)
They should be thread-safe and support concurrency at all times.
Two types of bindings, one is a ""factory"" creating objects on demand, and the second is ""singleton"" which can only be initialized once.
If I am correct there are two separate kind of problems to handle:
@Singleton 
class A(val dep: B)
@Singleton
class B(val dep: A)

As singleton construction is synchronized on the binding object, that should result in a classic deadlock.
class A(val dep: B)
class B(val dep: A)

That to me should result in a StackOverflowError.
What's the best way to detect these cycles and provide useful exceptions pointing to source of the cycle?
Do you think there's a 3rd type of recursive problem to handle that cannot be generalized to these 2 cases?
","What types of cycle/recursive dependency problems can occur with dependency injection systems?
I am implementing a dependency injection library, and so far I finished the library as far as running applications without recursive dependency problems.
I am looking to implement protections against cycles so the app could be stopped as soon as a problem is detected, ideally with a helpful message on which classes cause the cycle.
A typical contract for most DI libraries is this:
For each type you can request a Provider<T> with a get() method.
(all constructor injection etc. is just a syntax sugar on top if this contract)
They should be thread-safe and support concurrency at all times.
Two types of bindings, one is a ""factory"" creating objects on demand, and the second is ""singleton"" which can only be initialized once.
If I am correct there are two separate kind of problems to handle:
@Singleton 
class A(val dep: B)
@Singleton
class B(val dep: A)

As singleton construction is synchronized on the binding object, that should result in a classic deadlock.
class A(val dep: B)
class B(val dep: A)

That to me should result in a StackOverflowError.
What's the best way to detect these cycles and provide useful exceptions pointing to source of the cycle?
Do you think there's a 3rd type of recursive problem to handle that cannot be generalized to these 2 cases?
",0,1,1,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
40327565,40327565,Point in Polygon C++ Algorithm implementation,"<p>I have tried two algorithms that were answers in other StackOverflow questions:</p>
<blockquote>
<p><a href=""https://stackoverflow.com/questions/27589796/check-point-within-polygon"">Check point within polygon</a></p>
<p><a href=""https://stackoverflow.com/questions/11716268/point-in-polygon-algorithm"">Point in Polygon Algorithm</a></p>
</blockquote>
<p>Both were showing some points in as out or out as in while other points were correct. This test cast assumes that there are always only 4 vertices (rectangle)</p>
<pre><code>bool PointInPolygon(Coordinate point, vector&lt;Coordinate&gt; points) {
cout &lt;&lt; &quot;x,y&quot; &lt;&lt; point.getX() &lt;&lt; &quot;,&quot; &lt;&lt; point.getY() &lt;&lt; endl;
cout &lt;&lt; &quot;TEST COOR ARRAY&quot; &lt;&lt; endl;
for (int i=0; i&lt;4; i++) {
    cout &lt;&lt; points[i].getX() &lt;&lt; &quot;,&quot; &lt;&lt; points[i].getY() &lt;&lt; endl;
}
   
int i, j, nvert = points.size();
bool c = false;

for(i = 0, j = nvert - 1; i &lt; nvert; j = i++) {
  if( ( (points[i].getY() &gt; point.getY() ) != (points[j].getY() &gt; point.getY()) ) &amp;&amp;
      (point.getX() &lt; (points[j].getX() - points[i].getX()) * (point.getY() - points[i].getY()) / (points[j].getY() - points[i].getY()) + points[i].getX())
    )
    c = !c;
}

cout &lt;&lt; c &lt;&lt; &quot;======================&quot; &lt;&lt; endl;

return c;
}
</code></pre>
<p>And the output was wrong where (2,3) and (1,1) shouldn't be in.
Lines on the perimeter are not considered to be in.
But even so, then 2,3 should always be in.</p>
<pre><code>x,y1,1
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y2,2
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y2,3
TEST COOR ARRAY
1,1
1,3
4,3
4,1
0======================
OUT
x,y3,2
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y3,3
TEST COOR ARRAY
1,1
1,3
4,3
4,1
0======================
OUT
</code></pre>
<p>I have similar issues in using the other algorithms I found as well. If anyone could point me in the right direction, I'd appreciate it a lot thanks!</p>
","i have tried two algorithms that were answers in other stackoverflow questions

check point within polygon
point in polygon algorithm

both were showing some points in as out or out as in while other points were correct this test cast assumes that there are always only 4 vertices rectangle
and the output was wrong where 23 and 11 shouldnt be in
lines on the perimeter are not considered to be in
but even so then 23 should always be in
i have similar issues in using the other algorithms i found as well if anyone could point me in the right direction id appreciate it a lot thanks
",0,"I have tried two algorithms that were answers in other StackOverflow questions:

Check point within polygon
Point in Polygon Algorithm

Both were showing some points in as out or out as in while other points were correct. This test cast assumes that there are always only 4 vertices (rectangle)
bool PointInPolygon(Coordinate point, vector<Coordinate> points) {
cout << ""x,y"" << point.getX() << "","" << point.getY() << endl;
cout << ""TEST COOR ARRAY"" << endl;
for (int i=0; i<4; i++) {
    cout << points[i].getX() << "","" << points[i].getY() << endl;
}
   
int i, j, nvert = points.size();
bool c = false;

for(i = 0, j = nvert - 1; i < nvert; j = i++) {
  if( ( (points[i].getY() > point.getY() ) != (points[j].getY() > point.getY()) ) &&
      (point.getX() < (points[j].getX() - points[i].getX()) * (point.getY() - points[i].getY()) / (points[j].getY() - points[i].getY()) + points[i].getX())
    )
    c = !c;
}

cout << c << ""======================"" << endl;

return c;
}

And the output was wrong where (2,3) and (1,1) shouldn't be in.
Lines on the perimeter are not considered to be in.
But even so, then 2,3 should always be in.
x,y1,1
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y2,2
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y2,3
TEST COOR ARRAY
1,1
1,3
4,3
4,1
0======================
OUT
x,y3,2
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y3,3
TEST COOR ARRAY
1,1
1,3
4,3
4,1
0======================
OUT

I have similar issues in using the other algorithms I found as well. If anyone could point me in the right direction, I'd appreciate it a lot thanks!
","Point in Polygon C++ Algorithm implementation
I have tried two algorithms that were answers in other StackOverflow questions:

Check point within polygon
Point in Polygon Algorithm

Both were showing some points in as out or out as in while other points were correct. This test cast assumes that there are always only 4 vertices (rectangle)
bool PointInPolygon(Coordinate point, vector<Coordinate> points) {
cout << ""x,y"" << point.getX() << "","" << point.getY() << endl;
cout << ""TEST COOR ARRAY"" << endl;
for (int i=0; i<4; i++) {
    cout << points[i].getX() << "","" << points[i].getY() << endl;
}
   
int i, j, nvert = points.size();
bool c = false;

for(i = 0, j = nvert - 1; i < nvert; j = i++) {
  if( ( (points[i].getY() > point.getY() ) != (points[j].getY() > point.getY()) ) &&
      (point.getX() < (points[j].getX() - points[i].getX()) * (point.getY() - points[i].getY()) / (points[j].getY() - points[i].getY()) + points[i].getX())
    )
    c = !c;
}

cout << c << ""======================"" << endl;

return c;
}

And the output was wrong where (2,3) and (1,1) shouldn't be in.
Lines on the perimeter are not considered to be in.
But even so, then 2,3 should always be in.
x,y1,1
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y2,2
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y2,3
TEST COOR ARRAY
1,1
1,3
4,3
4,1
0======================
OUT
x,y3,2
TEST COOR ARRAY
1,1
1,3
4,3
4,1
1======================
IN
x,y3,3
TEST COOR ARRAY
1,1
1,3
4,3
4,1
0======================
OUT

I have similar issues in using the other algorithms I found as well. If anyone could point me in the right direction, I'd appreciate it a lot thanks!
",1,0,0,0,0,0,1,- Code not working as intended,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,0,0,0,0,0,,,Test,Test,,Test,Very Severe,Very Severe,,Very Severe
42355719,42355719,unity3d - byte[] to Texture kills performance,"<p>I added the functionality to download images and make a Skybox out of it. With a loop within a Coroutine, it works fine. But at the steps </p>

<pre><code>byte[] = bytes = null;
bytes = www.texture.EncodeToPNG();
Texture2D tx = new Texture2D(2, 2);
tx.LoadImage(bytes);
</code></pre>

<p>cost much of time and the app laggs.</p>

<p>Any ideas to prevent that?</p>

<p><a href=""https://i.stack.imgur.com/SxfeG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SxfeG.png"" alt=""image to byte[] to Texture""></a></p>
","i added the functionality to download images and make a skybox out of it with a loop within a coroutine it works fine but at the steps 
cost much of time and the app laggs
any ideas to prevent that

",1,"I added the functionality to download images and make a Skybox out of it. With a loop within a Coroutine, it works fine. But at the steps 
byte[] = bytes = null;
bytes = www.texture.EncodeToPNG();
Texture2D tx = new Texture2D(2, 2);
tx.LoadImage(bytes);

cost much of time and the app laggs.
Any ideas to prevent that?

","unity3d - byte[] to Texture kills performance
I added the functionality to download images and make a Skybox out of it. With a loop within a Coroutine, it works fine. But at the steps 
byte[] = bytes = null;
bytes = www.texture.EncodeToPNG();
Texture2D tx = new Texture2D(2, 2);
tx.LoadImage(bytes);

cost much of time and the app laggs.
Any ideas to prevent that?

",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
4046468,4046468,good refactoring or bad?,"<p>Given the following code:</p>

<pre><code>    status = row[COL_STATUS]

    if status == ""idle"":
        row[COL_EDITABLE] = True
        row[COL_FONTSTYLE] = pango.STYLE_NORMAL
        row[COL_WEIGHT] = pango.WEIGHT_NORMAL
    elif status == ""DCed"":
        row[COL_EDITABLE] = True
        row[COL_FONTSTYLE] = pango.STYLE_ITALIC
        row[COL_WEIGHT] = pango.WEIGHT_NORMAL
    else:
        row[COL_EDITABLE] = False
        row[COL_FONTSTYLE] = pango.STYLE_NORMAL
        row[COL_WEIGHT] = pango.WEIGHT_BOLD
</code></pre>

<p>Would the following be a net beneficial refactoring, in your opinion?</p>

<pre><code>    d = {""idle"": (True,  pango.STYLE_NORMAL, pango.WEIGHT_NORMAL),
         ""DCed"": (True,  pango.STYLE_ITALIC, pango.WEIGHT_NORMAL),
         None:   (False, pango.STYLE_NORMAL, pango.WEIGHT_BOLD)}
    e,f,w = d.get(status, d[None])
    row[COL_EDITABLE] = e
    row[COL_FONTSTYLE] = f
    row[COL_WEIGHT] = w
</code></pre>

<p>What if there were more cases or more row components to edit?</p>
","given the following code
would the following be a net beneficial refactoring in your opinion
what if there were more cases or more row components to edit
",0,"Given the following code:
    status = row[COL_STATUS]

    if status == ""idle"":
        row[COL_EDITABLE] = True
        row[COL_FONTSTYLE] = pango.STYLE_NORMAL
        row[COL_WEIGHT] = pango.WEIGHT_NORMAL
    elif status == ""DCed"":
        row[COL_EDITABLE] = True
        row[COL_FONTSTYLE] = pango.STYLE_ITALIC
        row[COL_WEIGHT] = pango.WEIGHT_NORMAL
    else:
        row[COL_EDITABLE] = False
        row[COL_FONTSTYLE] = pango.STYLE_NORMAL
        row[COL_WEIGHT] = pango.WEIGHT_BOLD

Would the following be a net beneficial refactoring, in your opinion?
    d = {""idle"": (True,  pango.STYLE_NORMAL, pango.WEIGHT_NORMAL),
         ""DCed"": (True,  pango.STYLE_ITALIC, pango.WEIGHT_NORMAL),
         None:   (False, pango.STYLE_NORMAL, pango.WEIGHT_BOLD)}
    e,f,w = d.get(status, d[None])
    row[COL_EDITABLE] = e
    row[COL_FONTSTYLE] = f
    row[COL_WEIGHT] = w

What if there were more cases or more row components to edit?
","good refactoring or bad?
Given the following code:
    status = row[COL_STATUS]

    if status == ""idle"":
        row[COL_EDITABLE] = True
        row[COL_FONTSTYLE] = pango.STYLE_NORMAL
        row[COL_WEIGHT] = pango.WEIGHT_NORMAL
    elif status == ""DCed"":
        row[COL_EDITABLE] = True
        row[COL_FONTSTYLE] = pango.STYLE_ITALIC
        row[COL_WEIGHT] = pango.WEIGHT_NORMAL
    else:
        row[COL_EDITABLE] = False
        row[COL_FONTSTYLE] = pango.STYLE_NORMAL
        row[COL_WEIGHT] = pango.WEIGHT_BOLD

Would the following be a net beneficial refactoring, in your opinion?
    d = {""idle"": (True,  pango.STYLE_NORMAL, pango.WEIGHT_NORMAL),
         ""DCed"": (True,  pango.STYLE_ITALIC, pango.WEIGHT_NORMAL),
         None:   (False, pango.STYLE_NORMAL, pango.WEIGHT_BOLD)}
    e,f,w = d.get(status, d[None])
    row[COL_EDITABLE] = e
    row[COL_FONTSTYLE] = f
    row[COL_WEIGHT] = w

What if there were more cases or more row components to edit?
",0,0,0,1,0,0,1,- Suboptimal code ,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,0,0,1,1,0,"Change: Code Review to 1
Reason: User is proposing an already written change asking for feedback on it",,Code,Code,,Code,None,None,,None
63976801,63976801,How does one refactor best this timer/stopwatch code-base towards a better code-reuse following the OOP paradigm and the DRY principle?,"<p>I am having trouble with following <strong>OOP</strong> in javascript.
How can I make this code more <strong>Object-oriented</strong> and reusable?</p>
<p>I tried reading up on OOP concepts in JS but couldn't figure a way around making this code be one. <em>Any suggestions?</em></p>
<p><strong>PS: This is the code for making a stopwatch</strong></p>
<pre><code>//Define variables to hold time values
let seconds = 0;
let minutes = 0;
let hours = 0;

//Define variable to  hold &quot;display&quot; value
let interval = null;

//Define variable to hold the clock status
let status = &quot;paused&quot;;

//Clock function ( logic to determine when to increment next value, etc.)
function clock() {
  seconds++;

  //Logic to determine when to increment next value
  if (seconds &gt;= 60) {
    seconds = 0;
    minutes++;

    if (minutes &gt;= 60) {
      minutes = 0;
      hours++;
    }
  }

  //Display updated time values to user
  document.getElementById(&quot;display&quot;).innerHTML =
    //If seconds/minutes/hours are only one digit, add a leading 0 to the value
    `${hours ? (hours &gt; 9 ? hours : `0${hours}`) : &quot;00&quot;}:${minutes ? (minutes &gt; 9 ? minutes : `0${minutes}`) : &quot;00&quot;}:${seconds &gt; 9 ? seconds : `0${seconds}`}`;
}

function startPause() {
  if (status === &quot;paused&quot;) {
    //Start the stopwatch (by calling the setInterval() function)
    interval = window.setInterval(clock, 1000);
    document.getElementById(&quot;startPause&quot;).innerHTML = &quot;Pause&quot;;
    status = &quot;started&quot;;
  } else {
    window.clearInterval(interval);
    document.getElementById(&quot;startPause&quot;).innerHTML = &quot;Resume&quot;;
    status = &quot;paused&quot;;
  }
}

//Function to reset the stopwatch
function reset() {
  seconds = 0;
  minutes = 0;
  hours = 0;
  document.getElementById(&quot;display&quot;).innerHTML = &quot;00:00:00&quot;;
  document.getElementById(&quot;startPause&quot;).innerHTML = &quot;Start&quot;;
  window.clearInterval(interval);
  status = &quot;paused&quot;;
}
</code></pre>
","i am having trouble with following oop in javascript
how can i make this code more objectoriented and reusable
i tried reading up on oop concepts in js but couldnt figure a way around making this code be one any suggestions
ps this is the code for making a stopwatch
",1,"I am having trouble with following OOP in javascript.
How can I make this code more Object-oriented and reusable?
I tried reading up on OOP concepts in JS but couldn't figure a way around making this code be one. Any suggestions?
PS: This is the code for making a stopwatch
//Define variables to hold time values
let seconds = 0;
let minutes = 0;
let hours = 0;

//Define variable to  hold ""display"" value
let interval = null;

//Define variable to hold the clock status
let status = ""paused"";

//Clock function ( logic to determine when to increment next value, etc.)
function clock() {
  seconds++;

  //Logic to determine when to increment next value
  if (seconds >= 60) {
    seconds = 0;
    minutes++;

    if (minutes >= 60) {
      minutes = 0;
      hours++;
    }
  }

  //Display updated time values to user
  document.getElementById(""display"").innerHTML =
    //If seconds/minutes/hours are only one digit, add a leading 0 to the value
    `${hours ? (hours > 9 ? hours : `0${hours}`) : ""00""}:${minutes ? (minutes > 9 ? minutes : `0${minutes}`) : ""00""}:${seconds > 9 ? seconds : `0${seconds}`}`;
}

function startPause() {
  if (status === ""paused"") {
    //Start the stopwatch (by calling the setInterval() function)
    interval = window.setInterval(clock, 1000);
    document.getElementById(""startPause"").innerHTML = ""Pause"";
    status = ""started"";
  } else {
    window.clearInterval(interval);
    document.getElementById(""startPause"").innerHTML = ""Resume"";
    status = ""paused"";
  }
}

//Function to reset the stopwatch
function reset() {
  seconds = 0;
  minutes = 0;
  hours = 0;
  document.getElementById(""display"").innerHTML = ""00:00:00"";
  document.getElementById(""startPause"").innerHTML = ""Start"";
  window.clearInterval(interval);
  status = ""paused"";
}

","How does one refactor best this timer/stopwatch code-base towards a better code-reuse following the OOP paradigm and the DRY principle?
I am having trouble with following OOP in javascript.
How can I make this code more Object-oriented and reusable?
I tried reading up on OOP concepts in JS but couldn't figure a way around making this code be one. Any suggestions?
PS: This is the code for making a stopwatch
//Define variables to hold time values
let seconds = 0;
let minutes = 0;
let hours = 0;

//Define variable to  hold ""display"" value
let interval = null;

//Define variable to hold the clock status
let status = ""paused"";

//Clock function ( logic to determine when to increment next value, etc.)
function clock() {
  seconds++;

  //Logic to determine when to increment next value
  if (seconds >= 60) {
    seconds = 0;
    minutes++;

    if (minutes >= 60) {
      minutes = 0;
      hours++;
    }
  }

  //Display updated time values to user
  document.getElementById(""display"").innerHTML =
    //If seconds/minutes/hours are only one digit, add a leading 0 to the value
    `${hours ? (hours > 9 ? hours : `0${hours}`) : ""00""}:${minutes ? (minutes > 9 ? minutes : `0${minutes}`) : ""00""}:${seconds > 9 ? seconds : `0${seconds}`}`;
}

function startPause() {
  if (status === ""paused"") {
    //Start the stopwatch (by calling the setInterval() function)
    interval = window.setInterval(clock, 1000);
    document.getElementById(""startPause"").innerHTML = ""Pause"";
    status = ""started"";
  } else {
    window.clearInterval(interval);
    document.getElementById(""startPause"").innerHTML = ""Resume"";
    status = ""paused"";
  }
}

//Function to reset the stopwatch
function reset() {
  seconds = 0;
  minutes = 0;
  hours = 0;
  document.getElementById(""display"").innerHTML = ""00:00:00"";
  document.getElementById(""startPause"").innerHTML = ""Start"";
  window.clearInterval(interval);
  status = ""paused"";
}

",0,1,1,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
11509824,11509824,Performance penalty(?) of allocating too much memory,"<p>As part of a new variant on an algorithm I am using (leaning toward SMP use), I am considering sorting my <code>particle</code>s into <code>box</code>es, by having each <code>box</code> have a <code>particle* []</code>.  My only concern is that since the number of particles per box can vary, (average 10 has been known to go as high as 70 in very rare cases), I would need to massively overallocate the pointer arrays.  I was thinking probably on the order of 5-10% utilization.</p>

<p>Extreme cases would have on the order of a million of these, so we're considering half a GB of allocated memory.  It's not a problem in terms of the machines it will run on, but I'm wondering if there's a performance penalty (caches hate it or something) associated with striding memory that much. (Accessing the first 60 or so bytes out of 500, for example).
I do know to make sure my stride width doesn't end up being a cache-destroying multiple of 64 though...</p>

<p>I am OK with having it fail in VERY rare cases, as long as this plan buys me enough performance gain to be able to run more successful copies in the same amount of time.</p>

<p>If it's relevant, this code will mostly be run on Xeon E5620's, though that will change and I'd rather not do anything architecture-specific.</p>

<p>EDIT: This is comparing N bytes of packed data in contiguous memory to N bytes of data evenly strided over a much larger area of contiguous memory.</p>
","as part of a new variant on an algorithm i am using leaning toward smp use i am considering sorting my particles into boxes by having each box have a particle   my only concern is that since the number of particles per box can vary average 10 has been known to go as high as 70 in very rare cases i would need to massively overallocate the pointer arrays  i was thinking probably on the order of 510 utilization
extreme cases would have on the order of a million of these so were considering half a gb of allocated memory  its not a problem in terms of the machines it will run on but im wondering if theres a performance penalty caches hate it or something associated with striding memory that much accessing the first 60 or so bytes out of 500 for example
i do know to make sure my stride width doesnt end up being a cachedestroying multiple of 64 though
i am ok with having it fail in very rare cases as long as this plan buys me enough performance gain to be able to run more successful copies in the same amount of time
if its relevant this code will mostly be run on xeon e5620s though that will change and id rather not do anything architecturespecific
edit this is comparing n bytes of packed data in contiguous memory to n bytes of data evenly strided over a much larger area of contiguous memory
",1,"As part of a new variant on an algorithm I am using (leaning toward SMP use), I am considering sorting my particles into boxes, by having each box have a particle* [].  My only concern is that since the number of particles per box can vary, (average 10 has been known to go as high as 70 in very rare cases), I would need to massively overallocate the pointer arrays.  I was thinking probably on the order of 5-10% utilization.
Extreme cases would have on the order of a million of these, so we're considering half a GB of allocated memory.  It's not a problem in terms of the machines it will run on, but I'm wondering if there's a performance penalty (caches hate it or something) associated with striding memory that much. (Accessing the first 60 or so bytes out of 500, for example).
I do know to make sure my stride width doesn't end up being a cache-destroying multiple of 64 though...
I am OK with having it fail in VERY rare cases, as long as this plan buys me enough performance gain to be able to run more successful copies in the same amount of time.
If it's relevant, this code will mostly be run on Xeon E5620's, though that will change and I'd rather not do anything architecture-specific.
EDIT: This is comparing N bytes of packed data in contiguous memory to N bytes of data evenly strided over a much larger area of contiguous memory.
","Performance penalty(?) of allocating too much memory
As part of a new variant on an algorithm I am using (leaning toward SMP use), I am considering sorting my particles into boxes, by having each box have a particle* [].  My only concern is that since the number of particles per box can vary, (average 10 has been known to go as high as 70 in very rare cases), I would need to massively overallocate the pointer arrays.  I was thinking probably on the order of 5-10% utilization.
Extreme cases would have on the order of a million of these, so we're considering half a GB of allocated memory.  It's not a problem in terms of the machines it will run on, but I'm wondering if there's a performance penalty (caches hate it or something) associated with striding memory that much. (Accessing the first 60 or so bytes out of 500, for example).
I do know to make sure my stride width doesn't end up being a cache-destroying multiple of 64 though...
I am OK with having it fail in VERY rare cases, as long as this plan buys me enough performance gain to be able to run more successful copies in the same amount of time.
If it's relevant, this code will mostly be run on Xeon E5620's, though that will change and I'd rather not do anything architecture-specific.
EDIT: This is comparing N bytes of packed data in contiguous memory to N bytes of data evenly strided over a much larger area of contiguous memory.
",0,0,1,1,0,0,0,- Is a theoretical question,"Label: 1

Reasoning:

This question expresses technical debt because it highlights a design flaw and potential performance issue in the proposed algorithm. The user is concerned about the massive overallocation of memory (up to half a GB) due to the varying number of particles per box, which can lead to poor memory utilization (5-10%). This can result in performance penalties, such as cache misses, due to the striding memory access pattern.

The user is aware of the potential issue and is seeking advice on whether this approach would indeed lead to performance degradation. This indicates that the current design may not be optimal and requires improvement to achieve the desired performance gain.

Additionally, the user mentions that they are willing to accept failure in very rare cases, which suggests that the current design may not be robust or scalable. This is a characteristic of technical debt, where a short-term solution may not be sustainable or maintainable in the long run.

The question is not simply asking about a conceptual understanding of a specific topic, but rather seeking guidance on how to address a specific design flaw and potential performance issue in the algorithm. This is a clear indication of technical debt, as the current implementation may require future refactoring to improve performance and design.",Yes,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,1,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
55710878,55710878,How to parallelise nested FOR loops with OpenMP,"<p>I am attempting to iterate through a large PPM file containing pixel data by iterating through the file in cells (rows x columns). I want to optimise these two FOR loops by using the <code>omp_set_nested</code> function, but I am getting the following error: </p>

<p><code>termination test in OpenMP 'for' statement has improper form</code></p>

<p>For simplicity, I have taken away the functions relating to retrieving the data itself, and just have just included the <code>FOR</code> loops. </p>

<p><strong>Summary:</strong> How can I keep <code>i &lt; OUTER_LOOPS &amp;&amp; i &lt; OUTER_LOOPS</code> and <code>j &lt; INNER_LOOPS &amp;&amp; j &lt; INNER_LOOPS</code> within the <code>FOR</code> loop conditions without getting an error? </p>

<p><strong>UPDATE:</strong> I now realise the ridiculous mistake I was making by having <code>i &lt; OUTER_LOOPS &amp;&amp; i &lt; OUTER_LOOPS</code> which is far less efficient than simply <code>i &lt; OUTER_LOOPS</code> etc. </p>

<p>What I am trying to achieve: </p>

<pre><code>omp_set_nested(1);
int i, j;
#define OUTER_LOOPS width
#define INNER_LOOPS height

#pragma omp parallel for
    for (i = 0; i &lt; OUTER_LOOPS &amp;&amp; i &lt; OUTER_LOOPS; i += 4) {     //Issue probably here
                #pragma omp parallel for
                    for (j = 0; j &lt; INNER_LOOPS &amp;&amp; j &lt; INNER_LOOPS; j += 4) { //Issue probably here
                        printf(""Hello world\n"");
                    }

            }
</code></pre>
","i am attempting to iterate through a large ppm file containing pixel data by iterating through the file in cells rows x columns i want to optimise these two for loops by using the omp_set_nested function but i am getting the following error 
termination test in openmp for statement has improper form
for simplicity i have taken away the functions relating to retrieving the data itself and just have just included the for loops 
summary how can i keep i  outer_loops  i  outer_loops and j  inner_loops  j  inner_loops within the for loop conditions without getting an error 
update i now realise the ridiculous mistake i was making by having i  outer_loops  i  outer_loops which is far less efficient than simply i  outer_loops etc 
what i am trying to achieve 
",1,"I am attempting to iterate through a large PPM file containing pixel data by iterating through the file in cells (rows x columns). I want to optimise these two FOR loops by using the omp_set_nested function, but I am getting the following error: 
termination test in OpenMP 'for' statement has improper form
For simplicity, I have taken away the functions relating to retrieving the data itself, and just have just included the FOR loops. 
Summary: How can I keep i < OUTER_LOOPS && i < OUTER_LOOPS and j < INNER_LOOPS && j < INNER_LOOPS within the FOR loop conditions without getting an error? 
UPDATE: I now realise the ridiculous mistake I was making by having i < OUTER_LOOPS && i < OUTER_LOOPS which is far less efficient than simply i < OUTER_LOOPS etc. 
What I am trying to achieve: 
omp_set_nested(1);
int i, j;
#define OUTER_LOOPS width
#define INNER_LOOPS height

#pragma omp parallel for
    for (i = 0; i < OUTER_LOOPS && i < OUTER_LOOPS; i += 4) {     //Issue probably here
                #pragma omp parallel for
                    for (j = 0; j < INNER_LOOPS && j < INNER_LOOPS; j += 4) { //Issue probably here
                        printf(""Hello world\n"");
                    }

            }

","How to parallelise nested FOR loops with OpenMP
I am attempting to iterate through a large PPM file containing pixel data by iterating through the file in cells (rows x columns). I want to optimise these two FOR loops by using the omp_set_nested function, but I am getting the following error: 
termination test in OpenMP 'for' statement has improper form
For simplicity, I have taken away the functions relating to retrieving the data itself, and just have just included the FOR loops. 
Summary: How can I keep i < OUTER_LOOPS && i < OUTER_LOOPS and j < INNER_LOOPS && j < INNER_LOOPS within the FOR loop conditions without getting an error? 
UPDATE: I now realise the ridiculous mistake I was making by having i < OUTER_LOOPS && i < OUTER_LOOPS which is far less efficient than simply i < OUTER_LOOPS etc. 
What I am trying to achieve: 
omp_set_nested(1);
int i, j;
#define OUTER_LOOPS width
#define INNER_LOOPS height

#pragma omp parallel for
    for (i = 0; i < OUTER_LOOPS && i < OUTER_LOOPS; i += 4) {     //Issue probably here
                #pragma omp parallel for
                    for (j = 0; j < INNER_LOOPS && j < INNER_LOOPS; j += 4) { //Issue probably here
                        printf(""Hello world\n"");
                    }

            }

",1,1,0,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
72635434,72635434,Pytorch custom function fails to optimize,"<p>I have converted my problem into something more tangible.</p>
<p>I have the following equations;</p>
<p><a href=""https://i.stack.imgur.com/V1CjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V1CjA.png"" alt=""enter image description here"" /></a></p>
<p>Where x is the input variables, and I want to find the betas, as they are useful in another setting. I know that in this simple code example one could just estimate y and then apply OLS to get the betas, however for my real-life usage this is not applicable.
I use Pytorch to keep track of gradients and SGD to optimize my parameters. A simple FFN is used as an approximation for the functions f.</p>
<p>My code is written below (I'm relatively new to Pytorch and tensors)</p>
<pre><code>import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import copy
import torch.nn.functional as F
import matplotlib.pyplot as plt


# Make data
df = pd.date_range('2020-12-01', '2022-05-31', freq='H')
df = pd.DataFrame(df[0:len(df)-1], columns={'DateTime'})
np.random.seed(1)
A = 10

X = pd.DataFrame(np.random.uniform(0, 1, len(df)), columns={'X1'})
X['X2'] = np.random.normal(0, 1, len(df))
X['X3'] = np.random.normal(0, 1, len(df))


Y = pd.DataFrame()


for i in range(0, len(df)):
    Y[str(i)] = X['X1'][i] ** 2 + X['X1'][i] * X['X2'][i] * range(0, A) + X['X1'][i] **2 * X['X2'][i] * np.array(range(0, A))**2



Y = Y.T.reset_index(drop=True)
df = pd.concat([df, Y], axis=1)


# Setting
valid_start = '2021-07-01'
test_start = '2022-01-01'

Out = 2

X_array = X.copy()

Y_train = df[df['DateTime'].dt.date.astype(str) &lt; valid_start].drop(columns='DateTime')
Y_valid = df[df['DateTime'].dt.date.astype(str) &gt;= valid_start].drop(columns='DateTime')
Y_test = df[df['DateTime'].dt.date.astype(str) &gt;= valid_start].drop(columns='DateTime')
Y_test_date = pd.DataFrame(df[df['DateTime'].dt.date.astype(str) &gt;= valid_start]['DateTime'], columns={'DateTime'})


X_train = X_array[X_array.index.isin(Y_train.index)]
X_valid = X_array[X_array.index.isin(Y_valid.index)]
X_test = X_array[X_array.index.isin(Y_test.index)]

# Make cuda
torch.cuda.is_available()
torch.cuda.device_count()
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(device)

# make tensor
Y_train_tensor = torch.tensor(np.float32(Y_train)).to(device)
X_train_tensor = torch.tensor(np.float32(X_train)).to(device)

Y_valid_tensor = torch.tensor(np.float32(Y_valid)).to(device)
X_valid_tensor = torch.tensor(np.float32(X_valid)).to(device)

Y_test_tensor = torch.tensor(np.float32(Y_test)).to(device)
X_test_tensor = torch.tensor(np.float32(X_test)).to(device)

## Model


class Model(nn.Module):

    def __init__(self):
        # how many layers?
        super().__init__()
        self.fc1 = nn.Linear(X_train_tensor.shape[1], 4)
        self.out = nn.Linear(4, int(Out) + 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.out(x)

        return x

net = Model().to(device)
criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.001)
Range_tensor = torch.tensor(np.float32(range(0, A))).to(device)

# Early stopping
Network_weight = [copy.deepcopy(net.out.weight)]

Epoch = 100
patience = 10
Valloss_Array = []
trigger_times = 0
last_loss = 1000000000000000
Valloss_Array.append(last_loss)


for epoch in range(Epoch):  # loop over the dataset multiple times


    x = X_train_tensor
    y = Y_train_tensor

    # zero the parameter gradients
    optimizer.zero_grad()

    # forward + backward + optimize
    outputs = net(x)

    y_hat = []
    for e in range(0, len(y)):
        y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

        y_hat.append(y_hat_temp)


    y_hat = torch.stack(y_hat)

    loss = criterion(y_hat, y)

    loss.backward()
    optimizer.step()
    print('Running training loss is; ' + str(loss.item()))
    Network_weight.append([copy.deepcopy(net.fc1.weight), copy.deepcopy(net.out.weight)])

    ################## Validation ##################

    # break
    x = X_valid_tensor
    y = Y_valid_tensor

    outputs = net(x)

    y_hat = []
    for e in range(0, len(y)):

        y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

        y_hat.append(y_hat_temp)

    y_hat = torch.stack(y_hat)


    Valloss = criterion(y_hat, y)


    if Valloss.item() &gt;= min(Valloss_Array):
        trigger_times += 1

        if trigger_times &gt;= patience:
            print('Early stopping!')
            break

    else:
        trigger_times = 0
        Best = [copy.deepcopy(net.fc1.weight), copy.deepcopy(net.out.weight)]

    Valloss_Array.append(Valloss.item())


# With best weights
net.fc1.weight = Best[0]
net.out.weight = Best[1]



outputs = net(X_test_tensor)

y_hat = []
for e in range(0, len(Y_test_tensor)):

    y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

    y_hat.append(y_hat_temp)

y_hat = torch.stack(y_hat)

BestModelLoss = criterion(y_hat, Y_test_tensor)


Y_Prediction = pd.DataFrame(y_hat.cpu().detach().numpy(), index=Y_test.index, columns={0, 1, 2, 3, 4, 5, 6, 7, 8, 9})

Y_test_date['SE'] = ((Y_Prediction - Y_test) ** 2).sum(axis=1)

Y_Prediction['DateTime'] = Y_test_date['DateTime']
Y_test['DateTime'] = Y_test_date['DateTime']

### Plot

MinDate = Y_test_date.loc[Y_test_date['SE'] == Y_test_date['SE'].min(), 'DateTime']
YMinForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MinDate.values[0])].drop(columns='DateTime'))
YMin = np.array(Y_test[(Y_test['DateTime'] == MinDate.values[0])].drop(columns='DateTime'))

MaxDate = Y_test_date.loc[Y_test_date['SE'] == Y_test_date['SE'].max(), 'DateTime']
YMaxForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MaxDate.values[0])].drop(columns='DateTime'))
YMax = np.array(Y_test[(Y_test['DateTime'] == MaxDate.values[0])].drop(columns='DateTime'))

MedianValue = Y_test_date['SE'].iloc[(Y_test_date['SE']-Y_test_date['SE'].median()).abs().argsort()[:2]].values[0]

MedDate = Y_test_date.loc[Y_test_date['SE'] == MedianValue, 'DateTime']
YMedForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MedDate.values[0])].drop(columns='DateTime'))
YMed = np.array(Y_test[(Y_test['DateTime'] == MedDate.values[0])].drop(columns='DateTime'))


# plot

plt.plot(range(0, y_hat.shape[1]), YMinForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMin.reshape(-1), label='Actual')
plt.grid()
plt.title(&quot;Best performance&quot;)
plt.legend()
plt.show()

plt.plot(range(0, y_hat.shape[1]), YMaxForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMax.reshape(-1), label='Actual')
plt.grid()
plt.title(&quot;Poor performance&quot;)
plt.legend()
plt.show()

plt.plot(range(0, y_hat.shape[1]), YMedForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMed.reshape(-1), label='Actual')
plt.grid()
plt.title(&quot;Median performance&quot;)
plt.legend()
plt.show()
</code></pre>
","i have converted my problem into something more tangible
i have the following equations

where x is the input variables and i want to find the betas as they are useful in another setting i know that in this simple code example one could just estimate y and then apply ols to get the betas however for my reallife usage this is not applicable
i use pytorch to keep track of gradients and sgd to optimize my parameters a simple ffn is used as an approximation for the functions f
my code is written below im relatively new to pytorch and tensors
",1,"I have converted my problem into something more tangible.
I have the following equations;

Where x is the input variables, and I want to find the betas, as they are useful in another setting. I know that in this simple code example one could just estimate y and then apply OLS to get the betas, however for my real-life usage this is not applicable.
I use Pytorch to keep track of gradients and SGD to optimize my parameters. A simple FFN is used as an approximation for the functions f.
My code is written below (I'm relatively new to Pytorch and tensors)
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import copy
import torch.nn.functional as F
import matplotlib.pyplot as plt


# Make data
df = pd.date_range('2020-12-01', '2022-05-31', freq='H')
df = pd.DataFrame(df[0:len(df)-1], columns={'DateTime'})
np.random.seed(1)
A = 10

X = pd.DataFrame(np.random.uniform(0, 1, len(df)), columns={'X1'})
X['X2'] = np.random.normal(0, 1, len(df))
X['X3'] = np.random.normal(0, 1, len(df))


Y = pd.DataFrame()


for i in range(0, len(df)):
    Y[str(i)] = X['X1'][i] ** 2 + X['X1'][i] * X['X2'][i] * range(0, A) + X['X1'][i] **2 * X['X2'][i] * np.array(range(0, A))**2



Y = Y.T.reset_index(drop=True)
df = pd.concat([df, Y], axis=1)


# Setting
valid_start = '2021-07-01'
test_start = '2022-01-01'

Out = 2

X_array = X.copy()

Y_train = df[df['DateTime'].dt.date.astype(str) < valid_start].drop(columns='DateTime')
Y_valid = df[df['DateTime'].dt.date.astype(str) >= valid_start].drop(columns='DateTime')
Y_test = df[df['DateTime'].dt.date.astype(str) >= valid_start].drop(columns='DateTime')
Y_test_date = pd.DataFrame(df[df['DateTime'].dt.date.astype(str) >= valid_start]['DateTime'], columns={'DateTime'})


X_train = X_array[X_array.index.isin(Y_train.index)]
X_valid = X_array[X_array.index.isin(Y_valid.index)]
X_test = X_array[X_array.index.isin(Y_test.index)]

# Make cuda
torch.cuda.is_available()
torch.cuda.device_count()
device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
print(device)

# make tensor
Y_train_tensor = torch.tensor(np.float32(Y_train)).to(device)
X_train_tensor = torch.tensor(np.float32(X_train)).to(device)

Y_valid_tensor = torch.tensor(np.float32(Y_valid)).to(device)
X_valid_tensor = torch.tensor(np.float32(X_valid)).to(device)

Y_test_tensor = torch.tensor(np.float32(Y_test)).to(device)
X_test_tensor = torch.tensor(np.float32(X_test)).to(device)

## Model


class Model(nn.Module):

    def __init__(self):
        # how many layers?
        super().__init__()
        self.fc1 = nn.Linear(X_train_tensor.shape[1], 4)
        self.out = nn.Linear(4, int(Out) + 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.out(x)

        return x

net = Model().to(device)
criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.001)
Range_tensor = torch.tensor(np.float32(range(0, A))).to(device)

# Early stopping
Network_weight = [copy.deepcopy(net.out.weight)]

Epoch = 100
patience = 10
Valloss_Array = []
trigger_times = 0
last_loss = 1000000000000000
Valloss_Array.append(last_loss)


for epoch in range(Epoch):  # loop over the dataset multiple times


    x = X_train_tensor
    y = Y_train_tensor

    # zero the parameter gradients
    optimizer.zero_grad()

    # forward + backward + optimize
    outputs = net(x)

    y_hat = []
    for e in range(0, len(y)):
        y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

        y_hat.append(y_hat_temp)


    y_hat = torch.stack(y_hat)

    loss = criterion(y_hat, y)

    loss.backward()
    optimizer.step()
    print('Running training loss is; ' + str(loss.item()))
    Network_weight.append([copy.deepcopy(net.fc1.weight), copy.deepcopy(net.out.weight)])

    ################## Validation ##################

    # break
    x = X_valid_tensor
    y = Y_valid_tensor

    outputs = net(x)

    y_hat = []
    for e in range(0, len(y)):

        y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

        y_hat.append(y_hat_temp)

    y_hat = torch.stack(y_hat)


    Valloss = criterion(y_hat, y)


    if Valloss.item() >= min(Valloss_Array):
        trigger_times += 1

        if trigger_times >= patience:
            print('Early stopping!')
            break

    else:
        trigger_times = 0
        Best = [copy.deepcopy(net.fc1.weight), copy.deepcopy(net.out.weight)]

    Valloss_Array.append(Valloss.item())


# With best weights
net.fc1.weight = Best[0]
net.out.weight = Best[1]



outputs = net(X_test_tensor)

y_hat = []
for e in range(0, len(Y_test_tensor)):

    y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

    y_hat.append(y_hat_temp)

y_hat = torch.stack(y_hat)

BestModelLoss = criterion(y_hat, Y_test_tensor)


Y_Prediction = pd.DataFrame(y_hat.cpu().detach().numpy(), index=Y_test.index, columns={0, 1, 2, 3, 4, 5, 6, 7, 8, 9})

Y_test_date['SE'] = ((Y_Prediction - Y_test) ** 2).sum(axis=1)

Y_Prediction['DateTime'] = Y_test_date['DateTime']
Y_test['DateTime'] = Y_test_date['DateTime']

### Plot

MinDate = Y_test_date.loc[Y_test_date['SE'] == Y_test_date['SE'].min(), 'DateTime']
YMinForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MinDate.values[0])].drop(columns='DateTime'))
YMin = np.array(Y_test[(Y_test['DateTime'] == MinDate.values[0])].drop(columns='DateTime'))

MaxDate = Y_test_date.loc[Y_test_date['SE'] == Y_test_date['SE'].max(), 'DateTime']
YMaxForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MaxDate.values[0])].drop(columns='DateTime'))
YMax = np.array(Y_test[(Y_test['DateTime'] == MaxDate.values[0])].drop(columns='DateTime'))

MedianValue = Y_test_date['SE'].iloc[(Y_test_date['SE']-Y_test_date['SE'].median()).abs().argsort()[:2]].values[0]

MedDate = Y_test_date.loc[Y_test_date['SE'] == MedianValue, 'DateTime']
YMedForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MedDate.values[0])].drop(columns='DateTime'))
YMed = np.array(Y_test[(Y_test['DateTime'] == MedDate.values[0])].drop(columns='DateTime'))


# plot

plt.plot(range(0, y_hat.shape[1]), YMinForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMin.reshape(-1), label='Actual')
plt.grid()
plt.title(""Best performance"")
plt.legend()
plt.show()

plt.plot(range(0, y_hat.shape[1]), YMaxForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMax.reshape(-1), label='Actual')
plt.grid()
plt.title(""Poor performance"")
plt.legend()
plt.show()

plt.plot(range(0, y_hat.shape[1]), YMedForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMed.reshape(-1), label='Actual')
plt.grid()
plt.title(""Median performance"")
plt.legend()
plt.show()

","Pytorch custom function fails to optimize
I have converted my problem into something more tangible.
I have the following equations;

Where x is the input variables, and I want to find the betas, as they are useful in another setting. I know that in this simple code example one could just estimate y and then apply OLS to get the betas, however for my real-life usage this is not applicable.
I use Pytorch to keep track of gradients and SGD to optimize my parameters. A simple FFN is used as an approximation for the functions f.
My code is written below (I'm relatively new to Pytorch and tensors)
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import copy
import torch.nn.functional as F
import matplotlib.pyplot as plt


# Make data
df = pd.date_range('2020-12-01', '2022-05-31', freq='H')
df = pd.DataFrame(df[0:len(df)-1], columns={'DateTime'})
np.random.seed(1)
A = 10

X = pd.DataFrame(np.random.uniform(0, 1, len(df)), columns={'X1'})
X['X2'] = np.random.normal(0, 1, len(df))
X['X3'] = np.random.normal(0, 1, len(df))


Y = pd.DataFrame()


for i in range(0, len(df)):
    Y[str(i)] = X['X1'][i] ** 2 + X['X1'][i] * X['X2'][i] * range(0, A) + X['X1'][i] **2 * X['X2'][i] * np.array(range(0, A))**2



Y = Y.T.reset_index(drop=True)
df = pd.concat([df, Y], axis=1)


# Setting
valid_start = '2021-07-01'
test_start = '2022-01-01'

Out = 2

X_array = X.copy()

Y_train = df[df['DateTime'].dt.date.astype(str) < valid_start].drop(columns='DateTime')
Y_valid = df[df['DateTime'].dt.date.astype(str) >= valid_start].drop(columns='DateTime')
Y_test = df[df['DateTime'].dt.date.astype(str) >= valid_start].drop(columns='DateTime')
Y_test_date = pd.DataFrame(df[df['DateTime'].dt.date.astype(str) >= valid_start]['DateTime'], columns={'DateTime'})


X_train = X_array[X_array.index.isin(Y_train.index)]
X_valid = X_array[X_array.index.isin(Y_valid.index)]
X_test = X_array[X_array.index.isin(Y_test.index)]

# Make cuda
torch.cuda.is_available()
torch.cuda.device_count()
device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
print(device)

# make tensor
Y_train_tensor = torch.tensor(np.float32(Y_train)).to(device)
X_train_tensor = torch.tensor(np.float32(X_train)).to(device)

Y_valid_tensor = torch.tensor(np.float32(Y_valid)).to(device)
X_valid_tensor = torch.tensor(np.float32(X_valid)).to(device)

Y_test_tensor = torch.tensor(np.float32(Y_test)).to(device)
X_test_tensor = torch.tensor(np.float32(X_test)).to(device)

## Model


class Model(nn.Module):

    def __init__(self):
        # how many layers?
        super().__init__()
        self.fc1 = nn.Linear(X_train_tensor.shape[1], 4)
        self.out = nn.Linear(4, int(Out) + 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.out(x)

        return x

net = Model().to(device)
criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.001)
Range_tensor = torch.tensor(np.float32(range(0, A))).to(device)

# Early stopping
Network_weight = [copy.deepcopy(net.out.weight)]

Epoch = 100
patience = 10
Valloss_Array = []
trigger_times = 0
last_loss = 1000000000000000
Valloss_Array.append(last_loss)


for epoch in range(Epoch):  # loop over the dataset multiple times


    x = X_train_tensor
    y = Y_train_tensor

    # zero the parameter gradients
    optimizer.zero_grad()

    # forward + backward + optimize
    outputs = net(x)

    y_hat = []
    for e in range(0, len(y)):
        y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

        y_hat.append(y_hat_temp)


    y_hat = torch.stack(y_hat)

    loss = criterion(y_hat, y)

    loss.backward()
    optimizer.step()
    print('Running training loss is; ' + str(loss.item()))
    Network_weight.append([copy.deepcopy(net.fc1.weight), copy.deepcopy(net.out.weight)])

    ################## Validation ##################

    # break
    x = X_valid_tensor
    y = Y_valid_tensor

    outputs = net(x)

    y_hat = []
    for e in range(0, len(y)):

        y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

        y_hat.append(y_hat_temp)

    y_hat = torch.stack(y_hat)


    Valloss = criterion(y_hat, y)


    if Valloss.item() >= min(Valloss_Array):
        trigger_times += 1

        if trigger_times >= patience:
            print('Early stopping!')
            break

    else:
        trigger_times = 0
        Best = [copy.deepcopy(net.fc1.weight), copy.deepcopy(net.out.weight)]

    Valloss_Array.append(Valloss.item())


# With best weights
net.fc1.weight = Best[0]
net.out.weight = Best[1]



outputs = net(X_test_tensor)

y_hat = []
for e in range(0, len(Y_test_tensor)):

    y_hat_temp = outputs[e, 0] + outputs[e, 1] * Range_tensor + outputs[e, 2] * torch.pow(Range_tensor, 2)

    y_hat.append(y_hat_temp)

y_hat = torch.stack(y_hat)

BestModelLoss = criterion(y_hat, Y_test_tensor)


Y_Prediction = pd.DataFrame(y_hat.cpu().detach().numpy(), index=Y_test.index, columns={0, 1, 2, 3, 4, 5, 6, 7, 8, 9})

Y_test_date['SE'] = ((Y_Prediction - Y_test) ** 2).sum(axis=1)

Y_Prediction['DateTime'] = Y_test_date['DateTime']
Y_test['DateTime'] = Y_test_date['DateTime']

### Plot

MinDate = Y_test_date.loc[Y_test_date['SE'] == Y_test_date['SE'].min(), 'DateTime']
YMinForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MinDate.values[0])].drop(columns='DateTime'))
YMin = np.array(Y_test[(Y_test['DateTime'] == MinDate.values[0])].drop(columns='DateTime'))

MaxDate = Y_test_date.loc[Y_test_date['SE'] == Y_test_date['SE'].max(), 'DateTime']
YMaxForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MaxDate.values[0])].drop(columns='DateTime'))
YMax = np.array(Y_test[(Y_test['DateTime'] == MaxDate.values[0])].drop(columns='DateTime'))

MedianValue = Y_test_date['SE'].iloc[(Y_test_date['SE']-Y_test_date['SE'].median()).abs().argsort()[:2]].values[0]

MedDate = Y_test_date.loc[Y_test_date['SE'] == MedianValue, 'DateTime']
YMedForecast = np.array(Y_Prediction[(Y_Prediction['DateTime'] == MedDate.values[0])].drop(columns='DateTime'))
YMed = np.array(Y_test[(Y_test['DateTime'] == MedDate.values[0])].drop(columns='DateTime'))


# plot

plt.plot(range(0, y_hat.shape[1]), YMinForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMin.reshape(-1), label='Actual')
plt.grid()
plt.title(""Best performance"")
plt.legend()
plt.show()

plt.plot(range(0, y_hat.shape[1]), YMaxForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMax.reshape(-1), label='Actual')
plt.grid()
plt.title(""Poor performance"")
plt.legend()
plt.show()

plt.plot(range(0, y_hat.shape[1]), YMedForecast.reshape(-1), label='Forecast')
plt.plot(range(0, y_hat.shape[1]), YMed.reshape(-1), label='Actual')
plt.grid()
plt.title(""Median performance"")
plt.legend()
plt.show()

",1,1,0,0,0,0,0,- User just doesn't know how to approach a problem,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Very Mild,Very Mild,,Very Mild
10240360,10240360,"In C, will a literal string address always be greater than any allocated string address?","<p>I'm attempting to differentiate literal strings from allocated strings so I don't accidentally try to free() literal strings, which would cause a segfault.</p>

<p>The reason I might try to free() literal strings involve a strcat() wrapper that can be embedded thusly:  <code>strcatex(""My name "", strcatex(""is "", strMyName));</code></p>

<p>Regardless of the reason, consider:</p>

<pre><code>void* mallocex(int intBytes) 
{
    void* newPtr;

    newPtr = malloc(intBytes);

    if(newPtr == 0)
        return 0;
    else if(newPtr &gt; maxPointer)
        maxPointer = newPtr;

    return newPtr;
}
</code></pre>

<p>and</p>

<pre><code>int SafeFree(void **ptr)
{
    if((unsigned long) ptr &gt; (unsigned long) maxPointer) 
        return 0;
    else
        free(*ptr);

    return 1;
}
</code></pre>

<p>example use:</p>

<pre><code>char *newString;
newString = (char*) mallocex(12);

strcpy(newString, ""Example one"");

SafeFree(&amp;newString);
SafeFree(""Example two"");
</code></pre>

<p>Will this always work, regardless of how large my heap becomes?</p>
","im attempting to differentiate literal strings from allocated strings so i dont accidentally try to free literal strings which would cause a segfault
the reason i might try to free literal strings involve a strcat wrapper that can be embedded thusly  strcatexmy name  strcatexis  strmyname
regardless of the reason consider
and
example use
will this always work regardless of how large my heap becomes
",1,"I'm attempting to differentiate literal strings from allocated strings so I don't accidentally try to free() literal strings, which would cause a segfault.
The reason I might try to free() literal strings involve a strcat() wrapper that can be embedded thusly:  strcatex(""My name "", strcatex(""is "", strMyName));
Regardless of the reason, consider:
void* mallocex(int intBytes) 
{
    void* newPtr;

    newPtr = malloc(intBytes);

    if(newPtr == 0)
        return 0;
    else if(newPtr > maxPointer)
        maxPointer = newPtr;

    return newPtr;
}

and
int SafeFree(void **ptr)
{
    if((unsigned long) ptr > (unsigned long) maxPointer) 
        return 0;
    else
        free(*ptr);

    return 1;
}

example use:
char *newString;
newString = (char*) mallocex(12);

strcpy(newString, ""Example one"");

SafeFree(&newString);
SafeFree(""Example two"");

Will this always work, regardless of how large my heap becomes?
","In C, will a literal string address always be greater than any allocated string address?
I'm attempting to differentiate literal strings from allocated strings so I don't accidentally try to free() literal strings, which would cause a segfault.
The reason I might try to free() literal strings involve a strcat() wrapper that can be embedded thusly:  strcatex(""My name "", strcatex(""is "", strMyName));
Regardless of the reason, consider:
void* mallocex(int intBytes) 
{
    void* newPtr;

    newPtr = malloc(intBytes);

    if(newPtr == 0)
        return 0;
    else if(newPtr > maxPointer)
        maxPointer = newPtr;

    return newPtr;
}

and
int SafeFree(void **ptr)
{
    if((unsigned long) ptr > (unsigned long) maxPointer) 
        return 0;
    else
        free(*ptr);

    return 1;
}

example use:
char *newString;
newString = (char*) mallocex(12);

strcpy(newString, ""Example one"");

SafeFree(&newString);
SafeFree(""Example two"");

Will this always work, regardless of how large my heap becomes?
",0,1,1,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,1,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
69487497,69487497,How do I optimize the code so that I don't have to write a new number every time?,"<p>I have js code like this:</p>
<pre><code> lightGallery($(&quot;#web1&quot;)[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: false,
    }
});
lightGallery($(&quot;#web2&quot;)[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: true,
    }
});
lightGallery($(&quot;#web3&quot;)[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: true,
    }
});
</code></pre>
<p>How do I optimize the code so that I don't have to write a new number every time?</p>
","i have js code like this
how do i optimize the code so that i dont have to write a new number every time
",0,"I have js code like this:
 lightGallery($(""#web1"")[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: false,
    }
});
lightGallery($(""#web2"")[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: true,
    }
});
lightGallery($(""#web3"")[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: true,
    }
});

How do I optimize the code so that I don't have to write a new number every time?
","How do I optimize the code so that I don't have to write a new number every time?
I have js code like this:
 lightGallery($(""#web1"")[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: false,
    }
});
lightGallery($(""#web2"")[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: true,
    }
});
lightGallery($(""#web3"")[0], {
    selector: 'this',
    mobileSettings: {
        controls: false,
        showCloseIcon: true,
        download: true,
    }
});

How do I optimize the code so that I don't have to write a new number every time?
",0,1,0,1,0,0,1,- Suboptimal code ,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
19763877,19763877,how to analyze slow queries?,"<p>We have this db application which continually runs a complex query with different parameters.
It's a compiled stored procedure (C#) which calls 4 normal stored procedures who join 4 to 6 tables together and the C# code merging the results in between calls.</p>

<p>This has been running fine (fast) for 6 months until the database was restored. Then the performance changed from almost instant to 20+ seconds. Googling it up made it clear that update statistics had to be run on the tables and yes it worked, instant query again.</p>

<p>But then the problem came back after a month. Ok no problem, ""update statistics"" again. Fast again. Then it took a week to come back. slow query out of the blue. And then the problem kept coming back faster and faster and at this point updating statistics does not help at all anymore.</p>

<p>How to analyze this kind of problem? Is there something to know about compiled stored procedures and query plans in terms of performance?</p>
","we have this db application which continually runs a complex query with different parameters
its a compiled stored procedure c which calls 4 normal stored procedures who join 4 to 6 tables together and the c code merging the results in between calls
this has been running fine fast for 6 months until the database was restored then the performance changed from almost instant to 20 seconds googling it up made it clear that update statistics had to be run on the tables and yes it worked instant query again
but then the problem came back after a month ok no problem update statistics again fast again then it took a week to come back slow query out of the blue and then the problem kept coming back faster and faster and at this point updating statistics does not help at all anymore
how to analyze this kind of problem is there something to know about compiled stored procedures and query plans in terms of performance
",1,"We have this db application which continually runs a complex query with different parameters.
It's a compiled stored procedure (C#) which calls 4 normal stored procedures who join 4 to 6 tables together and the C# code merging the results in between calls.
This has been running fine (fast) for 6 months until the database was restored. Then the performance changed from almost instant to 20+ seconds. Googling it up made it clear that update statistics had to be run on the tables and yes it worked, instant query again.
But then the problem came back after a month. Ok no problem, ""update statistics"" again. Fast again. Then it took a week to come back. slow query out of the blue. And then the problem kept coming back faster and faster and at this point updating statistics does not help at all anymore.
How to analyze this kind of problem? Is there something to know about compiled stored procedures and query plans in terms of performance?
","how to analyze slow queries?
We have this db application which continually runs a complex query with different parameters.
It's a compiled stored procedure (C#) which calls 4 normal stored procedures who join 4 to 6 tables together and the C# code merging the results in between calls.
This has been running fine (fast) for 6 months until the database was restored. Then the performance changed from almost instant to 20+ seconds. Googling it up made it clear that update statistics had to be run on the tables and yes it worked, instant query again.
But then the problem came back after a month. Ok no problem, ""update statistics"" again. Fast again. Then it took a week to come back. slow query out of the blue. And then the problem kept coming back faster and faster and at this point updating statistics does not help at all anymore.
How to analyze this kind of problem? Is there something to know about compiled stored procedures and query plans in terms of performance?
",1,0,1,0,0,0,1,,,,1,1,Database Performance,Database Performance,,Database Performance,1,0,1,0,0,0,,,Infrastructure,Infrastructure,,Infrastructure,Moderate,Moderate,,Moderate
44172183,44172183,Can I run a background process in Django without starting a parallel process?,"<p>This is a very naive question, but I feel I don't understand something fundamental about asynchronous/background tasks in django and python. </p>

<p>I try to replicate a simple example provided by  django-background-tasks (<a href=""https://github.com/collinmutembei/django-background-tasks-example"" rel=""nofollow noreferrer"">https://github.com/collinmutembei/django-background-tasks-example</a>) in order to make django perform a background task 60 seconds later than it was actually run. But I guess the same is valid for any other background task manager such as Celery or Huey</p>

<p>The example is pretty simple - as soon as the user accesses the url, a simple function that prints a message is executed without blocking the main django process, 60 sec later</p>

<pre><code>  from background_task import background
  from logging import getLogger

  logger = getLogger(__name__)

  @background(schedule=60)
  def demo_task(message):
      logger.debug('demo_task. message={0}'.format(message))
</code></pre>

<p>The problem is that I really don't understand the basics. It doesn't run unless I start a separate (or detached) process <code>python manage.py process_tasks</code>. Should I always do it to make the background task work, or there is a way to do it without starting a parallel process?</p>

<p>If I should start a parallel process, can I do it from inside of django code. Something like:</p>

<pre><code>    import subprocess

    process = subprocess.Popen(['python', 'manage.py','process_tasks'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
</code></pre>
","this is a very naive question but i feel i dont understand something fundamental about asynchronousbackground tasks in django and python 
i try to replicate a simple example provided by  djangobackgroundtasks httpsgithubcomcollinmutembeidjangobackgroundtasksexample in order to make django perform a background task 60 seconds later than it was actually run but i guess the same is valid for any other background task manager such as celery or huey
the example is pretty simple  as soon as the user accesses the url a simple function that prints a message is executed without blocking the main django process 60 sec later
the problem is that i really dont understand the basics it doesnt run unless i start a separate or detached process python managepy process_tasks should i always do it to make the background task work or there is a way to do it without starting a parallel process
if i should start a parallel process can i do it from inside of django code something like
",1,"This is a very naive question, but I feel I don't understand something fundamental about asynchronous/background tasks in django and python. 
I try to replicate a simple example provided by  django-background-tasks (https://github.com/collinmutembei/django-background-tasks-example) in order to make django perform a background task 60 seconds later than it was actually run. But I guess the same is valid for any other background task manager such as Celery or Huey
The example is pretty simple - as soon as the user accesses the url, a simple function that prints a message is executed without blocking the main django process, 60 sec later
  from background_task import background
  from logging import getLogger

  logger = getLogger(__name__)

  @background(schedule=60)
  def demo_task(message):
      logger.debug('demo_task. message={0}'.format(message))

The problem is that I really don't understand the basics. It doesn't run unless I start a separate (or detached) process python manage.py process_tasks. Should I always do it to make the background task work, or there is a way to do it without starting a parallel process?
If I should start a parallel process, can I do it from inside of django code. Something like:
    import subprocess

    process = subprocess.Popen(['python', 'manage.py','process_tasks'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

","Can I run a background process in Django without starting a parallel process?
This is a very naive question, but I feel I don't understand something fundamental about asynchronous/background tasks in django and python. 
I try to replicate a simple example provided by  django-background-tasks (https://github.com/collinmutembei/django-background-tasks-example) in order to make django perform a background task 60 seconds later than it was actually run. But I guess the same is valid for any other background task manager such as Celery or Huey
The example is pretty simple - as soon as the user accesses the url, a simple function that prints a message is executed without blocking the main django process, 60 sec later
  from background_task import background
  from logging import getLogger

  logger = getLogger(__name__)

  @background(schedule=60)
  def demo_task(message):
      logger.debug('demo_task. message={0}'.format(message))

The problem is that I really don't understand the basics. It doesn't run unless I start a separate (or detached) process python manage.py process_tasks. Should I always do it to make the background task work, or there is a way to do it without starting a parallel process?
If I should start a parallel process, can I do it from inside of django code. Something like:
    import subprocess

    process = subprocess.Popen(['python', 'manage.py','process_tasks'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

",0,1,1,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,0,1,1,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
6128211,6128211,Releasing virtual memory reserved by C++ 'new' on Windows,"<p>I'm writing a 32-bit .NET program with a 2 stage input process:</p>

<ol>
<li><p>It uses native C++ via C++/CLI to parse an indefinite number files into corresponding SQLite databases (all with the same schema). The allocations by C++ 'new' will typically consume up to 1GB of the virtual address space (out of 2GB available; I'm aware of the 3GB extension but that'll just delay the issue).</p></li>
<li><p>It uses complex SQL queries (run from C#) to merge the databases into a single database. I set the cache_size to 1GB for the merged database so that the merging part has minimal page faults.</p></li>
</ol>

<p>My problem is that the cache in stage 2 does not re-use the 1GB of memory allocated by 'new' and properly released by 'delete' in stage 1. I know there's no leak because immediately after leaving stage 1, 'private bytes' drops down to a low amount like I'd expect. 'Virtual size' however remains at about the peak of what the C++ used.</p>

<p>This non-sharing between the C++ and SQLite cache causes me to run out of virtual address space. How can I resolve this, preferably in a fairly standards-compliant way? I really would like to release the memory allocated by C++ back to the OS.</p>
","im writing a 32bit net program with a 2 stage input process

it uses native c via ccli to parse an indefinite number files into corresponding sqlite databases all with the same schema the allocations by c new will typically consume up to 1gb of the virtual address space out of 2gb available im aware of the 3gb extension but thatll just delay the issue
it uses complex sql queries run from c to merge the databases into a single database i set the cache_size to 1gb for the merged database so that the merging part has minimal page faults

my problem is that the cache in stage 2 does not reuse the 1gb of memory allocated by new and properly released by delete in stage 1 i know theres no leak because immediately after leaving stage 1 private bytes drops down to a low amount like id expect virtual size however remains at about the peak of what the c used
this nonsharing between the c and sqlite cache causes me to run out of virtual address space how can i resolve this preferably in a fairly standardscompliant way i really would like to release the memory allocated by c back to the os
",1,"I'm writing a 32-bit .NET program with a 2 stage input process:

It uses native C++ via C++/CLI to parse an indefinite number files into corresponding SQLite databases (all with the same schema). The allocations by C++ 'new' will typically consume up to 1GB of the virtual address space (out of 2GB available; I'm aware of the 3GB extension but that'll just delay the issue).
It uses complex SQL queries (run from C#) to merge the databases into a single database. I set the cache_size to 1GB for the merged database so that the merging part has minimal page faults.

My problem is that the cache in stage 2 does not re-use the 1GB of memory allocated by 'new' and properly released by 'delete' in stage 1. I know there's no leak because immediately after leaving stage 1, 'private bytes' drops down to a low amount like I'd expect. 'Virtual size' however remains at about the peak of what the C++ used.
This non-sharing between the C++ and SQLite cache causes me to run out of virtual address space. How can I resolve this, preferably in a fairly standards-compliant way? I really would like to release the memory allocated by C++ back to the OS.
","Releasing virtual memory reserved by C++ 'new' on Windows
I'm writing a 32-bit .NET program with a 2 stage input process:

It uses native C++ via C++/CLI to parse an indefinite number files into corresponding SQLite databases (all with the same schema). The allocations by C++ 'new' will typically consume up to 1GB of the virtual address space (out of 2GB available; I'm aware of the 3GB extension but that'll just delay the issue).
It uses complex SQL queries (run from C#) to merge the databases into a single database. I set the cache_size to 1GB for the merged database so that the merging part has minimal page faults.

My problem is that the cache in stage 2 does not re-use the 1GB of memory allocated by 'new' and properly released by 'delete' in stage 1. I know there's no leak because immediately after leaving stage 1, 'private bytes' drops down to a low amount like I'd expect. 'Virtual size' however remains at about the peak of what the C++ used.
This non-sharing between the C++ and SQLite cache causes me to run out of virtual address space. How can I resolve this, preferably in a fairly standards-compliant way? I really would like to release the memory allocated by C++ back to the OS.
",1,1,0,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,1,0,0,0,0,,,Infrastructure,Infrastructure,,Infrastructure,Very Severe,Very Severe,,Very Severe
44807151,44807151,"Combining Events into one Handler, efficient?","<p>There are two parts to this question.</p>

<ol>
<li>Is it better to have one event-specific function per object or combine object events and if so,</li>
<li>Would using hash codes to define the sender object and compare to find out whats being updated be efficient?</li>
</ol>

<p>Example if code below.</p>

<p>My event function:</p>

<pre><code>private void GeneralEventHandler(object sender, EventArgs e){
        var senderHash = sender.GetHashCode();

        if (senderHash == tbDatabase.GetHashCode())
            Settings.DB.Default.Database = tbDatabase.Text;
        else if (senderHash == tbSchema.GetHashCode())
            Settings.DB.Default.Schema = tbSchema.Text;
}
</code></pre>

<p>My object definitions:</p>

<pre><code>    // tbDatabase
    / 
    this.tbDatabase.Anchor = ((System.Windows.Forms.AnchorStyles)((System.Windows.Forms.AnchorStyles.Left | System.Windows.Forms.AnchorStyles.Right)));
    this.tbDatabase.Location = new System.Drawing.Point(62, 3);
    this.tbDatabase.Name = ""tbDatabase"";
    this.tbDatabase.Size = new System.Drawing.Size(210, 20);
    this.tbDatabase.TabIndex = 0;
    this.tbDatabase.LostFocus += new System.EventHandler(this.GeneralEventHandler);
    // 
    // tbSchema
    // 
    this.tbSchema.Location = new System.Drawing.Point(55, 3);
    this.tbSchema.Name = ""tbSchema"";
    this.tbSchema.Size = new System.Drawing.Size(217, 20);
    this.tbSchema.TabIndex = 1;
    this.tbSchema.LostFocus += new System.EventHandler(this.GeneralEventHandler);
</code></pre>

<p>I'm mainly just using it to update user settings on the fly. I save the settings file on exit. </p>
","there are two parts to this question

is it better to have one eventspecific function per object or combine object events and if so
would using hash codes to define the sender object and compare to find out whats being updated be efficient

example if code below
my event function
my object definitions
im mainly just using it to update user settings on the fly i save the settings file on exit 
",1,"There are two parts to this question.

Is it better to have one event-specific function per object or combine object events and if so,
Would using hash codes to define the sender object and compare to find out whats being updated be efficient?

Example if code below.
My event function:
private void GeneralEventHandler(object sender, EventArgs e){
        var senderHash = sender.GetHashCode();

        if (senderHash == tbDatabase.GetHashCode())
            Settings.DB.Default.Database = tbDatabase.Text;
        else if (senderHash == tbSchema.GetHashCode())
            Settings.DB.Default.Schema = tbSchema.Text;
}

My object definitions:
    // tbDatabase
    / 
    this.tbDatabase.Anchor = ((System.Windows.Forms.AnchorStyles)((System.Windows.Forms.AnchorStyles.Left | System.Windows.Forms.AnchorStyles.Right)));
    this.tbDatabase.Location = new System.Drawing.Point(62, 3);
    this.tbDatabase.Name = ""tbDatabase"";
    this.tbDatabase.Size = new System.Drawing.Size(210, 20);
    this.tbDatabase.TabIndex = 0;
    this.tbDatabase.LostFocus += new System.EventHandler(this.GeneralEventHandler);
    // 
    // tbSchema
    // 
    this.tbSchema.Location = new System.Drawing.Point(55, 3);
    this.tbSchema.Name = ""tbSchema"";
    this.tbSchema.Size = new System.Drawing.Size(217, 20);
    this.tbSchema.TabIndex = 1;
    this.tbSchema.LostFocus += new System.EventHandler(this.GeneralEventHandler);

I'm mainly just using it to update user settings on the fly. I save the settings file on exit. 
","Combining Events into one Handler, efficient?
There are two parts to this question.

Is it better to have one event-specific function per object or combine object events and if so,
Would using hash codes to define the sender object and compare to find out whats being updated be efficient?

Example if code below.
My event function:
private void GeneralEventHandler(object sender, EventArgs e){
        var senderHash = sender.GetHashCode();

        if (senderHash == tbDatabase.GetHashCode())
            Settings.DB.Default.Database = tbDatabase.Text;
        else if (senderHash == tbSchema.GetHashCode())
            Settings.DB.Default.Schema = tbSchema.Text;
}

My object definitions:
    // tbDatabase
    / 
    this.tbDatabase.Anchor = ((System.Windows.Forms.AnchorStyles)((System.Windows.Forms.AnchorStyles.Left | System.Windows.Forms.AnchorStyles.Right)));
    this.tbDatabase.Location = new System.Drawing.Point(62, 3);
    this.tbDatabase.Name = ""tbDatabase"";
    this.tbDatabase.Size = new System.Drawing.Size(210, 20);
    this.tbDatabase.TabIndex = 0;
    this.tbDatabase.LostFocus += new System.EventHandler(this.GeneralEventHandler);
    // 
    // tbSchema
    // 
    this.tbSchema.Location = new System.Drawing.Point(55, 3);
    this.tbSchema.Name = ""tbSchema"";
    this.tbSchema.Size = new System.Drawing.Size(217, 20);
    this.tbSchema.TabIndex = 1;
    this.tbSchema.LostFocus += new System.EventHandler(this.GeneralEventHandler);

I'm mainly just using it to update user settings on the fly. I save the settings file on exit. 
",0,0,1,1,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,0,1,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
47372768,47372768,Python memory free,"<p>I want to free some memory, for example, I define a variable:</p>

<pre><code>b = 10
id(b)   # it shows 1935260400
</code></pre>

<p>Then I changed the value of b:</p>

<pre><code>b = 11
id(b)  # it shows 1935260432
</code></pre>

<p>After that, I changed b again:</p>

<pre><code>b = 10
id(b)  # it still shows 1935260400,why is it same with first time?
</code></pre>

<p>Here are the questions, b = 10 at  first time, then b = 11 at second time,why is the id(b) at third time same with the first time?  does the value 10 is still in the memory? How  to free the memory that value 10 takes up?</p>
","i want to free some memory for example i define a variable
then i changed the value of b
after that i changed b again
here are the questions b  10 at  first time then b  11 at second timewhy is the idb at third time same with the first time  does the value 10 is still in the memory how  to free the memory that value 10 takes up
",0,"I want to free some memory, for example, I define a variable:
b = 10
id(b)   # it shows 1935260400

Then I changed the value of b:
b = 11
id(b)  # it shows 1935260432

After that, I changed b again:
b = 10
id(b)  # it still shows 1935260400,why is it same with first time?

Here are the questions, b = 10 at  first time, then b = 11 at second time,why is the id(b) at third time same with the first time?  does the value 10 is still in the memory? How  to free the memory that value 10 takes up?
","Python memory free
I want to free some memory, for example, I define a variable:
b = 10
id(b)   # it shows 1935260400

Then I changed the value of b:
b = 11
id(b)  # it shows 1935260432

After that, I changed b again:
b = 10
id(b)  # it still shows 1935260400,why is it same with first time?

Here are the questions, b = 10 at  first time, then b = 11 at second time,why is the id(b) at third time same with the first time?  does the value 10 is still in the memory? How  to free the memory that value 10 takes up?
",0,0,1,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,0,0,0,,,Infrastructure,Infrastructure,,Infrastructure,Mild,Mild,,Mild
18971873,18971873,How valuable is it to optimize use of function closures in Javascript?,"<p>If have a callback that is activated repeatedly such as the following...</p>

<pre class=""lang-js prettyprint-override""><code>Template.foo.rendered = function() {
    $(this.firstNode).droppable({
        // other arguments
        drop: function() {
            // some really long function that doesn't access anything in the closure
        }
    });
}
</code></pre>

<p>Should I optimize it to the following?</p>

<pre class=""lang-js prettyprint-override""><code>dropFunction = function() {
    // some really long function that doesn't access anything in the closure
} 

Template.foo.rendered = function() {
    $(this.firstNode).droppable({
        // other arguments
        drop: dropFunction
    });
}
</code></pre>

<p>In this case, the <code>rendered</code> callback is a Meteor construct which runs asynchronously over a DOM node with template <code>foo</code> whenever they are constructed; there can be quite a few of them. Does it help to declare the function somewhere in a global closure, to save the Javascript engine the hassle of keeping track of an extra local closure, or does it not matter?</p>
","if have a callback that is activated repeatedly such as the following
templatefoorendered  function 
    thisfirstnodedroppable
         other arguments
        drop function 
             some really long function that doesnt access anything in the closure
        
    


should i optimize it to the following
dropfunction  function 
     some really long function that doesnt access anything in the closure
 

templatefoorendered  function 
    thisfirstnodedroppable
         other arguments
        drop dropfunction
    


in this case the rendered callback is a meteor construct which runs asynchronously over a dom node with template foo whenever they are constructed there can be quite a few of them does it help to declare the function somewhere in a global closure to save the javascript engine the hassle of keeping track of an extra local closure or does it not matter
",1,"If have a callback that is activated repeatedly such as the following...
Template.foo.rendered = function() {
    $(this.firstNode).droppable({
        // other arguments
        drop: function() {
            // some really long function that doesn't access anything in the closure
        }
    });
}

Should I optimize it to the following?
dropFunction = function() {
    // some really long function that doesn't access anything in the closure
} 

Template.foo.rendered = function() {
    $(this.firstNode).droppable({
        // other arguments
        drop: dropFunction
    });
}

In this case, the rendered callback is a Meteor construct which runs asynchronously over a DOM node with template foo whenever they are constructed; there can be quite a few of them. Does it help to declare the function somewhere in a global closure, to save the Javascript engine the hassle of keeping track of an extra local closure, or does it not matter?
","How valuable is it to optimize use of function closures in Javascript?
If have a callback that is activated repeatedly such as the following...
Template.foo.rendered = function() {
    $(this.firstNode).droppable({
        // other arguments
        drop: function() {
            // some really long function that doesn't access anything in the closure
        }
    });
}

Should I optimize it to the following?
dropFunction = function() {
    // some really long function that doesn't access anything in the closure
} 

Template.foo.rendered = function() {
    $(this.firstNode).droppable({
        // other arguments
        drop: dropFunction
    });
}

In this case, the rendered callback is a Meteor construct which runs asynchronously over a DOM node with template foo whenever they are constructed; there can be quite a few of them. Does it help to declare the function somewhere in a global closure, to save the Javascript engine the hassle of keeping track of an extra local closure, or does it not matter?
",0,0,1,1,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
69184772,69184772,_alloca and std::vector of const char*,"<p>I did several searches on the _alloca function and it is generally not recommended.
I will have a dynamically updated array of strings on which I will have to iterate often.
I would like to have each string allocated on the stack to reduce the miss caches as much as possible.</p>
<p>Using _alloca I could create a char* on the stack and put it in the vector.
My char will be in a vector which will not impact my stack and I know that the strings will never be big enough to make a stackoverflow when I allocate them on the stack.</p>
<p>Is it a bad thing to use it in this case?</p>
<p>Does the program do what I want it to do?</p>
<pre class=""lang-cpp prettyprint-override""><code>#include &lt;vector&gt;
#include &lt;cstring&gt;

#ifdef  __GNUC__
# define _alloca(size) __builtin_alloca (size)
#endif /* GCC.  */

std::vector&lt;const char*&gt; vec;

void add(const char* message, int size) {
  char* c = (char*) _alloca (size * sizeof(char*));
  std::memcpy(c, message, sizeof(message));
  vec.push_back(c);
}

int main() {
  const char* c = &quot;OK&quot;;
  for (int i = 0; i &lt; 10; ++i) {
    add(c, 2);
  }
}
</code></pre>
","i did several searches on the _alloca function and it is generally not recommended
i will have a dynamically updated array of strings on which i will have to iterate often
i would like to have each string allocated on the stack to reduce the miss caches as much as possible
using _alloca i could create a char on the stack and put it in the vector
my char will be in a vector which will not impact my stack and i know that the strings will never be big enough to make a stackoverflow when i allocate them on the stack
is it a bad thing to use it in this case
does the program do what i want it to do
include vector
include cstring

ifdef  __gnuc__
 define _allocasize __builtin_alloca size
endif  gcc  

stdvectorconst char vec

void addconst char message int size 
  char c  char _alloca size  sizeofchar
  stdmemcpyc message sizeofmessage
  vecpush_backc


int main 
  const char c  ok
  for int i  0 i  10 i 
    addc 2
  


",1,"I did several searches on the _alloca function and it is generally not recommended.
I will have a dynamically updated array of strings on which I will have to iterate often.
I would like to have each string allocated on the stack to reduce the miss caches as much as possible.
Using _alloca I could create a char* on the stack and put it in the vector.
My char will be in a vector which will not impact my stack and I know that the strings will never be big enough to make a stackoverflow when I allocate them on the stack.
Is it a bad thing to use it in this case?
Does the program do what I want it to do?
#include <vector>
#include <cstring>

#ifdef  __GNUC__
# define _alloca(size) __builtin_alloca (size)
#endif /* GCC.  */

std::vector<const char*> vec;

void add(const char* message, int size) {
  char* c = (char*) _alloca (size * sizeof(char*));
  std::memcpy(c, message, sizeof(message));
  vec.push_back(c);
}

int main() {
  const char* c = ""OK"";
  for (int i = 0; i < 10; ++i) {
    add(c, 2);
  }
}

","_alloca and std::vector of const char*
I did several searches on the _alloca function and it is generally not recommended.
I will have a dynamically updated array of strings on which I will have to iterate often.
I would like to have each string allocated on the stack to reduce the miss caches as much as possible.
Using _alloca I could create a char* on the stack and put it in the vector.
My char will be in a vector which will not impact my stack and I know that the strings will never be big enough to make a stackoverflow when I allocate them on the stack.
Is it a bad thing to use it in this case?
Does the program do what I want it to do?
#include <vector>
#include <cstring>

#ifdef  __GNUC__
# define _alloca(size) __builtin_alloca (size)
#endif /* GCC.  */

std::vector<const char*> vec;

void add(const char* message, int size) {
  char* c = (char*) _alloca (size * sizeof(char*));
  std::memcpy(c, message, sizeof(message));
  vec.push_back(c);
}

int main() {
  const char* c = ""OK"";
  for (int i = 0; i < 10; ++i) {
    add(c, 2);
  }
}

",0,1,1,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,1,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
18357838,18357838,Jackson management for very large JSONs in Java,"<p>So what I have is a very large JSON file (600MB), and I have a few modules in my Java project that import each item in the JSON file and write it's properties to  several tables in a huge database. It works perfectly for relatively small JSON files which I had to deal with until now but this one  makes the JVM throw OutOfMemoryException.
 The library I use is Jackson and I load each item in the JSON file in a Java list in the following function:</p>

<pre class=""lang-java prettyprint-override""><code>  public List&lt;Products&gt; listOfProducts(String JSONString)
  throws JsonParseException, JsonMappingException, IOException {
byte[] latin1 = JSONString.getBytes(""ISO-8859-1"");
String fromUtf8Bytes = new String(latin1);
ObjectMapper mapper = new ObjectMapper();
Products[] productsArray = mapper.readValue(fromUtf8Bytes,
    Products[].class);

List&lt;Products&gt; productsList= Arrays.asList(productsArray);

return productsList;
</code></pre>

<p>}</p>

<p>Then the returned list is passed on to my importing functions, but before that happens the out of memory exception occurs.
Any ideas on how I can process such a large file without breaking the original JSON into several smaller ones?</p>
","so what i have is a very large json file 600mb and i have a few modules in my java project that import each item in the json file and write its properties to  several tables in a huge database it works perfectly for relatively small json files which i had to deal with until now but this one  makes the jvm throw outofmemoryexception
 the library i use is jackson and i load each item in the json file in a java list in the following function
  public listproducts listofproductsstring jsonstring
  throws jsonparseexception jsonmappingexception ioexception 
byte latin1  jsonstringgetbytesiso88591
string fromutf8bytes  new stringlatin1
objectmapper mapper  new objectmapper
products productsarray  mapperreadvaluefromutf8bytes
    productsclass

listproducts productslist arraysaslistproductsarray

return productslist


then the returned list is passed on to my importing functions but before that happens the out of memory exception occurs
any ideas on how i can process such a large file without breaking the original json into several smaller ones
",1,"So what I have is a very large JSON file (600MB), and I have a few modules in my Java project that import each item in the JSON file and write it's properties to  several tables in a huge database. It works perfectly for relatively small JSON files which I had to deal with until now but this one  makes the JVM throw OutOfMemoryException.
 The library I use is Jackson and I load each item in the JSON file in a Java list in the following function:
  public List<Products> listOfProducts(String JSONString)
  throws JsonParseException, JsonMappingException, IOException {
byte[] latin1 = JSONString.getBytes(""ISO-8859-1"");
String fromUtf8Bytes = new String(latin1);
ObjectMapper mapper = new ObjectMapper();
Products[] productsArray = mapper.readValue(fromUtf8Bytes,
    Products[].class);

List<Products> productsList= Arrays.asList(productsArray);

return productsList;

}
Then the returned list is passed on to my importing functions, but before that happens the out of memory exception occurs.
Any ideas on how I can process such a large file without breaking the original JSON into several smaller ones?
","Jackson management for very large JSONs in Java
So what I have is a very large JSON file (600MB), and I have a few modules in my Java project that import each item in the JSON file and write it's properties to  several tables in a huge database. It works perfectly for relatively small JSON files which I had to deal with until now but this one  makes the JVM throw OutOfMemoryException.
 The library I use is Jackson and I load each item in the JSON file in a Java list in the following function:
  public List<Products> listOfProducts(String JSONString)
  throws JsonParseException, JsonMappingException, IOException {
byte[] latin1 = JSONString.getBytes(""ISO-8859-1"");
String fromUtf8Bytes = new String(latin1);
ObjectMapper mapper = new ObjectMapper();
Products[] productsArray = mapper.readValue(fromUtf8Bytes,
    Products[].class);

List<Products> productsList= Arrays.asList(productsArray);

return productsList;

}
Then the returned list is passed on to my importing functions, but before that happens the out of memory exception occurs.
Any ideas on how I can process such a large file without breaking the original JSON into several smaller ones?
",1,1,0,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,1,0,0,0,0,,,Infrastructure,Design,User’s issue is more so focused on the scalability issue as the code works fine for smaller files,Infrastructure,Moderate,Moderate,,Moderate
36099152,36099152,How to iterate over lists from the middle out,"<p>I would like to iterate over all lists/tuples of length n with elements from -s...s.  Currently I do this with:</p>

<pre><code>for k in itertools.product(range(-s,s+1), repeat = n):
    #process k and maybe print out the result
</code></pre>

<p>However this not useful for me as there are a huge number of such tuples and my code may never terminate.  I would really like to start with the most interesting ones first. In this case the order I would like for the iteration is:</p>

<ol>
<li>All tuples that contain only 0 (there is only one)</li>
<li>All tuples that contain only 0, 1 and -1 excluding those tuples we have already seen.</li>
<li>All tuples that contain only 0, 1,-1, 2 and-2 excluding those tuples we have already seen.</li>
<li>And so on...</li>
</ol>

<p>How can one do this?</p>
","i would like to iterate over all liststuples of length n with elements from ss  currently i do this with
however this not useful for me as there are a huge number of such tuples and my code may never terminate  i would really like to start with the most interesting ones first in this case the order i would like for the iteration is

all tuples that contain only 0 there is only one
all tuples that contain only 0 1 and 1 excluding those tuples we have already seen
all tuples that contain only 0 11 2 and2 excluding those tuples we have already seen
and so on

how can one do this
",1,"I would like to iterate over all lists/tuples of length n with elements from -s...s.  Currently I do this with:
for k in itertools.product(range(-s,s+1), repeat = n):
    #process k and maybe print out the result

However this not useful for me as there are a huge number of such tuples and my code may never terminate.  I would really like to start with the most interesting ones first. In this case the order I would like for the iteration is:

All tuples that contain only 0 (there is only one)
All tuples that contain only 0, 1 and -1 excluding those tuples we have already seen.
All tuples that contain only 0, 1,-1, 2 and-2 excluding those tuples we have already seen.
And so on...

How can one do this?
","How to iterate over lists from the middle out
I would like to iterate over all lists/tuples of length n with elements from -s...s.  Currently I do this with:
for k in itertools.product(range(-s,s+1), repeat = n):
    #process k and maybe print out the result

However this not useful for me as there are a huge number of such tuples and my code may never terminate.  I would really like to start with the most interesting ones first. In this case the order I would like for the iteration is:

All tuples that contain only 0 (there is only one)
All tuples that contain only 0, 1 and -1 excluding those tuples we have already seen.
All tuples that contain only 0, 1,-1, 2 and-2 excluding those tuples we have already seen.
And so on...

How can one do this?
",0,1,0,1,0,0,0,- User just doesn't know how to approach a problem,"Label: 1

Reasoning: This question expresses technical debt because the current implementation is not efficient and may never terminate for large values of `n` and `s`. The user acknowledges that the number of tuples is huge and their code may not complete, indicating a performance issue. The existing approach using `itertools.product` is not scalable and needs to be improved.

The question also implies a design flaw, as the user wants to iterate over the tuples in a specific order, which is not possible with the current implementation. The requirement to start with the ""most interesting"" tuples first suggests that the existing code is not meeting the user's needs, and a better approach is required.

Furthermore, the user's description of the desired iteration order implies a need for a more sophisticated and efficient algorithm to generate the tuples, rather than relying on the brute-force approach of `itertools.product`. The fact that the user is looking for a way to iterate over the tuples in a specific order also suggests that the current implementation is not maintainable or flexible enough to accommodate the required changes.

Overall, this question expresses technical debt due to performance issues, design flaws, and the need for a more efficient and scalable solution.",Yes,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,1,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
46597511,46597511,[Searching in Text File][Notepad++ plugin in C++],"<p>I created a notepad++ plugin in C++.It searches between a keyword in a Log file and copy the entire data between this keyword.(This keyword occurs exactly two times in the file or does not occur at all).
 Here is my code </p>

<pre><code>        std::ifstream in(curr_file_path);
        std::string search(""TEXT TO SEARCH"");
  while (std::getline(in, line))
     {
        r_val = line.find(search);
        if (r_val != -1)
        {
            f_match = r_val;
            r_val = -1;
            while (r_val == -1)
            {
                std::getline(in, line);
                r_val = line.find(search);
                for (int i = 0; i &lt; line.size(); i++)
                {
                    pDestText[textLen++] = line[i];

                }
            }
            pDestText[textLen - 27] = '\0';
            break;
        }
    }
 }
</code></pre>

<p>search is the string to be searched.Input file is in. My query is as the log file is very large ,it is taking time around 5 seconds.Can i optimise This
 .One optimization which I am thinking is skip first 50000 lines of code as I am sure that the text I am looking wont come in first 50000 lines.But is there any better way of optimization.If not ,can anyone tell how to skip first  50000 lines of code and throw a error if it has less than 50000 lines.</p>
","i created a notepad plugin in cit searches between a keyword in a log file and copy the entire data between this keywordthis keyword occurs exactly two times in the file or does not occur at all
 here is my code 
search is the string to be searchedinput file is in my query is as the log file is very large it is taking time around 5 secondscan i optimise this
 one optimization which i am thinking is skip first 50000 lines of code as i am sure that the text i am looking wont come in first 50000 linesbut is there any better way of optimizationif not can anyone tell how to skip first  50000 lines of code and throw a error if it has less than 50000 lines
",0,"I created a notepad++ plugin in C++.It searches between a keyword in a Log file and copy the entire data between this keyword.(This keyword occurs exactly two times in the file or does not occur at all).
 Here is my code 
        std::ifstream in(curr_file_path);
        std::string search(""TEXT TO SEARCH"");
  while (std::getline(in, line))
     {
        r_val = line.find(search);
        if (r_val != -1)
        {
            f_match = r_val;
            r_val = -1;
            while (r_val == -1)
            {
                std::getline(in, line);
                r_val = line.find(search);
                for (int i = 0; i < line.size(); i++)
                {
                    pDestText[textLen++] = line[i];

                }
            }
            pDestText[textLen - 27] = '\0';
            break;
        }
    }
 }

search is the string to be searched.Input file is in. My query is as the log file is very large ,it is taking time around 5 seconds.Can i optimise This
 .One optimization which I am thinking is skip first 50000 lines of code as I am sure that the text I am looking wont come in first 50000 lines.But is there any better way of optimization.If not ,can anyone tell how to skip first  50000 lines of code and throw a error if it has less than 50000 lines.
","[Searching in Text File][Notepad++ plugin in C++]
I created a notepad++ plugin in C++.It searches between a keyword in a Log file and copy the entire data between this keyword.(This keyword occurs exactly two times in the file or does not occur at all).
 Here is my code 
        std::ifstream in(curr_file_path);
        std::string search(""TEXT TO SEARCH"");
  while (std::getline(in, line))
     {
        r_val = line.find(search);
        if (r_val != -1)
        {
            f_match = r_val;
            r_val = -1;
            while (r_val == -1)
            {
                std::getline(in, line);
                r_val = line.find(search);
                for (int i = 0; i < line.size(); i++)
                {
                    pDestText[textLen++] = line[i];

                }
            }
            pDestText[textLen - 27] = '\0';
            break;
        }
    }
 }

search is the string to be searched.Input file is in. My query is as the log file is very large ,it is taking time around 5 seconds.Can i optimise This
 .One optimization which I am thinking is skip first 50000 lines of code as I am sure that the text I am looking wont come in first 50000 lines.But is there any better way of optimization.If not ,can anyone tell how to skip first  50000 lines of code and throw a error if it has less than 50000 lines.
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
414801,414801,Any way to further optimize Java reflective method invocation?,"<p>I am wondering if there are any additional optimizations I can implement to improve the speed of reflective invocations in Java. Not that the performance is prohibitive, but I get the willies when thinking about some piece of code in a library I am writing being implemented in a tight loop somewhere.</p>

<p>Consider a utility method to invoke reflectively:</p>

<pre><code>public static Object invoke(Object targetObject, String methodName, Object[] arguments, Class&lt;?&gt;[] signature)
</code></pre>

<p>The basic operation is</p>

<pre><code>return method.invoke(targetObject, arguments);
</code></pre>

<p>As a performance optimization, I cache the method using a hash of the target object's class, method name and signature (the code of which might use some improvement) but beyond that, is there anything else I can do ? I have heard references to some early implementations of <strong>InvokeDynamic</strong> that sound promising, but I just assumed that they were probably not applicable yet, and I discounted my own byte code manipulation as I would like to keep the utility simple (but fast).</p>

<p>Cheers.</p>
","i am wondering if there are any additional optimizations i can implement to improve the speed of reflective invocations in java not that the performance is prohibitive but i get the willies when thinking about some piece of code in a library i am writing being implemented in a tight loop somewhere
consider a utility method to invoke reflectively
the basic operation is
as a performance optimization i cache the method using a hash of the target objects class method name and signature the code of which might use some improvement but beyond that is there anything else i can do  i have heard references to some early implementations of invokedynamic that sound promising but i just assumed that they were probably not applicable yet and i discounted my own byte code manipulation as i would like to keep the utility simple but fast
cheers
",1,"I am wondering if there are any additional optimizations I can implement to improve the speed of reflective invocations in Java. Not that the performance is prohibitive, but I get the willies when thinking about some piece of code in a library I am writing being implemented in a tight loop somewhere.
Consider a utility method to invoke reflectively:
public static Object invoke(Object targetObject, String methodName, Object[] arguments, Class<?>[] signature)

The basic operation is
return method.invoke(targetObject, arguments);

As a performance optimization, I cache the method using a hash of the target object's class, method name and signature (the code of which might use some improvement) but beyond that, is there anything else I can do ? I have heard references to some early implementations of InvokeDynamic that sound promising, but I just assumed that they were probably not applicable yet, and I discounted my own byte code manipulation as I would like to keep the utility simple (but fast).
Cheers.
","Any way to further optimize Java reflective method invocation?
I am wondering if there are any additional optimizations I can implement to improve the speed of reflective invocations in Java. Not that the performance is prohibitive, but I get the willies when thinking about some piece of code in a library I am writing being implemented in a tight loop somewhere.
Consider a utility method to invoke reflectively:
public static Object invoke(Object targetObject, String methodName, Object[] arguments, Class<?>[] signature)

The basic operation is
return method.invoke(targetObject, arguments);

As a performance optimization, I cache the method using a hash of the target object's class, method name and signature (the code of which might use some improvement) but beyond that, is there anything else I can do ? I have heard references to some early implementations of InvokeDynamic that sound promising, but I just assumed that they were probably not applicable yet, and I discounted my own byte code manipulation as I would like to keep the utility simple (but fast).
Cheers.
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
32647953,32647953,python multiprocessing pool abort if value,"<p>I am developping a script where I create objects at random but I don't want duplicates. They are stored and each time I create a new one I check it against the already existing ones. As I want to do it for large numbers of objects, I am now trying to parallelize it but so far with no success. I tried some solutions found on the web (mainly here in fact) but still not working.</p>

<p>The idea is that I launch a pool and map my function to it. When a process finds a match, it sets a value to 1. This value is readable by all processes, they can write it using a lock and I need it at the end to return. Thus I made a <code>Lock</code> and a <code>Value</code>, so that all processes can read the value (hence the <code>lock=False</code>) and check if a match has been found in another process. Then I tried something different with an <code>Event</code> and checking if it is set but this is still not working... I then tried to raise a special <code>Exception</code> but still no success in making the code to a success.</p>

<p>Please, I prefer to program OOP, so I will avoid until my last resource to define a <code>global</code> variable as I regard them as unsure (personal opinion).</p>

<p>Here is a MWE, I replaced my complicated objects by <code>int</code> and my stored objects by a <code>range(10000)</code> to help you understand.</p>

<pre><code>#!/usr/bin/env python3

import multiprocessing as muproc


def ParallelCheck(me):
    print("" Testing {}..."".format(me))
    #manager = muproc.Manager()
    #lock = manager.Lock()
    lock = muproc.Lock()
    back = muproc.Value(""i"", 0, lock=False)
    ParChild = ParallelChild(me, lock, back)
    with muproc.Pool() as pool:
        try:
            pool.map(ParChild.run, range(10000))
        except AbortPool:
            pool.terminate()
            print(""pool"")
    return back.value


def Computation(me, neighbour):
    return me == neighbour



class ParallelChild(object):
    def __init__(self, me, lock, back):
        self.abort = muproc.Event()
        self.lock = lock
        self.me = me
        self.back = back

    def run(self, neighbour):
        print(""run"")
        if self.abort.is_set():
            print(""Aborting"")
            pass
        else:
            if Computation(self.me, neighbour):
                self.lock.acquire()
                self.abort.set()
                self.back.value = 1
                print(""GOTCHA"")
                self.lock.release()
                raise AbortPool
            else:
                print(""..."")


class AbortPool(Exception):
    #pass
    def __init__(self):
        ## Just to check
        print(""AbortPool raised!"")



if __name__ == ""__main__"":
    values = [12000, 13, 7]
    for v in values:
        print(""value={} match={}"".format(v, ParallelCheck(v)))
</code></pre>

<p>Now it yields a <code>RunTimeError</code>:</p>

<pre><code>me@stigepc4$ python3 mwe.py 
 Testing 12000...
Traceback (most recent call last):
  File ""mwe.py"", line 63, in &lt;module&gt;
    print(""value={} match={}"".format(v, ParallelCheck(v)))
  File ""mwe.py"", line 16, in ParallelCheck
    pool.map(ParChild.run, range(10000))
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 260, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 599, in get
    raise self._value
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 383, in _handle_tasks
    put(task)
  File ""/usr/lib/python3.4/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(ForkingPickler.dumps(obj))
  File ""/usr/lib/python3.4/multiprocessing/reduction.py"", line 50, in dumps
    cls(buf, protocol).dump(obj)
  File ""/usr/lib/python3.4/multiprocessing/sharedctypes.py"", line 128, in reduce_ctype
    assert_spawning(obj)
  File ""/usr/lib/python3.4/multiprocessing/context.py"", line 347, in assert_spawning
    ' through inheritance' % type(obj).__name__
RuntimeError: c_int objects should only be shared between processes through inheritance
</code></pre>

<p>I guess it has something to do with the <code>Lock</code> (though the commented <code>Manager</code> but this did not work any better) or with the <code>Value</code> but now idea how to get rid of it...</p>

<p><strong>Edit</strong></p>

<p>As I continue to try to change my code to work the way I want it to, I realize I did not mention what my main problem was. My real difficulty is to have all the process in the pool stop if a match is found. This is what I need so that running parallel is better than serial. For now I can have an event to tell the child run if a match was already found or not, but it keeps looping through the data, even if I raise an exception...</p>

<p><strong>Edit 2</strong></p>

<p>Simply put, I have the following...</p>

<pre><code>for o in objects:
    if too_close(o, existing_objects):
        return 1
return 0
</code></pre>

<p>...that I want to distribute among CPUs in something like...</p>

<pre><code>for o in objects:
    if too_close(o, some_existing_objects):
        return 1 and abort other processes
return 0
</code></pre>
","i am developping a script where i create objects at random but i dont want duplicates they are stored and each time i create a new one i check it against the already existing ones as i want to do it for large numbers of objects i am now trying to parallelize it but so far with no success i tried some solutions found on the web mainly here in fact but still not working
the idea is that i launch a pool and map my function to it when a process finds a match it sets a value to 1 this value is readable by all processes they can write it using a lock and i need it at the end to return thus i made a lock and a value so that all processes can read the value hence the lockfalse and check if a match has been found in another process then i tried something different with an event and checking if it is set but this is still not working i then tried to raise a special exception but still no success in making the code to a success
please i prefer to program oop so i will avoid until my last resource to define a global variable as i regard them as unsure personal opinion
here is a mwe i replaced my complicated objects by int and my stored objects by a range10000 to help you understand
now it yields a runtimeerror
i guess it has something to do with the lock though the commented manager but this did not work any better or with the value but now idea how to get rid of it
edit
as i continue to try to change my code to work the way i want it to i realize i did not mention what my main problem was my real difficulty is to have all the process in the pool stop if a match is found this is what i need so that running parallel is better than serial for now i can have an event to tell the child run if a match was already found or not but it keeps looping through the data even if i raise an exception
edit 2
simply put i have the following
that i want to distribute among cpus in something like
",1,"I am developping a script where I create objects at random but I don't want duplicates. They are stored and each time I create a new one I check it against the already existing ones. As I want to do it for large numbers of objects, I am now trying to parallelize it but so far with no success. I tried some solutions found on the web (mainly here in fact) but still not working.
The idea is that I launch a pool and map my function to it. When a process finds a match, it sets a value to 1. This value is readable by all processes, they can write it using a lock and I need it at the end to return. Thus I made a Lock and a Value, so that all processes can read the value (hence the lock=False) and check if a match has been found in another process. Then I tried something different with an Event and checking if it is set but this is still not working... I then tried to raise a special Exception but still no success in making the code to a success.
Please, I prefer to program OOP, so I will avoid until my last resource to define a global variable as I regard them as unsure (personal opinion).
Here is a MWE, I replaced my complicated objects by int and my stored objects by a range(10000) to help you understand.
#!/usr/bin/env python3

import multiprocessing as muproc


def ParallelCheck(me):
    print("" Testing {}..."".format(me))
    #manager = muproc.Manager()
    #lock = manager.Lock()
    lock = muproc.Lock()
    back = muproc.Value(""i"", 0, lock=False)
    ParChild = ParallelChild(me, lock, back)
    with muproc.Pool() as pool:
        try:
            pool.map(ParChild.run, range(10000))
        except AbortPool:
            pool.terminate()
            print(""pool"")
    return back.value


def Computation(me, neighbour):
    return me == neighbour



class ParallelChild(object):
    def __init__(self, me, lock, back):
        self.abort = muproc.Event()
        self.lock = lock
        self.me = me
        self.back = back

    def run(self, neighbour):
        print(""run"")
        if self.abort.is_set():
            print(""Aborting"")
            pass
        else:
            if Computation(self.me, neighbour):
                self.lock.acquire()
                self.abort.set()
                self.back.value = 1
                print(""GOTCHA"")
                self.lock.release()
                raise AbortPool
            else:
                print(""..."")


class AbortPool(Exception):
    #pass
    def __init__(self):
        ## Just to check
        print(""AbortPool raised!"")



if __name__ == ""__main__"":
    values = [12000, 13, 7]
    for v in values:
        print(""value={} match={}"".format(v, ParallelCheck(v)))

Now it yields a RunTimeError:
me@stigepc4$ python3 mwe.py 
 Testing 12000...
Traceback (most recent call last):
  File ""mwe.py"", line 63, in <module>
    print(""value={} match={}"".format(v, ParallelCheck(v)))
  File ""mwe.py"", line 16, in ParallelCheck
    pool.map(ParChild.run, range(10000))
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 260, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 599, in get
    raise self._value
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 383, in _handle_tasks
    put(task)
  File ""/usr/lib/python3.4/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(ForkingPickler.dumps(obj))
  File ""/usr/lib/python3.4/multiprocessing/reduction.py"", line 50, in dumps
    cls(buf, protocol).dump(obj)
  File ""/usr/lib/python3.4/multiprocessing/sharedctypes.py"", line 128, in reduce_ctype
    assert_spawning(obj)
  File ""/usr/lib/python3.4/multiprocessing/context.py"", line 347, in assert_spawning
    ' through inheritance' % type(obj).__name__
RuntimeError: c_int objects should only be shared between processes through inheritance

I guess it has something to do with the Lock (though the commented Manager but this did not work any better) or with the Value but now idea how to get rid of it...
Edit
As I continue to try to change my code to work the way I want it to, I realize I did not mention what my main problem was. My real difficulty is to have all the process in the pool stop if a match is found. This is what I need so that running parallel is better than serial. For now I can have an event to tell the child run if a match was already found or not, but it keeps looping through the data, even if I raise an exception...
Edit 2
Simply put, I have the following...
for o in objects:
    if too_close(o, existing_objects):
        return 1
return 0

...that I want to distribute among CPUs in something like...
for o in objects:
    if too_close(o, some_existing_objects):
        return 1 and abort other processes
return 0

","python multiprocessing pool abort if value
I am developping a script where I create objects at random but I don't want duplicates. They are stored and each time I create a new one I check it against the already existing ones. As I want to do it for large numbers of objects, I am now trying to parallelize it but so far with no success. I tried some solutions found on the web (mainly here in fact) but still not working.
The idea is that I launch a pool and map my function to it. When a process finds a match, it sets a value to 1. This value is readable by all processes, they can write it using a lock and I need it at the end to return. Thus I made a Lock and a Value, so that all processes can read the value (hence the lock=False) and check if a match has been found in another process. Then I tried something different with an Event and checking if it is set but this is still not working... I then tried to raise a special Exception but still no success in making the code to a success.
Please, I prefer to program OOP, so I will avoid until my last resource to define a global variable as I regard them as unsure (personal opinion).
Here is a MWE, I replaced my complicated objects by int and my stored objects by a range(10000) to help you understand.
#!/usr/bin/env python3

import multiprocessing as muproc


def ParallelCheck(me):
    print("" Testing {}..."".format(me))
    #manager = muproc.Manager()
    #lock = manager.Lock()
    lock = muproc.Lock()
    back = muproc.Value(""i"", 0, lock=False)
    ParChild = ParallelChild(me, lock, back)
    with muproc.Pool() as pool:
        try:
            pool.map(ParChild.run, range(10000))
        except AbortPool:
            pool.terminate()
            print(""pool"")
    return back.value


def Computation(me, neighbour):
    return me == neighbour



class ParallelChild(object):
    def __init__(self, me, lock, back):
        self.abort = muproc.Event()
        self.lock = lock
        self.me = me
        self.back = back

    def run(self, neighbour):
        print(""run"")
        if self.abort.is_set():
            print(""Aborting"")
            pass
        else:
            if Computation(self.me, neighbour):
                self.lock.acquire()
                self.abort.set()
                self.back.value = 1
                print(""GOTCHA"")
                self.lock.release()
                raise AbortPool
            else:
                print(""..."")


class AbortPool(Exception):
    #pass
    def __init__(self):
        ## Just to check
        print(""AbortPool raised!"")



if __name__ == ""__main__"":
    values = [12000, 13, 7]
    for v in values:
        print(""value={} match={}"".format(v, ParallelCheck(v)))

Now it yields a RunTimeError:
me@stigepc4$ python3 mwe.py 
 Testing 12000...
Traceback (most recent call last):
  File ""mwe.py"", line 63, in <module>
    print(""value={} match={}"".format(v, ParallelCheck(v)))
  File ""mwe.py"", line 16, in ParallelCheck
    pool.map(ParChild.run, range(10000))
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 260, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 599, in get
    raise self._value
  File ""/usr/lib/python3.4/multiprocessing/pool.py"", line 383, in _handle_tasks
    put(task)
  File ""/usr/lib/python3.4/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(ForkingPickler.dumps(obj))
  File ""/usr/lib/python3.4/multiprocessing/reduction.py"", line 50, in dumps
    cls(buf, protocol).dump(obj)
  File ""/usr/lib/python3.4/multiprocessing/sharedctypes.py"", line 128, in reduce_ctype
    assert_spawning(obj)
  File ""/usr/lib/python3.4/multiprocessing/context.py"", line 347, in assert_spawning
    ' through inheritance' % type(obj).__name__
RuntimeError: c_int objects should only be shared between processes through inheritance

I guess it has something to do with the Lock (though the commented Manager but this did not work any better) or with the Value but now idea how to get rid of it...
Edit
As I continue to try to change my code to work the way I want it to, I realize I did not mention what my main problem was. My real difficulty is to have all the process in the pool stop if a match is found. This is what I need so that running parallel is better than serial. For now I can have an event to tell the child run if a match was already found or not, but it keeps looping through the data, even if I raise an exception...
Edit 2
Simply put, I have the following...
for o in objects:
    if too_close(o, existing_objects):
        return 1
return 0

...that I want to distribute among CPUs in something like...
for o in objects:
    if too_close(o, some_existing_objects):
        return 1 and abort other processes
return 0

",1,1,0,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,1,1,0,0,0,0,,,Design,Design,,Design,Moderate,Moderate,,Moderate
12653348,12653348,Performance penalties of interop'ing C# with C functions,"<p>Barring actual performance tests of my code (I'm at the design stage), what is the general consensus on interfacing C code into C#? When will it be fruitful to do so, and when would it not?</p>
","barring actual performance tests of my code im at the design stage what is the general consensus on interfacing c code into c when will it be fruitful to do so and when would it not
",1,"Barring actual performance tests of my code (I'm at the design stage), what is the general consensus on interfacing C code into C#? When will it be fruitful to do so, and when would it not?
","Performance penalties of interop'ing C# with C functions
Barring actual performance tests of my code (I'm at the design stage), what is the general consensus on interfacing C code into C#? When will it be fruitful to do so, and when would it not?
",0,0,1,1,0,0,0,- Is a theoretical question,,,1,1,Runtime Efficiency,API Optimization,User is discussing an interface between languages,Runtime Efficiency,0,0,1,1,0,0,,,Architecture,Design,Current algorithm design is not scalable resulting in long execution time,Architecture,Mild,Mild,,Mild
41476000,41476000,My dithering algorithm is super slow,"<p>So here's some context. I'm working on this game called ShiftOS which takes place in an OS that starts off as a bare-bones run of the mill 80s operating system with not many features.</p>

<p>I'm trying to add a mechanic in where the user has to start out with binary (2-color) color depth and can only display black and white on the screen. Then they have to upgrade the color depth from 1-bit to 2-bit to 4-bit all the way to 24-bit. It's a really neat mechanic, but in practice it's seeming to be extremely difficult.</p>

<p>Of course, older systems around this time did at least TRY to make images look nice but of course they were limited by the color palettes given by the engineers, so they had to dither the images to arrange pixels in a way that made it look like the image was using more colors when in all reality it could only be using 2.</p>

<p>So I looked up some good dithering algorithms and started to learn the Floyd-Steinberg algorithm and soon ported it over to <code>C#</code> and <code>System.Drawing</code>.</p>

<p>Here's the code that I use.</p>

<pre><code>var bmp = new Bitmap(source.Width, source.Height);
var sourceBmp = (Bitmap)source;
int error = 0;
for (int y = 0; y &lt; bmp.Height; y++)
{
    for (int x = 0; x &lt; bmp.Width; x++)
    {
        Color c = sourceBmp.GetPixel(x, y);
        int gray = ((c.R + c.G + c.B) / 3);
        if (gray &gt;= 127)
        {
            error = gray - 255;
            bmp.SetPixel(x, y, Color.White);
        }
        else
        {
            error = gray;
            bmp.SetPixel(x, y, Color.Black);
        }
        /*
         * Pixel error diffusion map: Floyd-Steinberg. Thanks to Wikipedia.
         * 
         *  pixel[x + 1][y    ] := pixel[x + 1][y    ] + quant_error * 7 / 16
         *  pixel[x - 1][y + 1] := pixel[x - 1][y + 1] + quant_error * 3 / 16
         *  pixel[x    ][y + 1] := pixel[x    ][y + 1] + quant_error * 5 / 16
         *  pixel[x + 1][y + 1] := pixel[x + 1][y + 1] + quant_error * 1 / 16
         */

        if(x - 1 &gt;= 0 &amp;&amp; y + 1 != bmp.Height)
        {
            var bottomRightColor = sourceBmp.GetPixel(x - 1, y + 1);
            int bottomRightGray = ((bottomRightColor.R + bottomRightColor.G + bottomRightColor.B) / 3) + ((error * 3) / 16);
            if (bottomRightGray &lt; 0)
                bottomRightGray = 0;
            if (bottomRightGray &gt; 255)
                bottomRightGray = 255;
            sourceBmp.SetPixel(x - 1, y + 1, Color.FromArgb(bottomRightGray, bottomRightGray, bottomRightGray));
        }
        if (x + 1 != sourceBmp.Width)
        {
            var rightColor = sourceBmp.GetPixel(x + 1, y);
            int rightGray = ((rightColor.R + rightColor.G + rightColor.B) / 3) + ((error * 7) / 16);
            if (rightGray &lt; 0)
                rightGray = 0;
            if (rightGray &gt; 255)
                rightGray = 255;
            sourceBmp.SetPixel(x + 1, y, Color.FromArgb(rightGray, rightGray, rightGray));
        }
        if (x + 1 != sourceBmp.Width &amp;&amp; y + 1 != sourceBmp.Height)
        {
            var bottomRightColor = sourceBmp.GetPixel(x + 1, y + 1);
            int bottomRightGray = ((bottomRightColor.R + bottomRightColor.G + bottomRightColor.B) / 3) + ((error) / 16);
            if (bottomRightGray &lt; 0)
                bottomRightGray = 0;
            if (bottomRightGray &gt; 255)
                bottomRightGray = 255;
            sourceBmp.SetPixel(x + 1, y + 1, Color.FromArgb(bottomRightGray, bottomRightGray, bottomRightGray));
        }
        if (y + 1 != sourceBmp.Height)
        {
            var bottomColor = sourceBmp.GetPixel(x, y + 1);
            int bottomGray = ((bottomColor.R + bottomColor.G + bottomColor.B) / 3) + ((error * 5) / 16);
            if (bottomGray &lt; 0)
                bottomGray = 0;
            if (bottomGray &gt; 255)
                bottomGray = 255;
            sourceBmp.SetPixel(x, y + 1, Color.FromArgb(bottomGray, bottomGray, bottomGray));
        }
    }
}
</code></pre>

<p>Note that <code>source</code> is an <code>Image</code> that is passed through to the function via an argument.</p>

<p>This code works pretty well, however the problem is, the dithering is happening on a separate thread to minimize slowdowns/lag in the game, and while the dithering is occurring, the regular 24-bit colors/images of the operating system are shown. This would be fine if the dithering didn't take so long.</p>

<p>However I notice that the algorithm is EXTREMELY slow in this code and depending on the size of the image I'm dithering, the dithering process could take up to more than a minute!</p>

<p>I have applied all optimizations I can think of - such as running things in a separate thread from the game thread and invoking an Action that's given to the function when the thread finishes but this only shaves off a tiny bit of time if any.</p>

<p>So I'm wondering if there are any further optimizations to make this operate any faster, a few seconds total if possible. I'd like to also note that while the dithering operation is occurring I am having noticeable system lag - the mouse even jitters and jumps around at times. Not cool for those must-have-60FPS PC master race guys.</p>
","so heres some context im working on this game called shiftos which takes place in an os that starts off as a barebones run of the mill 80s operating system with not many features
im trying to add a mechanic in where the user has to start out with binary 2color color depth and can only display black and white on the screen then they have to upgrade the color depth from 1bit to 2bit to 4bit all the way to 24bit its a really neat mechanic but in practice its seeming to be extremely difficult
of course older systems around this time did at least try to make images look nice but of course they were limited by the color palettes given by the engineers so they had to dither the images to arrange pixels in a way that made it look like the image was using more colors when in all reality it could only be using 2
so i looked up some good dithering algorithms and started to learn the floydsteinberg algorithm and soon ported it over to c and systemdrawing
heres the code that i use
note that source is an image that is passed through to the function via an argument
this code works pretty well however the problem is the dithering is happening on a separate thread to minimize slowdownslag in the game and while the dithering is occurring the regular 24bit colorsimages of the operating system are shown this would be fine if the dithering didnt take so long
however i notice that the algorithm is extremely slow in this code and depending on the size of the image im dithering the dithering process could take up to more than a minute
i have applied all optimizations i can think of  such as running things in a separate thread from the game thread and invoking an action thats given to the function when the thread finishes but this only shaves off a tiny bit of time if any
so im wondering if there are any further optimizations to make this operate any faster a few seconds total if possible id like to also note that while the dithering operation is occurring i am having noticeable system lag  the mouse even jitters and jumps around at times not cool for those musthave60fps pc master race guys
",1,"So here's some context. I'm working on this game called ShiftOS which takes place in an OS that starts off as a bare-bones run of the mill 80s operating system with not many features.
I'm trying to add a mechanic in where the user has to start out with binary (2-color) color depth and can only display black and white on the screen. Then they have to upgrade the color depth from 1-bit to 2-bit to 4-bit all the way to 24-bit. It's a really neat mechanic, but in practice it's seeming to be extremely difficult.
Of course, older systems around this time did at least TRY to make images look nice but of course they were limited by the color palettes given by the engineers, so they had to dither the images to arrange pixels in a way that made it look like the image was using more colors when in all reality it could only be using 2.
So I looked up some good dithering algorithms and started to learn the Floyd-Steinberg algorithm and soon ported it over to C# and System.Drawing.
Here's the code that I use.
var bmp = new Bitmap(source.Width, source.Height);
var sourceBmp = (Bitmap)source;
int error = 0;
for (int y = 0; y < bmp.Height; y++)
{
    for (int x = 0; x < bmp.Width; x++)
    {
        Color c = sourceBmp.GetPixel(x, y);
        int gray = ((c.R + c.G + c.B) / 3);
        if (gray >= 127)
        {
            error = gray - 255;
            bmp.SetPixel(x, y, Color.White);
        }
        else
        {
            error = gray;
            bmp.SetPixel(x, y, Color.Black);
        }
        /*
         * Pixel error diffusion map: Floyd-Steinberg. Thanks to Wikipedia.
         * 
         *  pixel[x + 1][y    ] := pixel[x + 1][y    ] + quant_error * 7 / 16
         *  pixel[x - 1][y + 1] := pixel[x - 1][y + 1] + quant_error * 3 / 16
         *  pixel[x    ][y + 1] := pixel[x    ][y + 1] + quant_error * 5 / 16
         *  pixel[x + 1][y + 1] := pixel[x + 1][y + 1] + quant_error * 1 / 16
         */

        if(x - 1 >= 0 && y + 1 != bmp.Height)
        {
            var bottomRightColor = sourceBmp.GetPixel(x - 1, y + 1);
            int bottomRightGray = ((bottomRightColor.R + bottomRightColor.G + bottomRightColor.B) / 3) + ((error * 3) / 16);
            if (bottomRightGray < 0)
                bottomRightGray = 0;
            if (bottomRightGray > 255)
                bottomRightGray = 255;
            sourceBmp.SetPixel(x - 1, y + 1, Color.FromArgb(bottomRightGray, bottomRightGray, bottomRightGray));
        }
        if (x + 1 != sourceBmp.Width)
        {
            var rightColor = sourceBmp.GetPixel(x + 1, y);
            int rightGray = ((rightColor.R + rightColor.G + rightColor.B) / 3) + ((error * 7) / 16);
            if (rightGray < 0)
                rightGray = 0;
            if (rightGray > 255)
                rightGray = 255;
            sourceBmp.SetPixel(x + 1, y, Color.FromArgb(rightGray, rightGray, rightGray));
        }
        if (x + 1 != sourceBmp.Width && y + 1 != sourceBmp.Height)
        {
            var bottomRightColor = sourceBmp.GetPixel(x + 1, y + 1);
            int bottomRightGray = ((bottomRightColor.R + bottomRightColor.G + bottomRightColor.B) / 3) + ((error) / 16);
            if (bottomRightGray < 0)
                bottomRightGray = 0;
            if (bottomRightGray > 255)
                bottomRightGray = 255;
            sourceBmp.SetPixel(x + 1, y + 1, Color.FromArgb(bottomRightGray, bottomRightGray, bottomRightGray));
        }
        if (y + 1 != sourceBmp.Height)
        {
            var bottomColor = sourceBmp.GetPixel(x, y + 1);
            int bottomGray = ((bottomColor.R + bottomColor.G + bottomColor.B) / 3) + ((error * 5) / 16);
            if (bottomGray < 0)
                bottomGray = 0;
            if (bottomGray > 255)
                bottomGray = 255;
            sourceBmp.SetPixel(x, y + 1, Color.FromArgb(bottomGray, bottomGray, bottomGray));
        }
    }
}

Note that source is an Image that is passed through to the function via an argument.
This code works pretty well, however the problem is, the dithering is happening on a separate thread to minimize slowdowns/lag in the game, and while the dithering is occurring, the regular 24-bit colors/images of the operating system are shown. This would be fine if the dithering didn't take so long.
However I notice that the algorithm is EXTREMELY slow in this code and depending on the size of the image I'm dithering, the dithering process could take up to more than a minute!
I have applied all optimizations I can think of - such as running things in a separate thread from the game thread and invoking an Action that's given to the function when the thread finishes but this only shaves off a tiny bit of time if any.
So I'm wondering if there are any further optimizations to make this operate any faster, a few seconds total if possible. I'd like to also note that while the dithering operation is occurring I am having noticeable system lag - the mouse even jitters and jumps around at times. Not cool for those must-have-60FPS PC master race guys.
","My dithering algorithm is super slow
So here's some context. I'm working on this game called ShiftOS which takes place in an OS that starts off as a bare-bones run of the mill 80s operating system with not many features.
I'm trying to add a mechanic in where the user has to start out with binary (2-color) color depth and can only display black and white on the screen. Then they have to upgrade the color depth from 1-bit to 2-bit to 4-bit all the way to 24-bit. It's a really neat mechanic, but in practice it's seeming to be extremely difficult.
Of course, older systems around this time did at least TRY to make images look nice but of course they were limited by the color palettes given by the engineers, so they had to dither the images to arrange pixels in a way that made it look like the image was using more colors when in all reality it could only be using 2.
So I looked up some good dithering algorithms and started to learn the Floyd-Steinberg algorithm and soon ported it over to C# and System.Drawing.
Here's the code that I use.
var bmp = new Bitmap(source.Width, source.Height);
var sourceBmp = (Bitmap)source;
int error = 0;
for (int y = 0; y < bmp.Height; y++)
{
    for (int x = 0; x < bmp.Width; x++)
    {
        Color c = sourceBmp.GetPixel(x, y);
        int gray = ((c.R + c.G + c.B) / 3);
        if (gray >= 127)
        {
            error = gray - 255;
            bmp.SetPixel(x, y, Color.White);
        }
        else
        {
            error = gray;
            bmp.SetPixel(x, y, Color.Black);
        }
        /*
         * Pixel error diffusion map: Floyd-Steinberg. Thanks to Wikipedia.
         * 
         *  pixel[x + 1][y    ] := pixel[x + 1][y    ] + quant_error * 7 / 16
         *  pixel[x - 1][y + 1] := pixel[x - 1][y + 1] + quant_error * 3 / 16
         *  pixel[x    ][y + 1] := pixel[x    ][y + 1] + quant_error * 5 / 16
         *  pixel[x + 1][y + 1] := pixel[x + 1][y + 1] + quant_error * 1 / 16
         */

        if(x - 1 >= 0 && y + 1 != bmp.Height)
        {
            var bottomRightColor = sourceBmp.GetPixel(x - 1, y + 1);
            int bottomRightGray = ((bottomRightColor.R + bottomRightColor.G + bottomRightColor.B) / 3) + ((error * 3) / 16);
            if (bottomRightGray < 0)
                bottomRightGray = 0;
            if (bottomRightGray > 255)
                bottomRightGray = 255;
            sourceBmp.SetPixel(x - 1, y + 1, Color.FromArgb(bottomRightGray, bottomRightGray, bottomRightGray));
        }
        if (x + 1 != sourceBmp.Width)
        {
            var rightColor = sourceBmp.GetPixel(x + 1, y);
            int rightGray = ((rightColor.R + rightColor.G + rightColor.B) / 3) + ((error * 7) / 16);
            if (rightGray < 0)
                rightGray = 0;
            if (rightGray > 255)
                rightGray = 255;
            sourceBmp.SetPixel(x + 1, y, Color.FromArgb(rightGray, rightGray, rightGray));
        }
        if (x + 1 != sourceBmp.Width && y + 1 != sourceBmp.Height)
        {
            var bottomRightColor = sourceBmp.GetPixel(x + 1, y + 1);
            int bottomRightGray = ((bottomRightColor.R + bottomRightColor.G + bottomRightColor.B) / 3) + ((error) / 16);
            if (bottomRightGray < 0)
                bottomRightGray = 0;
            if (bottomRightGray > 255)
                bottomRightGray = 255;
            sourceBmp.SetPixel(x + 1, y + 1, Color.FromArgb(bottomRightGray, bottomRightGray, bottomRightGray));
        }
        if (y + 1 != sourceBmp.Height)
        {
            var bottomColor = sourceBmp.GetPixel(x, y + 1);
            int bottomGray = ((bottomColor.R + bottomColor.G + bottomColor.B) / 3) + ((error * 5) / 16);
            if (bottomGray < 0)
                bottomGray = 0;
            if (bottomGray > 255)
                bottomGray = 255;
            sourceBmp.SetPixel(x, y + 1, Color.FromArgb(bottomGray, bottomGray, bottomGray));
        }
    }
}

Note that source is an Image that is passed through to the function via an argument.
This code works pretty well, however the problem is, the dithering is happening on a separate thread to minimize slowdowns/lag in the game, and while the dithering is occurring, the regular 24-bit colors/images of the operating system are shown. This would be fine if the dithering didn't take so long.
However I notice that the algorithm is EXTREMELY slow in this code and depending on the size of the image I'm dithering, the dithering process could take up to more than a minute!
I have applied all optimizations I can think of - such as running things in a separate thread from the game thread and invoking an Action that's given to the function when the thread finishes but this only shaves off a tiny bit of time if any.
So I'm wondering if there are any further optimizations to make this operate any faster, a few seconds total if possible. I'd like to also note that while the dithering operation is occurring I am having noticeable system lag - the mouse even jitters and jumps around at times. Not cool for those must-have-60FPS PC master race guys.
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
59555742,59555742,An object holding small tables vs a larger object holding no tables,"<p>I asked this question about lua, but it wouldn't be bad to know if javascript or other programming languages are the same.</p>

<p>So, say I have an object:</p>

<pre><code>{og = {-200000, -200000, -199200, -200000}, re = {-200000, -200000, -199200, -200000}, GID = 0, isActive = true}
</code></pre>

<p>Right now it's made so that it holds two tables holding four things each.
My question is, would it be more memory or speed efficient to do it that way, or to have it in a bigger object like this?</p>

<pre><code>{ox1=-200000, oy1=-200000, ox2=-199200, oy2=-200000, rx1=-200000, ry1=-200000, rx2=-199200, ry2=-200000, GID = 0, isActive = true}
</code></pre>
","i asked this question about lua but it wouldnt be bad to know if javascript or other programming languages are the same
so say i have an object
right now its made so that it holds two tables holding four things each
my question is would it be more memory or speed efficient to do it that way or to have it in a bigger object like this
",0,"I asked this question about lua, but it wouldn't be bad to know if javascript or other programming languages are the same.
So, say I have an object:
{og = {-200000, -200000, -199200, -200000}, re = {-200000, -200000, -199200, -200000}, GID = 0, isActive = true}

Right now it's made so that it holds two tables holding four things each.
My question is, would it be more memory or speed efficient to do it that way, or to have it in a bigger object like this?
{ox1=-200000, oy1=-200000, ox2=-199200, oy2=-200000, rx1=-200000, ry1=-200000, rx2=-199200, ry2=-200000, GID = 0, isActive = true}

","An object holding small tables vs a larger object holding no tables
I asked this question about lua, but it wouldn't be bad to know if javascript or other programming languages are the same.
So, say I have an object:
{og = {-200000, -200000, -199200, -200000}, re = {-200000, -200000, -199200, -200000}, GID = 0, isActive = true}

Right now it's made so that it holds two tables holding four things each.
My question is, would it be more memory or speed efficient to do it that way, or to have it in a bigger object like this?
{ox1=-200000, oy1=-200000, ox2=-199200, oy2=-200000, rx1=-200000, ry1=-200000, rx2=-199200, ry2=-200000, GID = 0, isActive = true}

",0,0,1,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
8917696,8917696,library for transforming a node tree,"<p>I'd like to be able to express a general transformation of one tree into another without writing a bunch of repetitive spaghetti code. Are there any libraries to help with this problem? My target language is Python, but I'll look at other languages as long as it's feasible to port to Python.</p>

<p>Example: I'd like to transform this node tree: (please excuse the <a href=""http://en.wikipedia.org/wiki/S-expression"" rel=""noreferrer"">S-expressions</a>)</p>

<pre><code>(A (B) (C) (D))
</code></pre>

<p>Into this one:</p>

<pre><code>(C (B) (D))
</code></pre>

<p>As long as the parent is A and the second ancestor is C, regardless of context (there may be more parents or ancestors). I'd like to express this transformation in a simple, concise, and re-usable way. Of course this example is very specific. Please try to address the general case.</p>

<p>Edit: <a href=""http://kenai.com/projects/refactoringng/pages/Tutorial"" rel=""noreferrer"">RefactoringNG</a> is the kind of thing I'm looking for, although it introduces an entirely new grammar to solve the problem, which i'd like to avoid.  I'm still looking for more and/or better examples.</p>

<hr>

<p>Background:</p>

<p>I'm able to convert python and cheetah (don't ask!) files into tokenized tree representations, and in turn convert those into <a href=""http://lxml.de/"" rel=""noreferrer"">lxml</a> trees. I plan to then re-organize the tree and write-out the results in order to implement automated refactoring. XSLT seems to be the standard tool to rewrite XML, but the syntax is terrible (in my opinion, obviously) and nobody at our shop would understand it.</p>

<p>I could write some functions which simply use the lxml methods (.xpath and such) to implement my refactorings, but I'm worried that I will wind up with a bunch of purpose-built spaghetti code which can't be re-used.</p>
","id like to be able to express a general transformation of one tree into another without writing a bunch of repetitive spaghetti code are there any libraries to help with this problem my target language is python but ill look at other languages as long as its feasible to port to python
example id like to transform this node tree please excuse the sexpressions
into this one
as long as the parent is a and the second ancestor is c regardless of context there may be more parents or ancestors id like to express this transformation in a simple concise and reusable way of course this example is very specific please try to address the general case
edit refactoringng is the kind of thing im looking for although it introduces an entirely new grammar to solve the problem which id like to avoid  im still looking for more andor better examples

background
im able to convert python and cheetah dont ask files into tokenized tree representations and in turn convert those into lxml trees i plan to then reorganize the tree and writeout the results in order to implement automated refactoring xslt seems to be the standard tool to rewrite xml but the syntax is terrible in my opinion obviously and nobody at our shop would understand it
i could write some functions which simply use the lxml methods xpath and such to implement my refactorings but im worried that i will wind up with a bunch of purposebuilt spaghetti code which cant be reused
",1,"I'd like to be able to express a general transformation of one tree into another without writing a bunch of repetitive spaghetti code. Are there any libraries to help with this problem? My target language is Python, but I'll look at other languages as long as it's feasible to port to Python.
Example: I'd like to transform this node tree: (please excuse the S-expressions)
(A (B) (C) (D))

Into this one:
(C (B) (D))

As long as the parent is A and the second ancestor is C, regardless of context (there may be more parents or ancestors). I'd like to express this transformation in a simple, concise, and re-usable way. Of course this example is very specific. Please try to address the general case.
Edit: RefactoringNG is the kind of thing I'm looking for, although it introduces an entirely new grammar to solve the problem, which i'd like to avoid.  I'm still looking for more and/or better examples.

Background:
I'm able to convert python and cheetah (don't ask!) files into tokenized tree representations, and in turn convert those into lxml trees. I plan to then re-organize the tree and write-out the results in order to implement automated refactoring. XSLT seems to be the standard tool to rewrite XML, but the syntax is terrible (in my opinion, obviously) and nobody at our shop would understand it.
I could write some functions which simply use the lxml methods (.xpath and such) to implement my refactorings, but I'm worried that I will wind up with a bunch of purpose-built spaghetti code which can't be re-used.
","library for transforming a node tree
I'd like to be able to express a general transformation of one tree into another without writing a bunch of repetitive spaghetti code. Are there any libraries to help with this problem? My target language is Python, but I'll look at other languages as long as it's feasible to port to Python.
Example: I'd like to transform this node tree: (please excuse the S-expressions)
(A (B) (C) (D))

Into this one:
(C (B) (D))

As long as the parent is A and the second ancestor is C, regardless of context (there may be more parents or ancestors). I'd like to express this transformation in a simple, concise, and re-usable way. Of course this example is very specific. Please try to address the general case.
Edit: RefactoringNG is the kind of thing I'm looking for, although it introduces an entirely new grammar to solve the problem, which i'd like to avoid.  I'm still looking for more and/or better examples.

Background:
I'm able to convert python and cheetah (don't ask!) files into tokenized tree representations, and in turn convert those into lxml trees. I plan to then re-organize the tree and write-out the results in order to implement automated refactoring. XSLT seems to be the standard tool to rewrite XML, but the syntax is terrible (in my opinion, obviously) and nobody at our shop would understand it.
I could write some functions which simply use the lxml methods (.xpath and such) to implement my refactorings, but I'm worried that I will wind up with a bunch of purpose-built spaghetti code which can't be re-used.
",0,1,0,1,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,0,1,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
77761155,77761155,Mapstruct adding parameter impossible?,"<p>I want to upgrade my MapStruct mapper with using specefic class to remove duplicated code, but I don't really know how can I do it because in my case I have to use a specefic class for each mapper but I didn't know if there is a method to use/pass other things than my source and target</p>
<p>Exemple of mapper :</p>
<pre><code>    @Named(&quot;idsToActivity&quot;)
    default List&lt;Activity&gt; idsToActivity(List&lt;Long&gt; longs) {
        return longs.stream().map(id -&gt; {
            if (id == null) {
                return null;
            }
            Activity activity = new Activity();
            activity.setId(id);
            return activity;
        }).collect(Collectors.toList());
    }
</code></pre>
<p>I tried something like this for using it inside my mapper :</p>
<pre><code>    private List&lt;?&gt; mapperIdToEntity(List&lt;Long&gt; longs){
        return longs.stream()
                .map(id -&gt; {
                    if (id == null) {
                        return null;
                    }
                    ClassFromMapstruct entity = new ClassFromMapstruct();
                    entity.setId(id);
                    return entity;
                }).collect(Collectors.toList());
    }
</code></pre>
","i want to upgrade my mapstruct mapper with using specefic class to remove duplicated code but i dont really know how can i do it because in my case i have to use a specefic class for each mapper but i didnt know if there is a method to usepass other things than my source and target
exemple of mapper 
i tried something like this for using it inside my mapper 
",1,"I want to upgrade my MapStruct mapper with using specefic class to remove duplicated code, but I don't really know how can I do it because in my case I have to use a specefic class for each mapper but I didn't know if there is a method to use/pass other things than my source and target
Exemple of mapper :
    @Named(""idsToActivity"")
    default List<Activity> idsToActivity(List<Long> longs) {
        return longs.stream().map(id -> {
            if (id == null) {
                return null;
            }
            Activity activity = new Activity();
            activity.setId(id);
            return activity;
        }).collect(Collectors.toList());
    }

I tried something like this for using it inside my mapper :
    private List<?> mapperIdToEntity(List<Long> longs){
        return longs.stream()
                .map(id -> {
                    if (id == null) {
                        return null;
                    }
                    ClassFromMapstruct entity = new ClassFromMapstruct();
                    entity.setId(id);
                    return entity;
                }).collect(Collectors.toList());
    }

","Mapstruct adding parameter impossible?
I want to upgrade my MapStruct mapper with using specefic class to remove duplicated code, but I don't really know how can I do it because in my case I have to use a specefic class for each mapper but I didn't know if there is a method to use/pass other things than my source and target
Exemple of mapper :
    @Named(""idsToActivity"")
    default List<Activity> idsToActivity(List<Long> longs) {
        return longs.stream().map(id -> {
            if (id == null) {
                return null;
            }
            Activity activity = new Activity();
            activity.setId(id);
            return activity;
        }).collect(Collectors.toList());
    }

I tried something like this for using it inside my mapper :
    private List<?> mapperIdToEntity(List<Long> longs){
        return longs.stream()
                .map(id -> {
                    if (id == null) {
                        return null;
                    }
                    ClassFromMapstruct entity = new ClassFromMapstruct();
                    entity.setId(id);
                    return entity;
                }).collect(Collectors.toList());
    }

",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
2812143,2812143,Followup: Table Per Concrete Type Performance/Scaling,"<p>This is a followup/post question to another question; as I have discovered a solution that has led me to a new problem.</p>

<p><a href=""https://stackoverflow.com/questions/2802902/classes-to-entities-like-class-inheritence-problems"">Classes to Entities; Like-class inheritence problems</a></p>

<p>We're building this game system, so to speak; and I am using Table Per Concrete Type concept to perform inheritance modeling. </p>

<p>My concern is scalability and performance. I believe I have used a similar technique before and it came out pretty sluggish, but I am not a good DBA, I am a programmer. So I'm very foreign to how this will translate to SQL. But basically it seems to me that it would be slow and weak to have so many 'linked' queries running together. I've posted my schema and entity model for reference. If I can give clearer information, please ask, as I am really not certain what is relevant to 'tuning' this approach.</p>

<p><a href=""http://img46.imageshack.us/img46/1043/database.png"" rel=""nofollow noreferrer"">alt text http://img46.imageshack.us/img46/1043/database.png</a></p>

<p><a href=""http://img293.imageshack.us/img293/6454/entities.png"" rel=""nofollow noreferrer"">alt text http://img293.imageshack.us/img293/6454/entities.png</a></p>

<p>Sample use would be typical as follows;</p>

<pre><code>static void Main(string[] args)
{
    DataContext db = new DataContext();

    Character character = new Character()
    {
        Name = ""First"",
        Ring = new Mapping.Ring()
    };


    character.Ring.Keys.Add(
        new Random()
        {
            Dice = 2,
            Sides = 30,
        });

    character.Ring.Keys.Add(
        new Measure()
        {
            Maximum = 100,
            Minimum = 0
        });
    character.Ring.Keys.Add(
        new Measure()
        {
            Maximum = 100,
            Minimum = 0
        });

    db.Characters.AddObject(character);
    db.SaveChanges();

    foreach (Character c in db.Characters)
    {
        Console.WriteLine(c.Name);

        Console.WriteLine(new string('-', 80));

        foreach (Measure k in c.Ring.Keys.OfType&lt;Measure&gt;())
            Console.WriteLine(String.Format(""{0}/{1}"", k.Minimum, k.Maximum));

        foreach (Random k in c.Ring.Keys.OfType&lt;Random&gt;())
            Console.WriteLine(String.Format(""{0}d{1}"", k.Dice, k.Sides));
    }

    Console.ReadLine();
}
</code></pre>
","this is a followuppost question to another question as i have discovered a solution that has led me to a new problem
classes to entities likeclass inheritence problems
were building this game system so to speak and i am using table per concrete type concept to perform inheritance modeling 
my concern is scalability and performance i believe i have used a similar technique before and it came out pretty sluggish but i am not a good dba i am a programmer so im very foreign to how this will translate to sql but basically it seems to me that it would be slow and weak to have so many linked queries running together ive posted my schema and entity model for reference if i can give clearer information please ask as i am really not certain what is relevant to tuning this approach
alt text httpimg46imageshackusimg461043databasepng
alt text httpimg293imageshackusimg2936454entitiespng
sample use would be typical as follows
",1,"This is a followup/post question to another question; as I have discovered a solution that has led me to a new problem.
Classes to Entities; Like-class inheritence problems
We're building this game system, so to speak; and I am using Table Per Concrete Type concept to perform inheritance modeling. 
My concern is scalability and performance. I believe I have used a similar technique before and it came out pretty sluggish, but I am not a good DBA, I am a programmer. So I'm very foreign to how this will translate to SQL. But basically it seems to me that it would be slow and weak to have so many 'linked' queries running together. I've posted my schema and entity model for reference. If I can give clearer information, please ask, as I am really not certain what is relevant to 'tuning' this approach.
alt text http://img46.imageshack.us/img46/1043/database.png
alt text http://img293.imageshack.us/img293/6454/entities.png
Sample use would be typical as follows;
static void Main(string[] args)
{
    DataContext db = new DataContext();

    Character character = new Character()
    {
        Name = ""First"",
        Ring = new Mapping.Ring()
    };


    character.Ring.Keys.Add(
        new Random()
        {
            Dice = 2,
            Sides = 30,
        });

    character.Ring.Keys.Add(
        new Measure()
        {
            Maximum = 100,
            Minimum = 0
        });
    character.Ring.Keys.Add(
        new Measure()
        {
            Maximum = 100,
            Minimum = 0
        });

    db.Characters.AddObject(character);
    db.SaveChanges();

    foreach (Character c in db.Characters)
    {
        Console.WriteLine(c.Name);

        Console.WriteLine(new string('-', 80));

        foreach (Measure k in c.Ring.Keys.OfType<Measure>())
            Console.WriteLine(String.Format(""{0}/{1}"", k.Minimum, k.Maximum));

        foreach (Random k in c.Ring.Keys.OfType<Random>())
            Console.WriteLine(String.Format(""{0}d{1}"", k.Dice, k.Sides));
    }

    Console.ReadLine();
}

","Followup: Table Per Concrete Type Performance/Scaling
This is a followup/post question to another question; as I have discovered a solution that has led me to a new problem.
Classes to Entities; Like-class inheritence problems
We're building this game system, so to speak; and I am using Table Per Concrete Type concept to perform inheritance modeling. 
My concern is scalability and performance. I believe I have used a similar technique before and it came out pretty sluggish, but I am not a good DBA, I am a programmer. So I'm very foreign to how this will translate to SQL. But basically it seems to me that it would be slow and weak to have so many 'linked' queries running together. I've posted my schema and entity model for reference. If I can give clearer information, please ask, as I am really not certain what is relevant to 'tuning' this approach.
alt text http://img46.imageshack.us/img46/1043/database.png
alt text http://img293.imageshack.us/img293/6454/entities.png
Sample use would be typical as follows;
static void Main(string[] args)
{
    DataContext db = new DataContext();

    Character character = new Character()
    {
        Name = ""First"",
        Ring = new Mapping.Ring()
    };


    character.Ring.Keys.Add(
        new Random()
        {
            Dice = 2,
            Sides = 30,
        });

    character.Ring.Keys.Add(
        new Measure()
        {
            Maximum = 100,
            Minimum = 0
        });
    character.Ring.Keys.Add(
        new Measure()
        {
            Maximum = 100,
            Minimum = 0
        });

    db.Characters.AddObject(character);
    db.SaveChanges();

    foreach (Character c in db.Characters)
    {
        Console.WriteLine(c.Name);

        Console.WriteLine(new string('-', 80));

        foreach (Measure k in c.Ring.Keys.OfType<Measure>())
            Console.WriteLine(String.Format(""{0}/{1}"", k.Minimum, k.Maximum));

        foreach (Random k in c.Ring.Keys.OfType<Random>())
            Console.WriteLine(String.Format(""{0}d{1}"", k.Dice, k.Sides));
    }

    Console.ReadLine();
}

",0,1,1,0,0,0,1,,,,1,1,Database Performance,Database Performance,,Database Performance,0,1,1,0,0,0,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
17926123,17926123,Advice on refactoring jQuery time duration calculation,"<p>I am relatively new to jQuery and javascript in general; I'm knowledgeable enough to write basic scripts but I could do with some help optimising some of my code which to me does not seem efficient, but I don't know the best way to alter it. </p>

<p>I have two input type='time' form fields and a hidden form field which is used to hold the difference between the two fields.</p>

<p>Here is my code:</p>

<pre><code>function timeDifference(field1, field2) {
    var field1val = $(field1).val();
    var field2val = $(field2).val();

    if ((field1val !== '--:--') &amp;&amp; (field2val !== '--:--')) {           
        var hours = field1val.split(':')[0] - field2val.split(':')[0];
        var minutes = field1val.split(':')[1] - field2val.split(':')[1];

        minutes = minutes.toString().length&lt;2?'0'+minutes:minutes;
        if(minutes&lt;0){ 
        hours--;
        minutes = 60 + minutes;
        }
        hours = hours.toString().length&lt;2?'0'+hours:hours;

        var difference = hours + ':' + minutes;
        return difference;          
    } else return false;
};

    $('#assessmentHeaderStartTime, #assessmentHeaderEndTime').change(function() {
    var duration = timeDifference('#assessmentHeaderStartTime','#assessmentHeaderEndTime');
    $('#assessmentHeaderDuration').val(duration);
    });
</code></pre>

<p>I am dubious that .change() is the best way to go about updating the duration value but I'm not sure what would be the optimal solution. I know input type='time' is not supported across all browsers but for now Chrome &amp; Safari are our main focus which both support it. </p>

<p>I thought about maybe using .keyup() to only calculate the duration once the user has finished typing in a time, but Chrome &amp; Safari both provide the option of clicking arrows to increase/decrease time so you can input data in a few ways. Plus I know this will be used on tablets and I'm not exactly sure whether the time dials they use to select times count as clicks or key presses.</p>

<p>It seems inefficient to call the function every time a change is made when users could, for example, click and hold the button to increase the minute counter from 01 to 59 and have it call the function at every interval in between as part of normal usage behaviour. </p>

<p>I don't know what the optimal solution to this would be. Does anyone have any suggestions?</p>
","i am relatively new to jquery and javascript in general im knowledgeable enough to write basic scripts but i could do with some help optimising some of my code which to me does not seem efficient but i dont know the best way to alter it 
i have two input typetime form fields and a hidden form field which is used to hold the difference between the two fields
here is my code
i am dubious that change is the best way to go about updating the duration value but im not sure what would be the optimal solution i know input typetime is not supported across all browsers but for now chrome  safari are our main focus which both support it 
i thought about maybe using keyup to only calculate the duration once the user has finished typing in a time but chrome  safari both provide the option of clicking arrows to increasedecrease time so you can input data in a few ways plus i know this will be used on tablets and im not exactly sure whether the time dials they use to select times count as clicks or key presses
it seems inefficient to call the function every time a change is made when users could for example click and hold the button to increase the minute counter from 01 to 59 and have it call the function at every interval in between as part of normal usage behaviour 
i dont know what the optimal solution to this would be does anyone have any suggestions
",1,"I am relatively new to jQuery and javascript in general; I'm knowledgeable enough to write basic scripts but I could do with some help optimising some of my code which to me does not seem efficient, but I don't know the best way to alter it. 
I have two input type='time' form fields and a hidden form field which is used to hold the difference between the two fields.
Here is my code:
function timeDifference(field1, field2) {
    var field1val = $(field1).val();
    var field2val = $(field2).val();

    if ((field1val !== '--:--') && (field2val !== '--:--')) {           
        var hours = field1val.split(':')[0] - field2val.split(':')[0];
        var minutes = field1val.split(':')[1] - field2val.split(':')[1];

        minutes = minutes.toString().length<2?'0'+minutes:minutes;
        if(minutes<0){ 
        hours--;
        minutes = 60 + minutes;
        }
        hours = hours.toString().length<2?'0'+hours:hours;

        var difference = hours + ':' + minutes;
        return difference;          
    } else return false;
};

    $('#assessmentHeaderStartTime, #assessmentHeaderEndTime').change(function() {
    var duration = timeDifference('#assessmentHeaderStartTime','#assessmentHeaderEndTime');
    $('#assessmentHeaderDuration').val(duration);
    });

I am dubious that .change() is the best way to go about updating the duration value but I'm not sure what would be the optimal solution. I know input type='time' is not supported across all browsers but for now Chrome & Safari are our main focus which both support it. 
I thought about maybe using .keyup() to only calculate the duration once the user has finished typing in a time, but Chrome & Safari both provide the option of clicking arrows to increase/decrease time so you can input data in a few ways. Plus I know this will be used on tablets and I'm not exactly sure whether the time dials they use to select times count as clicks or key presses.
It seems inefficient to call the function every time a change is made when users could, for example, click and hold the button to increase the minute counter from 01 to 59 and have it call the function at every interval in between as part of normal usage behaviour. 
I don't know what the optimal solution to this would be. Does anyone have any suggestions?
","Advice on refactoring jQuery time duration calculation
I am relatively new to jQuery and javascript in general; I'm knowledgeable enough to write basic scripts but I could do with some help optimising some of my code which to me does not seem efficient, but I don't know the best way to alter it. 
I have two input type='time' form fields and a hidden form field which is used to hold the difference between the two fields.
Here is my code:
function timeDifference(field1, field2) {
    var field1val = $(field1).val();
    var field2val = $(field2).val();

    if ((field1val !== '--:--') && (field2val !== '--:--')) {           
        var hours = field1val.split(':')[0] - field2val.split(':')[0];
        var minutes = field1val.split(':')[1] - field2val.split(':')[1];

        minutes = minutes.toString().length<2?'0'+minutes:minutes;
        if(minutes<0){ 
        hours--;
        minutes = 60 + minutes;
        }
        hours = hours.toString().length<2?'0'+hours:hours;

        var difference = hours + ':' + minutes;
        return difference;          
    } else return false;
};

    $('#assessmentHeaderStartTime, #assessmentHeaderEndTime').change(function() {
    var duration = timeDifference('#assessmentHeaderStartTime','#assessmentHeaderEndTime');
    $('#assessmentHeaderDuration').val(duration);
    });

I am dubious that .change() is the best way to go about updating the duration value but I'm not sure what would be the optimal solution. I know input type='time' is not supported across all browsers but for now Chrome & Safari are our main focus which both support it. 
I thought about maybe using .keyup() to only calculate the duration once the user has finished typing in a time, but Chrome & Safari both provide the option of clicking arrows to increase/decrease time so you can input data in a few ways. Plus I know this will be used on tablets and I'm not exactly sure whether the time dials they use to select times count as clicks or key presses.
It seems inefficient to call the function every time a change is made when users could, for example, click and hold the button to increase the minute counter from 01 to 59 and have it call the function at every interval in between as part of normal usage behaviour. 
I don't know what the optimal solution to this would be. Does anyone have any suggestions?
",0,1,0,1,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
18322339,18322339,Throws exception and return from finally - Tomcat hangs,"<p><br>
Recently I was looking at the frequent hangs of the tomcat server and came across some exceptions thrown frequently in some part of the code. <br> </p>

<p>When I examined the code, this what it looked like <br> </p>

<pre><code>public static String doSomething() {
    String returnVakue = ""default value"";

    try {
        ResultSet rs = getMyResultSet(); 

        rs.first(); 
        returnValue = rs.getString(""my_field""); // Note that Exception happens at times when the ResultSet is empty

    } catch (Exception e) {
        throw new Exception(e);
    } finally {
        return returnValue;
    }

}
</code></pre>

<p>While I am aware that it is perfectly OK to have both throws exception and return, wondering if this can cause any kind of leaks in tomcat. And Is there any potential risk with performance. ? Hoowever my caller function stops the execution at this point. Any views on this? Does it affect the GC?<br></p>

<p>EDIT : NOTE : <strong><em>I Know how to correct this code. Please share your views whether this can potentially cause tomcat hanging.</em></strong></p>
","
recently i was looking at the frequent hangs of the tomcat server and came across some exceptions thrown frequently in some part of the code  
when i examined the code this what it looked like  
while i am aware that it is perfectly ok to have both throws exception and return wondering if this can cause any kind of leaks in tomcat and is there any potential risk with performance  hoowever my caller function stops the execution at this point any views on this does it affect the gc
edit  note  i know how to correct this code please share your views whether this can potentially cause tomcat hanging
",1,"
Recently I was looking at the frequent hangs of the tomcat server and came across some exceptions thrown frequently in some part of the code.  
When I examined the code, this what it looked like  
public static String doSomething() {
    String returnVakue = ""default value"";

    try {
        ResultSet rs = getMyResultSet(); 

        rs.first(); 
        returnValue = rs.getString(""my_field""); // Note that Exception happens at times when the ResultSet is empty

    } catch (Exception e) {
        throw new Exception(e);
    } finally {
        return returnValue;
    }

}

While I am aware that it is perfectly OK to have both throws exception and return, wondering if this can cause any kind of leaks in tomcat. And Is there any potential risk with performance. ? Hoowever my caller function stops the execution at this point. Any views on this? Does it affect the GC?
EDIT : NOTE : I Know how to correct this code. Please share your views whether this can potentially cause tomcat hanging.
","Throws exception and return from finally - Tomcat hangs

Recently I was looking at the frequent hangs of the tomcat server and came across some exceptions thrown frequently in some part of the code.  
When I examined the code, this what it looked like  
public static String doSomething() {
    String returnVakue = ""default value"";

    try {
        ResultSet rs = getMyResultSet(); 

        rs.first(); 
        returnValue = rs.getString(""my_field""); // Note that Exception happens at times when the ResultSet is empty

    } catch (Exception e) {
        throw new Exception(e);
    } finally {
        return returnValue;
    }

}

While I am aware that it is perfectly OK to have both throws exception and return, wondering if this can cause any kind of leaks in tomcat. And Is there any potential risk with performance. ? Hoowever my caller function stops the execution at this point. Any views on this? Does it affect the GC?
EDIT : NOTE : I Know how to correct this code. Please share your views whether this can potentially cause tomcat hanging.
",1,0,0,0,1,0,1,,,,1,1,Security vs. Performance Trade-offs,Security vs. Performance Trade-offs,,Security vs. Performance Trade-offs,1,0,0,0,1,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
43016874,43016874,"Sort part of array with heapsort, bug","<p>Here is my code for a introsort. I have trouble getting the heapsort part of the code working.</p>

<p><code>partition()</code> and <code>sort()</code> works like it should, but the heapsort part doesnt sort correctly. I get sorted arrays (size=10) like this:</p>

<pre><code>10
18
26
35
25
39
49
49
57
89
</code></pre>

<p>There are mostly sorted, except for a few numbers. I am only trying to sort a part of the array in each <code>heapsort()</code> call.</p>

<pre><code>public class IntroSort {

    public static void sort(int[] arrayToSort){     
        int depth = ((int) Math.log(arrayToSort.length))*2;
        sort(arrayToSort, depth, 0, arrayToSort.length-1);
    }

    private static void sort(int[] arrayToSort, int depth, int start, int end){
        int length = arrayToSort.length;
        if(length &lt;= 1){
            return;
        }else if(depth == 0){
            heapSort(arrayToSort, start, end);
        }else{
            if(start &gt;= end)
                return;
            int pivot = arrayToSort[(start + end)/2];
            int index =  partition(arrayToSort, start, end, pivot);
            sort(arrayToSort, depth-1, start, index-1);
            sort(arrayToSort, depth-1, index, end);
        }
    }

    private static void heapSort(int[] arrayToSort, int start, int end){
        for (int i = end / 2 - 1; i &gt;= start; i--)
            heapify(arrayToSort, end, i);
        for (int i=end-1; i&gt;=start; i--){
            int temp = arrayToSort[start];
            arrayToSort[start] = arrayToSort[i];
            arrayToSort[i] = temp;
            heapify(arrayToSort, i, start);
        }
    }

    private static void heapify(int[] arrayToSort, int n, int i){
        int largest = i;
        int l = 2*i + 1;
        int r = 2*i + 2;
        if (l &lt; n &amp;&amp; arrayToSort[l] &gt; arrayToSort[largest])
            largest = l;
        if (r &lt; n &amp;&amp; arrayToSort[r] &gt; arrayToSort[largest])
            largest = r;
        if (largest != i){
            int swap = arrayToSort[i];
            arrayToSort[i] = arrayToSort[largest];
            arrayToSort[largest] = swap;
            heapify(arrayToSort, n, largest);
        }
    }

    private static int partition(int[] arrayToSort, int start, int end, int pivot){
        while(start &lt;= end){
            while(arrayToSort[start] &lt; pivot){
                start++;
            }
            while(arrayToSort[end] &gt; pivot){
                end--;
            }
            if(start &lt;= end){
                int temp = arrayToSort[start];
                arrayToSort[start] = arrayToSort[end];
                arrayToSort[end] = temp;
                start++;
                end--;
            }
        }
        return start;
    }

}
</code></pre>

<p>Any ideas?</p>
","here is my code for a introsort i have trouble getting the heapsort part of the code working
partition and sort works like it should but the heapsort part doesnt sort correctly i get sorted arrays size10 like this
there are mostly sorted except for a few numbers i am only trying to sort a part of the array in each heapsort call
any ideas
",1,"Here is my code for a introsort. I have trouble getting the heapsort part of the code working.
partition() and sort() works like it should, but the heapsort part doesnt sort correctly. I get sorted arrays (size=10) like this:
10
18
26
35
25
39
49
49
57
89

There are mostly sorted, except for a few numbers. I am only trying to sort a part of the array in each heapsort() call.
public class IntroSort {

    public static void sort(int[] arrayToSort){     
        int depth = ((int) Math.log(arrayToSort.length))*2;
        sort(arrayToSort, depth, 0, arrayToSort.length-1);
    }

    private static void sort(int[] arrayToSort, int depth, int start, int end){
        int length = arrayToSort.length;
        if(length <= 1){
            return;
        }else if(depth == 0){
            heapSort(arrayToSort, start, end);
        }else{
            if(start >= end)
                return;
            int pivot = arrayToSort[(start + end)/2];
            int index =  partition(arrayToSort, start, end, pivot);
            sort(arrayToSort, depth-1, start, index-1);
            sort(arrayToSort, depth-1, index, end);
        }
    }

    private static void heapSort(int[] arrayToSort, int start, int end){
        for (int i = end / 2 - 1; i >= start; i--)
            heapify(arrayToSort, end, i);
        for (int i=end-1; i>=start; i--){
            int temp = arrayToSort[start];
            arrayToSort[start] = arrayToSort[i];
            arrayToSort[i] = temp;
            heapify(arrayToSort, i, start);
        }
    }

    private static void heapify(int[] arrayToSort, int n, int i){
        int largest = i;
        int l = 2*i + 1;
        int r = 2*i + 2;
        if (l < n && arrayToSort[l] > arrayToSort[largest])
            largest = l;
        if (r < n && arrayToSort[r] > arrayToSort[largest])
            largest = r;
        if (largest != i){
            int swap = arrayToSort[i];
            arrayToSort[i] = arrayToSort[largest];
            arrayToSort[largest] = swap;
            heapify(arrayToSort, n, largest);
        }
    }

    private static int partition(int[] arrayToSort, int start, int end, int pivot){
        while(start <= end){
            while(arrayToSort[start] < pivot){
                start++;
            }
            while(arrayToSort[end] > pivot){
                end--;
            }
            if(start <= end){
                int temp = arrayToSort[start];
                arrayToSort[start] = arrayToSort[end];
                arrayToSort[end] = temp;
                start++;
                end--;
            }
        }
        return start;
    }

}

Any ideas?
","Sort part of array with heapsort, bug
Here is my code for a introsort. I have trouble getting the heapsort part of the code working.
partition() and sort() works like it should, but the heapsort part doesnt sort correctly. I get sorted arrays (size=10) like this:
10
18
26
35
25
39
49
49
57
89

There are mostly sorted, except for a few numbers. I am only trying to sort a part of the array in each heapsort() call.
public class IntroSort {

    public static void sort(int[] arrayToSort){     
        int depth = ((int) Math.log(arrayToSort.length))*2;
        sort(arrayToSort, depth, 0, arrayToSort.length-1);
    }

    private static void sort(int[] arrayToSort, int depth, int start, int end){
        int length = arrayToSort.length;
        if(length <= 1){
            return;
        }else if(depth == 0){
            heapSort(arrayToSort, start, end);
        }else{
            if(start >= end)
                return;
            int pivot = arrayToSort[(start + end)/2];
            int index =  partition(arrayToSort, start, end, pivot);
            sort(arrayToSort, depth-1, start, index-1);
            sort(arrayToSort, depth-1, index, end);
        }
    }

    private static void heapSort(int[] arrayToSort, int start, int end){
        for (int i = end / 2 - 1; i >= start; i--)
            heapify(arrayToSort, end, i);
        for (int i=end-1; i>=start; i--){
            int temp = arrayToSort[start];
            arrayToSort[start] = arrayToSort[i];
            arrayToSort[i] = temp;
            heapify(arrayToSort, i, start);
        }
    }

    private static void heapify(int[] arrayToSort, int n, int i){
        int largest = i;
        int l = 2*i + 1;
        int r = 2*i + 2;
        if (l < n && arrayToSort[l] > arrayToSort[largest])
            largest = l;
        if (r < n && arrayToSort[r] > arrayToSort[largest])
            largest = r;
        if (largest != i){
            int swap = arrayToSort[i];
            arrayToSort[i] = arrayToSort[largest];
            arrayToSort[largest] = swap;
            heapify(arrayToSort, n, largest);
        }
    }

    private static int partition(int[] arrayToSort, int start, int end, int pivot){
        while(start <= end){
            while(arrayToSort[start] < pivot){
                start++;
            }
            while(arrayToSort[end] > pivot){
                end--;
            }
            if(start <= end){
                int temp = arrayToSort[start];
                arrayToSort[start] = arrayToSort[end];
                arrayToSort[end] = temp;
                start++;
                end--;
            }
        }
        return start;
    }

}

Any ideas?
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
12495140,12495140,How can I refactor this C# code below?,"<p>I have 12 buttons in my Form1, and each button has a textbox next to it. The button event calls a method called dialogueOpen which handles getting the an object from form2 and placing a string value in a textbox. </p>

<p>How can I place the value returned in a textbox depending on what button the user clicked on? So if it is button1 a user clicked on, then the text returned should be placed in textbox1 and if it is button2 the user clicked on then the text returned should be placed in textbox2. The point is avoid using a string name to check as the buttons can all be called ""browse"".</p>

<p>Right now my code below does that but it is quite repetitive is there is a better of doing this? </p>

<pre><code>    private void dailogueOpen(String btnName)
    {
        if (listBox1.SelectedItem == null)
        {
            MessageBox.Show(""Please Select a form"");
        }
        else
        {
            var selectedItem = (FormItems)listBox1.SelectedItem;
            var form2result = new Form2(myDataSet, selectedItem);
            var resulOfForm2 = form2result.ShowDialog();

            if (resulOfForm2 == DialogResult.OK)
            {
                switch (btnName)
                {
                    case ""btn1"":
                        textBox1.Text = form2result.getValue();
                        break;
                    case ""btn2"":
                        textBox2.Text = form2result.getValue();
                        break;
                    case ""btn3"":
                        textBox3.Text = form2result.getValue();
                        break;
                    case ""btn4"":
                        textBox4.Text = form2result.getValue();
                        break;
                    case ""btn5"":
                        textBox5.Text = form2result.getValue();
                        break;
                }
            }
        }
    }


    private void button1_Click(object sender, EventArgs e)
    {
        String name = ""btn1"";
        dailogueOpen(name);
    }

    private void button2_Click(object sender, EventArgs e)
    {
        String name = ""btn2"";
        dailogueOpen(name);
    }

    private void button3_Click(object sender, EventArgs e)
    {
        String name = ""btn3"";
        dailogueOpen(name);
    }

    private void button4_Click(object sender, EventArgs e)
    {
        String name = ""btn4"";
        dailogueOpen(name);
    }

    private void button5_Click(object sender, EventArgs e)
    {
        String name = ""btn5"";
        dailogueOpen(name);
    }
</code></pre>
","i have 12 buttons in my form1 and each button has a textbox next to it the button event calls a method called dialogueopen which handles getting the an object from form2 and placing a string value in a textbox 
how can i place the value returned in a textbox depending on what button the user clicked on so if it is button1 a user clicked on then the text returned should be placed in textbox1 and if it is button2 the user clicked on then the text returned should be placed in textbox2 the point is avoid using a string name to check as the buttons can all be called browse
right now my code below does that but it is quite repetitive is there is a better of doing this 
",1,"I have 12 buttons in my Form1, and each button has a textbox next to it. The button event calls a method called dialogueOpen which handles getting the an object from form2 and placing a string value in a textbox. 
How can I place the value returned in a textbox depending on what button the user clicked on? So if it is button1 a user clicked on, then the text returned should be placed in textbox1 and if it is button2 the user clicked on then the text returned should be placed in textbox2. The point is avoid using a string name to check as the buttons can all be called ""browse"".
Right now my code below does that but it is quite repetitive is there is a better of doing this? 
    private void dailogueOpen(String btnName)
    {
        if (listBox1.SelectedItem == null)
        {
            MessageBox.Show(""Please Select a form"");
        }
        else
        {
            var selectedItem = (FormItems)listBox1.SelectedItem;
            var form2result = new Form2(myDataSet, selectedItem);
            var resulOfForm2 = form2result.ShowDialog();

            if (resulOfForm2 == DialogResult.OK)
            {
                switch (btnName)
                {
                    case ""btn1"":
                        textBox1.Text = form2result.getValue();
                        break;
                    case ""btn2"":
                        textBox2.Text = form2result.getValue();
                        break;
                    case ""btn3"":
                        textBox3.Text = form2result.getValue();
                        break;
                    case ""btn4"":
                        textBox4.Text = form2result.getValue();
                        break;
                    case ""btn5"":
                        textBox5.Text = form2result.getValue();
                        break;
                }
            }
        }
    }


    private void button1_Click(object sender, EventArgs e)
    {
        String name = ""btn1"";
        dailogueOpen(name);
    }

    private void button2_Click(object sender, EventArgs e)
    {
        String name = ""btn2"";
        dailogueOpen(name);
    }

    private void button3_Click(object sender, EventArgs e)
    {
        String name = ""btn3"";
        dailogueOpen(name);
    }

    private void button4_Click(object sender, EventArgs e)
    {
        String name = ""btn4"";
        dailogueOpen(name);
    }

    private void button5_Click(object sender, EventArgs e)
    {
        String name = ""btn5"";
        dailogueOpen(name);
    }

","How can I refactor this C# code below?
I have 12 buttons in my Form1, and each button has a textbox next to it. The button event calls a method called dialogueOpen which handles getting the an object from form2 and placing a string value in a textbox. 
How can I place the value returned in a textbox depending on what button the user clicked on? So if it is button1 a user clicked on, then the text returned should be placed in textbox1 and if it is button2 the user clicked on then the text returned should be placed in textbox2. The point is avoid using a string name to check as the buttons can all be called ""browse"".
Right now my code below does that but it is quite repetitive is there is a better of doing this? 
    private void dailogueOpen(String btnName)
    {
        if (listBox1.SelectedItem == null)
        {
            MessageBox.Show(""Please Select a form"");
        }
        else
        {
            var selectedItem = (FormItems)listBox1.SelectedItem;
            var form2result = new Form2(myDataSet, selectedItem);
            var resulOfForm2 = form2result.ShowDialog();

            if (resulOfForm2 == DialogResult.OK)
            {
                switch (btnName)
                {
                    case ""btn1"":
                        textBox1.Text = form2result.getValue();
                        break;
                    case ""btn2"":
                        textBox2.Text = form2result.getValue();
                        break;
                    case ""btn3"":
                        textBox3.Text = form2result.getValue();
                        break;
                    case ""btn4"":
                        textBox4.Text = form2result.getValue();
                        break;
                    case ""btn5"":
                        textBox5.Text = form2result.getValue();
                        break;
                }
            }
        }
    }


    private void button1_Click(object sender, EventArgs e)
    {
        String name = ""btn1"";
        dailogueOpen(name);
    }

    private void button2_Click(object sender, EventArgs e)
    {
        String name = ""btn2"";
        dailogueOpen(name);
    }

    private void button3_Click(object sender, EventArgs e)
    {
        String name = ""btn3"";
        dailogueOpen(name);
    }

    private void button4_Click(object sender, EventArgs e)
    {
        String name = ""btn4"";
        dailogueOpen(name);
    }

    private void button5_Click(object sender, EventArgs e)
    {
        String name = ""btn5"";
        dailogueOpen(name);
    }

",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
31485849,31485849,how to refactor this multi-nested if else block,"<p>I have this kind of multi-nested if-else block. My understanding is that there is a 'data-driven' approach that can help eliminate the need for it and trim down the code, however, I'm not experienced with it in a large way yet, so can anyone help me refactor this code to work in the 'data-driven' approach? </p>

<pre><code>function (arg1) {
  if(this.thing[arg1]){
    // there is a valid property for this arg1
    if(this.thing.a){
      // there exists an 'a' propertie also
      if(this.thing.a.arg1 == arg1){
        // the a property has a property is the same as the arg1    
        // if this 'a' has a number higher than 0, avoid doing anything
        if(this.thing.a.number &gt; 0){
        }else{
          // 'a' number was 0 or lower, so we do something
          this.thing.a = this.thing[arg1];
          this.thing.a.arg1 = arg1;
        }
      }else{
        // the' a' is not the arg1
        // so we want to use the current arg1!
        // but only if its 'number' is lower than 1
        if(this.thing.a.number &gt; 0){
        }else{
          // 'a' number was 0 or lower, so we do something
          this.thing.a = this.thing[arg1];
          this.thing.a.arg1 = arg1;
        }
      }
    }else{
      // there is no thing.a so we set it to arg1 
      this.thing.a = this.thing[arg1];
      this.thing.a.arg1 = arg1;
    }
  }
}
</code></pre>
","i have this kind of multinested ifelse block my understanding is that there is a datadriven approach that can help eliminate the need for it and trim down the code however im not experienced with it in a large way yet so can anyone help me refactor this code to work in the datadriven approach 
",1,"I have this kind of multi-nested if-else block. My understanding is that there is a 'data-driven' approach that can help eliminate the need for it and trim down the code, however, I'm not experienced with it in a large way yet, so can anyone help me refactor this code to work in the 'data-driven' approach? 
function (arg1) {
  if(this.thing[arg1]){
    // there is a valid property for this arg1
    if(this.thing.a){
      // there exists an 'a' propertie also
      if(this.thing.a.arg1 == arg1){
        // the a property has a property is the same as the arg1    
        // if this 'a' has a number higher than 0, avoid doing anything
        if(this.thing.a.number > 0){
        }else{
          // 'a' number was 0 or lower, so we do something
          this.thing.a = this.thing[arg1];
          this.thing.a.arg1 = arg1;
        }
      }else{
        // the' a' is not the arg1
        // so we want to use the current arg1!
        // but only if its 'number' is lower than 1
        if(this.thing.a.number > 0){
        }else{
          // 'a' number was 0 or lower, so we do something
          this.thing.a = this.thing[arg1];
          this.thing.a.arg1 = arg1;
        }
      }
    }else{
      // there is no thing.a so we set it to arg1 
      this.thing.a = this.thing[arg1];
      this.thing.a.arg1 = arg1;
    }
  }
}

","how to refactor this multi-nested if else block
I have this kind of multi-nested if-else block. My understanding is that there is a 'data-driven' approach that can help eliminate the need for it and trim down the code, however, I'm not experienced with it in a large way yet, so can anyone help me refactor this code to work in the 'data-driven' approach? 
function (arg1) {
  if(this.thing[arg1]){
    // there is a valid property for this arg1
    if(this.thing.a){
      // there exists an 'a' propertie also
      if(this.thing.a.arg1 == arg1){
        // the a property has a property is the same as the arg1    
        // if this 'a' has a number higher than 0, avoid doing anything
        if(this.thing.a.number > 0){
        }else{
          // 'a' number was 0 or lower, so we do something
          this.thing.a = this.thing[arg1];
          this.thing.a.arg1 = arg1;
        }
      }else{
        // the' a' is not the arg1
        // so we want to use the current arg1!
        // but only if its 'number' is lower than 1
        if(this.thing.a.number > 0){
        }else{
          // 'a' number was 0 or lower, so we do something
          this.thing.a = this.thing[arg1];
          this.thing.a.arg1 = arg1;
        }
      }
    }else{
      // there is no thing.a so we set it to arg1 
      this.thing.a = this.thing[arg1];
      this.thing.a.arg1 = arg1;
    }
  }
}

",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
53698981,53698981,Add Data From Text File Using Parallel Programming,"<p>I am trying to read data from a .txt file, store them into a List, and then using parallel programming to do this twice at the same time. I have 2 .txt files containing a bunch of numbers to add up. I declare both files, then store them into List objects.</p>

<pre><code>    private void Form1_Load(object sender, EventArgs e)
{
    var file1 = @""filepath.txt"";
    var file2 = @""filepath.txt"";
    List&lt;string&gt; List = File.ReadAllLines(file1).ToList();
    List&lt;string&gt; List2 = File.ReadAllLines(file2).ToList();

    int num = 0;
    int num2 = 0;

    for(int i = 0; i &lt; List.Count i++)
    {
        num += Convert.ToInt16(List[i].ToString());
    }

    for(int i = 0; i &lt; List2.Count; i++)
    {
        num2 += Convert.ToInt16(List2[i].ToString());
    }
    listBox1.Items.Add(num);
    listBox2.Items.AddNum
}
</code></pre>

<p>As for the for the @""filepath.txt"", they are both exact paths, just didn't want to have 2 long lines of file location.
This is what I need to do, but do it using multi-threading, parallel programming, or asynchronous programming.</p>
","i am trying to read data from a txt file store them into a list and then using parallel programming to do this twice at the same time i have 2 txt files containing a bunch of numbers to add up i declare both files then store them into list objects
as for the for the filepathtxt they are both exact paths just didnt want to have 2 long lines of file location
this is what i need to do but do it using multithreading parallel programming or asynchronous programming
",0,"I am trying to read data from a .txt file, store them into a List, and then using parallel programming to do this twice at the same time. I have 2 .txt files containing a bunch of numbers to add up. I declare both files, then store them into List objects.
    private void Form1_Load(object sender, EventArgs e)
{
    var file1 = @""filepath.txt"";
    var file2 = @""filepath.txt"";
    List<string> List = File.ReadAllLines(file1).ToList();
    List<string> List2 = File.ReadAllLines(file2).ToList();

    int num = 0;
    int num2 = 0;

    for(int i = 0; i < List.Count i++)
    {
        num += Convert.ToInt16(List[i].ToString());
    }

    for(int i = 0; i < List2.Count; i++)
    {
        num2 += Convert.ToInt16(List2[i].ToString());
    }
    listBox1.Items.Add(num);
    listBox2.Items.AddNum
}

As for the for the @""filepath.txt"", they are both exact paths, just didn't want to have 2 long lines of file location.
This is what I need to do, but do it using multi-threading, parallel programming, or asynchronous programming.
","Add Data From Text File Using Parallel Programming
I am trying to read data from a .txt file, store them into a List, and then using parallel programming to do this twice at the same time. I have 2 .txt files containing a bunch of numbers to add up. I declare both files, then store them into List objects.
    private void Form1_Load(object sender, EventArgs e)
{
    var file1 = @""filepath.txt"";
    var file2 = @""filepath.txt"";
    List<string> List = File.ReadAllLines(file1).ToList();
    List<string> List2 = File.ReadAllLines(file2).ToList();

    int num = 0;
    int num2 = 0;

    for(int i = 0; i < List.Count i++)
    {
        num += Convert.ToInt16(List[i].ToString());
    }

    for(int i = 0; i < List2.Count; i++)
    {
        num2 += Convert.ToInt16(List2[i].ToString());
    }
    listBox1.Items.Add(num);
    listBox2.Items.AddNum
}

As for the for the @""filepath.txt"", they are both exact paths, just didn't want to have 2 long lines of file location.
This is what I need to do, but do it using multi-threading, parallel programming, or asynchronous programming.
",0,1,0,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,0,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
52329970,52329970,Execute different functions based on conditions without if statements,"<p>Is there a better approach/coding style to execute different functions under different conditions <strong>without</strong> using if statements?</p>

<p>I have been coding in the following way in JavaScript: (for example, a function in different conditions might use different fetch methods from an API)</p>

<pre><code>if(answer == 'a'){
  foo()

if(answer == 'b'){
  bar()

if(answer == 'c'){
  bar_2()

if(answer == 'd'){
  foo_3()
</code></pre>

<p>I have thought of using <code>eval()</code>, but is it a good approach? for example, creating an object consists of keys as conditions and function names as property.</p>

<pre><code>conditions:{
  a: 'foo',
  b: 'bar',
  c: 'foo_2',
  d: 'bar_2',
}
</code></pre>

<p>and run it like <code>eval(this.conditions[a])</code> </p>

<p>But I also heard that using <code>eval</code> would be difficult for testing.</p>
","is there a better approachcoding style to execute different functions under different conditions without using if statements
i have been coding in the following way in javascript for example a function in different conditions might use different fetch methods from an api
i have thought of using eval but is it a good approach for example creating an object consists of keys as conditions and function names as property
and run it like evalthisconditionsa 
but i also heard that using eval would be difficult for testing
",1,"Is there a better approach/coding style to execute different functions under different conditions without using if statements?
I have been coding in the following way in JavaScript: (for example, a function in different conditions might use different fetch methods from an API)
if(answer == 'a'){
  foo()

if(answer == 'b'){
  bar()

if(answer == 'c'){
  bar_2()

if(answer == 'd'){
  foo_3()

I have thought of using eval(), but is it a good approach? for example, creating an object consists of keys as conditions and function names as property.
conditions:{
  a: 'foo',
  b: 'bar',
  c: 'foo_2',
  d: 'bar_2',
}

and run it like eval(this.conditions[a]) 
But I also heard that using eval would be difficult for testing.
","Execute different functions based on conditions without if statements
Is there a better approach/coding style to execute different functions under different conditions without using if statements?
I have been coding in the following way in JavaScript: (for example, a function in different conditions might use different fetch methods from an API)
if(answer == 'a'){
  foo()

if(answer == 'b'){
  bar()

if(answer == 'c'){
  bar_2()

if(answer == 'd'){
  foo_3()

I have thought of using eval(), but is it a good approach? for example, creating an object consists of keys as conditions and function names as property.
conditions:{
  a: 'foo',
  b: 'bar',
  c: 'foo_2',
  d: 'bar_2',
}

and run it like eval(this.conditions[a]) 
But I also heard that using eval would be difficult for testing.
",0,1,0,1,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
43427877,43427877,Java code execution time issue,"<p>I have the following code modeling a lightweight framework for a vertex in my study of network diffusion. The initial prototype was from a framework in python, which I translated into Java. The issue I have is that while this code runs much faster than its python version up to 10000 vertices, for a larger number of vertices (100,000+), it grinds to a halt. In fact the python version executed in 1.2 minutes, while the java build didn't return even after 7 minutes of execution. I am not sure why the same code is breaking down at a larger number of vertices and I need help on fixing the code.</p>

<pre><code>import java.util.*;

public class Vertex
{
  private int id;
  private HashMap&lt;Integer, Double&gt; connectedTo;
  private int status;

  public Vertex(int key)
  {
    this.id = key;
    this.connectedTo = new HashMap&lt;Integer, Double&gt;();
    this.status = 0;
  }

  public void addNeighbour(int nbr, double weight)
  {
    this.connectedTo.put(nbr, weight);
  }

  public int getId()
  {
    return this.id;
  }

  public double getWeight(int nbr)
  {
    return this.connectedTo.get(nbr);
  }

  public int getStatus()
  {
    return this.status;
  }

  public Set&lt;Integer&gt; getConnections()
  {
    return this.connectedTo.keySet();
  }

//testing the class

  public static void main(String[] args)
  {
    int noOfVertices = 100000;

    Vertex[] vertexList = new Vertex[noOfVertices];

    for (int i = 0; i &lt; noOfVertices; i++) {
        vertexList[i] = new Vertex(i);
    }

    for (Vertex v : vertexList) {
        int degree = (int)(500*Math.random()); //random choice of degree 
        int neighbourCount = 0; // count number of neighbours built up

        while (neighbourCount &lt;= degree) {
            int nbr = (int) (noOfVertices * Math.random()); // randomly choose a neighbour
            double weight = Math.random(); // randomly assign a weight for the relationship
            v.addNeighbour(nbr, weight);
            neighbourCount++;
        }
    }

  }
}
</code></pre>

<p>For reference, the python version of this code is as follows:</p>

<pre><code>import random

class Vertex:
    def __init__(self, key):
      self.id = key
      self.connectedTo = {}

    def addNeighbor(self, nbr, weight=0):
      self.connectedTo[nbr] = weight

    def __str__(self):
      return str(self.id) + ' connectedTo: ' \
          + str([x.id for x in self.connectedTo])

    def getConnections(self):
      return self.connectedTo.keys()

    def getId(self):
      return self.id

    def getWeight(self, nbr):
      return self.connectedTo[nbr]

if __name__ == '__main__':
  numberOfVertices = 100000
  vertexList = [Vertex(i) for i in range(numberOfVertices)] # list of vertices

  for vertex in vertexList:
    degree = 500*random.random() 
    # build up neighbors one by one
    neighbourCount = 0 

    while neighbourCount &lt;= degree:
        neighbour = random.choice(range(numberOfVertices))
        weight = random.random() # random choice of weight
        vertex.addNeighbor(neighbour, weight)
        neighbourCount = neighbourCount + 1
</code></pre>
","i have the following code modeling a lightweight framework for a vertex in my study of network diffusion the initial prototype was from a framework in python which i translated into java the issue i have is that while this code runs much faster than its python version up to 10000 vertices for a larger number of vertices 100000 it grinds to a halt in fact the python version executed in 12 minutes while the java build didnt return even after 7 minutes of execution i am not sure why the same code is breaking down at a larger number of vertices and i need help on fixing the code
for reference the python version of this code is as follows
",1,"I have the following code modeling a lightweight framework for a vertex in my study of network diffusion. The initial prototype was from a framework in python, which I translated into Java. The issue I have is that while this code runs much faster than its python version up to 10000 vertices, for a larger number of vertices (100,000+), it grinds to a halt. In fact the python version executed in 1.2 minutes, while the java build didn't return even after 7 minutes of execution. I am not sure why the same code is breaking down at a larger number of vertices and I need help on fixing the code.
import java.util.*;

public class Vertex
{
  private int id;
  private HashMap<Integer, Double> connectedTo;
  private int status;

  public Vertex(int key)
  {
    this.id = key;
    this.connectedTo = new HashMap<Integer, Double>();
    this.status = 0;
  }

  public void addNeighbour(int nbr, double weight)
  {
    this.connectedTo.put(nbr, weight);
  }

  public int getId()
  {
    return this.id;
  }

  public double getWeight(int nbr)
  {
    return this.connectedTo.get(nbr);
  }

  public int getStatus()
  {
    return this.status;
  }

  public Set<Integer> getConnections()
  {
    return this.connectedTo.keySet();
  }

//testing the class

  public static void main(String[] args)
  {
    int noOfVertices = 100000;

    Vertex[] vertexList = new Vertex[noOfVertices];

    for (int i = 0; i < noOfVertices; i++) {
        vertexList[i] = new Vertex(i);
    }

    for (Vertex v : vertexList) {
        int degree = (int)(500*Math.random()); //random choice of degree 
        int neighbourCount = 0; // count number of neighbours built up

        while (neighbourCount <= degree) {
            int nbr = (int) (noOfVertices * Math.random()); // randomly choose a neighbour
            double weight = Math.random(); // randomly assign a weight for the relationship
            v.addNeighbour(nbr, weight);
            neighbourCount++;
        }
    }

  }
}

For reference, the python version of this code is as follows:
import random

class Vertex:
    def __init__(self, key):
      self.id = key
      self.connectedTo = {}

    def addNeighbor(self, nbr, weight=0):
      self.connectedTo[nbr] = weight

    def __str__(self):
      return str(self.id) + ' connectedTo: ' \
          + str([x.id for x in self.connectedTo])

    def getConnections(self):
      return self.connectedTo.keys()

    def getId(self):
      return self.id

    def getWeight(self, nbr):
      return self.connectedTo[nbr]

if __name__ == '__main__':
  numberOfVertices = 100000
  vertexList = [Vertex(i) for i in range(numberOfVertices)] # list of vertices

  for vertex in vertexList:
    degree = 500*random.random() 
    # build up neighbors one by one
    neighbourCount = 0 

    while neighbourCount <= degree:
        neighbour = random.choice(range(numberOfVertices))
        weight = random.random() # random choice of weight
        vertex.addNeighbor(neighbour, weight)
        neighbourCount = neighbourCount + 1

","Java code execution time issue
I have the following code modeling a lightweight framework for a vertex in my study of network diffusion. The initial prototype was from a framework in python, which I translated into Java. The issue I have is that while this code runs much faster than its python version up to 10000 vertices, for a larger number of vertices (100,000+), it grinds to a halt. In fact the python version executed in 1.2 minutes, while the java build didn't return even after 7 minutes of execution. I am not sure why the same code is breaking down at a larger number of vertices and I need help on fixing the code.
import java.util.*;

public class Vertex
{
  private int id;
  private HashMap<Integer, Double> connectedTo;
  private int status;

  public Vertex(int key)
  {
    this.id = key;
    this.connectedTo = new HashMap<Integer, Double>();
    this.status = 0;
  }

  public void addNeighbour(int nbr, double weight)
  {
    this.connectedTo.put(nbr, weight);
  }

  public int getId()
  {
    return this.id;
  }

  public double getWeight(int nbr)
  {
    return this.connectedTo.get(nbr);
  }

  public int getStatus()
  {
    return this.status;
  }

  public Set<Integer> getConnections()
  {
    return this.connectedTo.keySet();
  }

//testing the class

  public static void main(String[] args)
  {
    int noOfVertices = 100000;

    Vertex[] vertexList = new Vertex[noOfVertices];

    for (int i = 0; i < noOfVertices; i++) {
        vertexList[i] = new Vertex(i);
    }

    for (Vertex v : vertexList) {
        int degree = (int)(500*Math.random()); //random choice of degree 
        int neighbourCount = 0; // count number of neighbours built up

        while (neighbourCount <= degree) {
            int nbr = (int) (noOfVertices * Math.random()); // randomly choose a neighbour
            double weight = Math.random(); // randomly assign a weight for the relationship
            v.addNeighbour(nbr, weight);
            neighbourCount++;
        }
    }

  }
}

For reference, the python version of this code is as follows:
import random

class Vertex:
    def __init__(self, key):
      self.id = key
      self.connectedTo = {}

    def addNeighbor(self, nbr, weight=0):
      self.connectedTo[nbr] = weight

    def __str__(self):
      return str(self.id) + ' connectedTo: ' \
          + str([x.id for x in self.connectedTo])

    def getConnections(self):
      return self.connectedTo.keys()

    def getId(self):
      return self.id

    def getWeight(self, nbr):
      return self.connectedTo[nbr]

if __name__ == '__main__':
  numberOfVertices = 100000
  vertexList = [Vertex(i) for i in range(numberOfVertices)] # list of vertices

  for vertex in vertexList:
    degree = 500*random.random() 
    # build up neighbors one by one
    neighbourCount = 0 

    while neighbourCount <= degree:
        neighbour = random.choice(range(numberOfVertices))
        weight = random.random() # random choice of weight
        vertex.addNeighbor(neighbour, weight)
        neighbourCount = neighbourCount + 1

",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
1081040,1081040,Linked List: Is this solution good?,"<p>I was looking for a way to avoid starting from the head of the list each time I want to find a node, so I thought of assigning indexes to nodes, keeping a pointer to a random (not exactly random; see below) node and then finding the pointer that's closest to the index I want to find. Allow me to explain with code:</p>

<pre><code>// head and last are pointers to the first and last items of a doubly-linked list
// current is a pointer that will change over time. It's used as a temporary pointer
template &lt;class T&gt;a
Node&lt;T&gt;* List&lt;T&gt;::get_closest(Node&lt;T&gt; node, int&amp; difference) {
    int curr_to_i = current-&gt;index - node-&gt;index;
    int last_to_i = last-&gt;index - node-&gt;index;
    Node* closest = node-&gt;index &lt; abs(curr_to_i) ? head : current;
    closest = closest-&gt;index &lt; abs(last_to_i) ? closest : last;
    difference = closest-&gt;index - node-&gt;index;
    return closest;
}

/*
 * This functions adds a node with the given value to the given index. The node at that
 * index and all the following are moved, and the new node is inserted before them.
 */ 
template &lt;class T&gt;
bool List&lt;T&gt;::add(T value, int index) {
    if (index &lt; 0) { //Invalid index
        return false;
    } else if (index == last-&gt;index +1) {
        push(value);
        return true;
    } else if (index &gt; 0) {
        Node* new_n = new Node;
        new_n-&gt;value = value;
        new_n-&gt;index = index;
        int difference;
        Node* closest = get_closest(new_n, difference);
        if (difference &lt; 0) {
            for (int i = 0; i &lt; abs(difference); i++) {
                current = current-&gt;previous;
            }
        } else if (difference &gt; 0) {
                for (int i = 0; i &lt; abs(difference); i++) {
                current = current-&gt;next;
            }
        } /* current now points to the node we want to move */
        new_n-&gt;previous = current-&gt;previous;
        new_n-&gt;next = current;
        current-&gt;previous-&gt;next = new_n;
        current-&gt;previous = new_n;
        if (index == 0) {
            root = new_n;
        }
        new_n = new_n-&gt;next;
        while (new_n != null) {
            new_n-&gt;index++;
            new_n = new_n-&gt;next;
        }
        return true;        
    }
}
</code></pre>

<p>Is this more efficient than starting from the head and advancing forward a number of times?</p>
","i was looking for a way to avoid starting from the head of the list each time i want to find a node so i thought of assigning indexes to nodes keeping a pointer to a random not exactly random see below node and then finding the pointer thats closest to the index i want to find allow me to explain with code
is this more efficient than starting from the head and advancing forward a number of times
",1,"I was looking for a way to avoid starting from the head of the list each time I want to find a node, so I thought of assigning indexes to nodes, keeping a pointer to a random (not exactly random; see below) node and then finding the pointer that's closest to the index I want to find. Allow me to explain with code:
// head and last are pointers to the first and last items of a doubly-linked list
// current is a pointer that will change over time. It's used as a temporary pointer
template <class T>a
Node<T>* List<T>::get_closest(Node<T> node, int& difference) {
    int curr_to_i = current->index - node->index;
    int last_to_i = last->index - node->index;
    Node* closest = node->index < abs(curr_to_i) ? head : current;
    closest = closest->index < abs(last_to_i) ? closest : last;
    difference = closest->index - node->index;
    return closest;
}

/*
 * This functions adds a node with the given value to the given index. The node at that
 * index and all the following are moved, and the new node is inserted before them.
 */ 
template <class T>
bool List<T>::add(T value, int index) {
    if (index < 0) { //Invalid index
        return false;
    } else if (index == last->index +1) {
        push(value);
        return true;
    } else if (index > 0) {
        Node* new_n = new Node;
        new_n->value = value;
        new_n->index = index;
        int difference;
        Node* closest = get_closest(new_n, difference);
        if (difference < 0) {
            for (int i = 0; i < abs(difference); i++) {
                current = current->previous;
            }
        } else if (difference > 0) {
                for (int i = 0; i < abs(difference); i++) {
                current = current->next;
            }
        } /* current now points to the node we want to move */
        new_n->previous = current->previous;
        new_n->next = current;
        current->previous->next = new_n;
        current->previous = new_n;
        if (index == 0) {
            root = new_n;
        }
        new_n = new_n->next;
        while (new_n != null) {
            new_n->index++;
            new_n = new_n->next;
        }
        return true;        
    }
}

Is this more efficient than starting from the head and advancing forward a number of times?
","Linked List: Is this solution good?
I was looking for a way to avoid starting from the head of the list each time I want to find a node, so I thought of assigning indexes to nodes, keeping a pointer to a random (not exactly random; see below) node and then finding the pointer that's closest to the index I want to find. Allow me to explain with code:
// head and last are pointers to the first and last items of a doubly-linked list
// current is a pointer that will change over time. It's used as a temporary pointer
template <class T>a
Node<T>* List<T>::get_closest(Node<T> node, int& difference) {
    int curr_to_i = current->index - node->index;
    int last_to_i = last->index - node->index;
    Node* closest = node->index < abs(curr_to_i) ? head : current;
    closest = closest->index < abs(last_to_i) ? closest : last;
    difference = closest->index - node->index;
    return closest;
}

/*
 * This functions adds a node with the given value to the given index. The node at that
 * index and all the following are moved, and the new node is inserted before them.
 */ 
template <class T>
bool List<T>::add(T value, int index) {
    if (index < 0) { //Invalid index
        return false;
    } else if (index == last->index +1) {
        push(value);
        return true;
    } else if (index > 0) {
        Node* new_n = new Node;
        new_n->value = value;
        new_n->index = index;
        int difference;
        Node* closest = get_closest(new_n, difference);
        if (difference < 0) {
            for (int i = 0; i < abs(difference); i++) {
                current = current->previous;
            }
        } else if (difference > 0) {
                for (int i = 0; i < abs(difference); i++) {
                current = current->next;
            }
        } /* current now points to the node we want to move */
        new_n->previous = current->previous;
        new_n->next = current;
        current->previous->next = new_n;
        current->previous = new_n;
        if (index == 0) {
            root = new_n;
        }
        new_n = new_n->next;
        while (new_n != null) {
            new_n->index++;
            new_n = new_n->next;
        }
        return true;        
    }
}

Is this more efficient than starting from the head and advancing forward a number of times?
",0,1,1,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,1,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
48866864,48866864,Return by value vs return by reference?,"<p>I wish to seek a bit of feedback (to help me see if my position on the issue is correct or not) regarding returning by <strong>const reference</strong> vs. returning by <strong>value</strong> (for performance reasons).</p>

<p>To elaborate:</p>

<pre><code>#include ""BigDataStruct.h""

class DataGiver
{
  public:
    BigDataStruct getByValue() const { return myData; };
    const BigDataStruct&amp; getByConstRef() const { return myData; };

  private:
    BigDataStruct myData;
}
</code></pre>

<p>My gut feeling says there may (or may not) be a slight performance increase for <strong><code>getByConstRef()</code></strong> over <strong><code>getByValue()</code></strong> in situations like:</p>

<pre><code>const BigDataStruct&amp; someData = someDataGiver.getByConstRef();
</code></pre>

<p>but there won't be (hardly) any for calls like:</p>

<pre><code>BigDataStruct someData = someDataGiver.getByConstRef();
</code></pre>

<p>Also - which gives me some discomfort - is that in the first scenario <em>someData</em> is tied to actual member in <em>someDataGiver</em>, in the sense that if the member there changes <em>someData</em> also changes and if <em>someDataGiver</em> expires/dies <em>someData</em> becomes... undefined?</p>

<p>As such I try to discourage my coworkers from returning by const reference (unless we actually WANT a reference to the object for other reasons - e.g. in singleton factories). Do I have a point here or am I merely nitpicking and annoying my coworkers for no good reason?</p>

<p>(I am aware of the rule of thumb for optimization: 1) Don't do it 2) Don't do it yet - and use a profiler before actually doing it)</p>
","i wish to seek a bit of feedback to help me see if my position on the issue is correct or not regarding returning by const reference vs returning by value for performance reasons
to elaborate
my gut feeling says there may or may not be a slight performance increase for getbyconstref over getbyvalue in situations like
but there wont be hardly any for calls like
also  which gives me some discomfort  is that in the first scenario somedata is tied to actual member in somedatagiver in the sense that if the member there changes somedata also changes and if somedatagiver expiresdies somedata becomes undefined
as such i try to discourage my coworkers from returning by const reference unless we actually want a reference to the object for other reasons  eg in singleton factories do i have a point here or am i merely nitpicking and annoying my coworkers for no good reason
i am aware of the rule of thumb for optimization 1 dont do it 2 dont do it yet  and use a profiler before actually doing it
",1,"I wish to seek a bit of feedback (to help me see if my position on the issue is correct or not) regarding returning by const reference vs. returning by value (for performance reasons).
To elaborate:
#include ""BigDataStruct.h""

class DataGiver
{
  public:
    BigDataStruct getByValue() const { return myData; };
    const BigDataStruct& getByConstRef() const { return myData; };

  private:
    BigDataStruct myData;
}

My gut feeling says there may (or may not) be a slight performance increase for getByConstRef() over getByValue() in situations like:
const BigDataStruct& someData = someDataGiver.getByConstRef();

but there won't be (hardly) any for calls like:
BigDataStruct someData = someDataGiver.getByConstRef();

Also - which gives me some discomfort - is that in the first scenario someData is tied to actual member in someDataGiver, in the sense that if the member there changes someData also changes and if someDataGiver expires/dies someData becomes... undefined?
As such I try to discourage my coworkers from returning by const reference (unless we actually WANT a reference to the object for other reasons - e.g. in singleton factories). Do I have a point here or am I merely nitpicking and annoying my coworkers for no good reason?
(I am aware of the rule of thumb for optimization: 1) Don't do it 2) Don't do it yet - and use a profiler before actually doing it)
","Return by value vs return by reference?
I wish to seek a bit of feedback (to help me see if my position on the issue is correct or not) regarding returning by const reference vs. returning by value (for performance reasons).
To elaborate:
#include ""BigDataStruct.h""

class DataGiver
{
  public:
    BigDataStruct getByValue() const { return myData; };
    const BigDataStruct& getByConstRef() const { return myData; };

  private:
    BigDataStruct myData;
}

My gut feeling says there may (or may not) be a slight performance increase for getByConstRef() over getByValue() in situations like:
const BigDataStruct& someData = someDataGiver.getByConstRef();

but there won't be (hardly) any for calls like:
BigDataStruct someData = someDataGiver.getByConstRef();

Also - which gives me some discomfort - is that in the first scenario someData is tied to actual member in someDataGiver, in the sense that if the member there changes someData also changes and if someDataGiver expires/dies someData becomes... undefined?
As such I try to discourage my coworkers from returning by const reference (unless we actually WANT a reference to the object for other reasons - e.g. in singleton factories). Do I have a point here or am I merely nitpicking and annoying my coworkers for no good reason?
(I am aware of the rule of thumb for optimization: 1) Don't do it 2) Don't do it yet - and use a profiler before actually doing it)
",0,0,1,1,0,0,0,- Is a theoretical question,"Label: 1

Reasoning: This question expresses technical debt because it highlights a design flaw and potential performance issue in the code. The user is concerned about the performance implications of returning by const reference versus returning by value and is seeking feedback on their stance. Specifically, they are worried about the potential benefits of returning by const reference in certain scenarios and the potential pitfalls, such as the returned reference becoming undefined if the original object expires.

The user's gut feeling about the performance difference between the two approaches and their discomfort with the potential risks associated with returning by const reference indicate that they are aware of the suboptimal nature of their current implementation. They are also considering the maintainability aspect, as they are concerned about the impact of their design choice on their coworkers' code.

Furthermore, the user mentions the rule of thumb for optimization, implying that they are aware of the importance of performance optimization and are taking a proactive approach to address potential issues. This suggests that they are acknowledging the technical debt in their current implementation and are seeking to improve it.

Overall, the question demonstrates a recognition of the need for improvement in the code's design and performance, which is a hallmark of technical debt.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,1,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
23969334,23969334,MemoryCache OutOfMemoryException,"<p>I am trying to figure out how the MemoryCache should be used in order to avoid getting out of memory exceptions. I come from ASP.Net background where the cache manages it's own memory usage so I expect that MemoryCache would do the same. This does not appear to be the case as illustrated in the bellow test program I made:</p>

<pre><code>class Program
{
    static void Main(string[] args)
    {
        var cache = new MemoryCache(""Cache"");

        for (int i = 0; i &lt; 100000; i++)
        {
            AddToCache(cache, i);
        }


        Console.ReadLine();
    }

    private static void AddToCache(MemoryCache cache, int i)
    {
        var key = ""File:"" + i;
        var contents = System.IO.File.ReadAllBytes(""File.txt"");
        var policy = new CacheItemPolicy
        {
            SlidingExpiration = TimeSpan.FromHours(12)
        };

        policy.ChangeMonitors.Add(
                new HostFileChangeMonitor(
                    new[] { Path.GetFullPath(""File.txt"") }
                    .ToList()));

        cache.Add(key, contents, policy);
        Console.Clear();
        Console.Write(i);
    }        
}
</code></pre>

<p>The above throws an out of memory exception after approximately reaching 2GB of memory usage (Any CPU) or after consuming all my machine's physical memory (x64)(16GB). </p>

<p>If I remove the cache.Add bit the program throws no exception. If I include a call to cache.Trim(5) after every cache add I see that it releases some memory and it keeps aproximately 150 objects in the cache at any given time (from cache.GetCount()). </p>

<p>Is calling cache.Trim my program's responsibility? If so when should it be called (like how can my program know that the memory is getting full)? How do you calculate the percentage argument?</p>

<p>Note: I am planning to use the MemoryCache in a long running windows service so it is critical for it to have proper memory management.</p>
","i am trying to figure out how the memorycache should be used in order to avoid getting out of memory exceptions i come from aspnet background where the cache manages its own memory usage so i expect that memorycache would do the same this does not appear to be the case as illustrated in the bellow test program i made
the above throws an out of memory exception after approximately reaching 2gb of memory usage any cpu or after consuming all my machines physical memory x6416gb 
if i remove the cacheadd bit the program throws no exception if i include a call to cachetrim5 after every cache add i see that it releases some memory and it keeps aproximately 150 objects in the cache at any given time from cachegetcount 
is calling cachetrim my programs responsibility if so when should it be called like how can my program know that the memory is getting full how do you calculate the percentage argument
note i am planning to use the memorycache in a long running windows service so it is critical for it to have proper memory management
",0,"I am trying to figure out how the MemoryCache should be used in order to avoid getting out of memory exceptions. I come from ASP.Net background where the cache manages it's own memory usage so I expect that MemoryCache would do the same. This does not appear to be the case as illustrated in the bellow test program I made:
class Program
{
    static void Main(string[] args)
    {
        var cache = new MemoryCache(""Cache"");

        for (int i = 0; i < 100000; i++)
        {
            AddToCache(cache, i);
        }


        Console.ReadLine();
    }

    private static void AddToCache(MemoryCache cache, int i)
    {
        var key = ""File:"" + i;
        var contents = System.IO.File.ReadAllBytes(""File.txt"");
        var policy = new CacheItemPolicy
        {
            SlidingExpiration = TimeSpan.FromHours(12)
        };

        policy.ChangeMonitors.Add(
                new HostFileChangeMonitor(
                    new[] { Path.GetFullPath(""File.txt"") }
                    .ToList()));

        cache.Add(key, contents, policy);
        Console.Clear();
        Console.Write(i);
    }        
}

The above throws an out of memory exception after approximately reaching 2GB of memory usage (Any CPU) or after consuming all my machine's physical memory (x64)(16GB). 
If I remove the cache.Add bit the program throws no exception. If I include a call to cache.Trim(5) after every cache add I see that it releases some memory and it keeps aproximately 150 objects in the cache at any given time (from cache.GetCount()). 
Is calling cache.Trim my program's responsibility? If so when should it be called (like how can my program know that the memory is getting full)? How do you calculate the percentage argument?
Note: I am planning to use the MemoryCache in a long running windows service so it is critical for it to have proper memory management.
","MemoryCache OutOfMemoryException
I am trying to figure out how the MemoryCache should be used in order to avoid getting out of memory exceptions. I come from ASP.Net background where the cache manages it's own memory usage so I expect that MemoryCache would do the same. This does not appear to be the case as illustrated in the bellow test program I made:
class Program
{
    static void Main(string[] args)
    {
        var cache = new MemoryCache(""Cache"");

        for (int i = 0; i < 100000; i++)
        {
            AddToCache(cache, i);
        }


        Console.ReadLine();
    }

    private static void AddToCache(MemoryCache cache, int i)
    {
        var key = ""File:"" + i;
        var contents = System.IO.File.ReadAllBytes(""File.txt"");
        var policy = new CacheItemPolicy
        {
            SlidingExpiration = TimeSpan.FromHours(12)
        };

        policy.ChangeMonitors.Add(
                new HostFileChangeMonitor(
                    new[] { Path.GetFullPath(""File.txt"") }
                    .ToList()));

        cache.Add(key, contents, policy);
        Console.Clear();
        Console.Write(i);
    }        
}

The above throws an out of memory exception after approximately reaching 2GB of memory usage (Any CPU) or after consuming all my machine's physical memory (x64)(16GB). 
If I remove the cache.Add bit the program throws no exception. If I include a call to cache.Trim(5) after every cache add I see that it releases some memory and it keeps aproximately 150 objects in the cache at any given time (from cache.GetCount()). 
Is calling cache.Trim my program's responsibility? If so when should it be called (like how can my program know that the memory is getting full)? How do you calculate the percentage argument?
Note: I am planning to use the MemoryCache in a long running windows service so it is critical for it to have proper memory management.
",0,1,0,0,0,0,1,- Code not working as intended,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,1,0,0,0,"Change: Conceptual to 1
Reason: Involves understanding regarding how the memory is working",,Infrastructure,Infrastructure,,Infrastructure,Very Severe,Very Severe,,Very Severe
47630871,47630871,C LuaJit Profiling,"<p>I have following execution stack:</p>
<blockquote>
<ol>
<li><p>C++</p>
</li>
<li><p>C Lua API</p>
</li>
<li><p>LuaJIT</p>
</li>
<li><p>C functions - my own functions,  injected to Lua via C-Lua-API which called from lua script</p>
</li>
</ol>
</blockquote>
<p>I want to profiling this code. And I want to know percentage time spent in luajit phase.</p>
<p>I see come problems here. If I will compile luajit with flags something like -pg -fprofile-generate -fprofile-use it will be not the same that  luajit compiled for production build.</p>
<p>Is there any way to figure out what's going on in LUAJit phase?</p>
<p>Good example of this executions stack - <a href=""https://github.com/openresty/lua-nginx-module"" rel=""nofollow noreferrer"">openresty</a></p>
","i have following execution stack


c

c lua api

luajit

c functions  my own functions  injected to lua via cluaapi which called from lua script



i want to profiling this code and i want to know percentage time spent in luajit phase
i see come problems here if i will compile luajit with flags something like pg fprofilegenerate fprofileuse it will be not the same that  luajit compiled for production build
is there any way to figure out whats going on in luajit phase
good example of this executions stack  openresty
",1,"I have following execution stack:


C++

C Lua API

LuaJIT

C functions - my own functions,  injected to Lua via C-Lua-API which called from lua script



I want to profiling this code. And I want to know percentage time spent in luajit phase.
I see come problems here. If I will compile luajit with flags something like -pg -fprofile-generate -fprofile-use it will be not the same that  luajit compiled for production build.
Is there any way to figure out what's going on in LUAJit phase?
Good example of this executions stack - openresty
","C LuaJit Profiling
I have following execution stack:


C++

C Lua API

LuaJIT

C functions - my own functions,  injected to Lua via C-Lua-API which called from lua script



I want to profiling this code. And I want to know percentage time spent in luajit phase.
I see come problems here. If I will compile luajit with flags something like -pg -fprofile-generate -fprofile-use it will be not the same that  luajit compiled for production build.
Is there any way to figure out what's going on in LUAJit phase?
Good example of this executions stack - openresty
",0,1,0,1,0,0,0,- User just doesn't know how to approach a problem,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Test,Build,User’s current compiling process is having issues and slowing down development ,Build,Mild,Mild,,Mild
27771199,27771199,What happens if you apply delete[] to a non-array pointer?,"<p>If you apply ""delete"" instead of ""delete[]"" to an array in C++, then you will create a memory leak because only the first element will be deleted. But what happens if you apply ""delete[]"" to a non-array pointer by accident? For instance (just a toy example):</p>

<pre><code>   class X {   
      void  foo()
      {
          X* x = new X();
          delete[] x; // What happens here?
      }
   };
</code></pre>

<p>Can it do any harm? And if so, what harm?</p>
","if you apply delete instead of delete to an array in c then you will create a memory leak because only the first element will be deleted but what happens if you apply delete to a nonarray pointer by accident for instance just a toy example
can it do any harm and if so what harm
",0,"If you apply ""delete"" instead of ""delete[]"" to an array in C++, then you will create a memory leak because only the first element will be deleted. But what happens if you apply ""delete[]"" to a non-array pointer by accident? For instance (just a toy example):
   class X {   
      void  foo()
      {
          X* x = new X();
          delete[] x; // What happens here?
      }
   };

Can it do any harm? And if so, what harm?
","What happens if you apply delete[] to a non-array pointer?
If you apply ""delete"" instead of ""delete[]"" to an array in C++, then you will create a memory leak because only the first element will be deleted. But what happens if you apply ""delete[]"" to a non-array pointer by accident? For instance (just a toy example):
   class X {   
      void  foo()
      {
          X* x = new X();
          delete[] x; // What happens here?
      }
   };

Can it do any harm? And if so, what harm?
",0,0,1,0,0,0,0,,,,1,1,Memory Management,Memory Management,,Memory Management,0,0,1,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
31041172,31041172,When and Why to use Refactor>Reorder Parameters,"<p>I am trying to search the answer of this simple question that
In Visual Studio we have a technique 'refactor' sub type Reorder Parameters</p>

<p>When and why do we need to reorder parameters of a certain method? What is the scenario where one has to change the order of parameters of function</p>
","i am trying to search the answer of this simple question that
in visual studio we have a technique refactor sub type reorder parameters
when and why do we need to reorder parameters of a certain method what is the scenario where one has to change the order of parameters of function
",1,"I am trying to search the answer of this simple question that
In Visual Studio we have a technique 'refactor' sub type Reorder Parameters
When and why do we need to reorder parameters of a certain method? What is the scenario where one has to change the order of parameters of function
","When and Why to use Refactor>Reorder Parameters
I am trying to search the answer of this simple question that
In Visual Studio we have a technique 'refactor' sub type Reorder Parameters
When and why do we need to reorder parameters of a certain method? What is the scenario where one has to change the order of parameters of function
",0,0,1,1,0,0,0,- Is a theoretical question,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,0,1,1,0,0,,,Design,Design,,Design,Very Mild,Very Mild,,Very Mild
69245336,69245336,Should I avoid nested functions?,"<p>Is this tree <strong>nowadays</strong></p>
<p>&quot;Try to avoid nested functions because they will always be <strong>lazily parsed</strong>, so they will be parsed repeatedly, and this will affect the performance&quot;</p>
<p>so, should I use this</p>
<pre><code>function fun() {
    function foo() {}
    foo();
}
</code></pre>
<p>or this</p>
<pre><code>function foo() {}
function fun() {
    foo();
}
</code></pre>
<p><strong>Which one is better in terms of performance?</strong></p>
","is this tree nowadays
try to avoid nested functions because they will always be lazily parsed so they will be parsed repeatedly and this will affect the performance
so should i use this
or this
which one is better in terms of performance
",1,"Is this tree nowadays
""Try to avoid nested functions because they will always be lazily parsed, so they will be parsed repeatedly, and this will affect the performance""
so, should I use this
function fun() {
    function foo() {}
    foo();
}

or this
function foo() {}
function fun() {
    foo();
}

Which one is better in terms of performance?
","Should I avoid nested functions?
Is this tree nowadays
""Try to avoid nested functions because they will always be lazily parsed, so they will be parsed repeatedly, and this will affect the performance""
so, should I use this
function fun() {
    function foo() {}
    foo();
}

or this
function foo() {}
function fun() {
    foo();
}

Which one is better in terms of performance?
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
48989632,48989632,"Use dlib's `find_max_global` in python, with changing number of parameters","<p>Dlib has recently released a new method for global optimization, minimizing the number of function calls (<a href=""http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html"" rel=""nofollow noreferrer"">http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html</a>)</p>

<p>I would like to use this in Python, and for a simple cost function it works well:</p>

<pre><code>import dlib
import numpy as np

def cost_fn_2_params(x0, x1):
    return np.sum([x0, x1])

dlib.find_max_global(f=cost_fn_2_params, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)
</code></pre>

<p>Now, in my particular use-case I have a variable number of parameters going in to my cost function, and <code>find_global_max</code> expects the cost function to have a static number of arguments. For example, defining the cost function in another way:</p>

<pre><code>def cost_fn_unspecified_params(**params):
    return np.sum(params)

dlib.find_max_global(f=cost_fn_unspecified_params, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)
</code></pre>

<p>I get the following error:</p>

<pre><code>Failing expression was num == args.size().
The function being optimized takes a number of arguments that doesn't agree with the size of the bounds lists you provided to find_max_global()
</code></pre>

<p>My question is: Is there any way to dynamically define functions with a certain number of arguments? Excluding the <code>*params</code> syntax as this apparently looks like a single argument to <code>dlib</code>.</p>

<p>I have considered using <code>exec</code> to define a function, but it seems overly complicated. This is my best solution so far:</p>

<pre><code>def get_fn_n_arguments(n):
    argument_list = [f'x{ii}' for ii in range(n)]
    argument_str = ','.join(argument_list)
    def_str = f'def fn({argument_str}): \n\tparams = [{argument_str}]\n\treturn cost_fn_unspecified_params(params)'
    print(def_str)
    exec(def_str, globals())
</code></pre>

<p>This puts a function <code>fn</code> into the global namespace, and now I can do </p>

<pre><code>get_fn_n_arguments(2)
dlib.find_max_global(f=fn, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)
</code></pre>

<p>It works, but it seems like a very brittle solutions. I would really like to improve it.</p>
","dlib has recently released a new method for global optimization minimizing the number of function calls httpblogdlibnet201712aglobaloptimizationalgorithmworthhtml
i would like to use this in python and for a simple cost function it works well
now in my particular usecase i have a variable number of parameters going in to my cost function and find_global_max expects the cost function to have a static number of arguments for example defining the cost function in another way
i get the following error
my question is is there any way to dynamically define functions with a certain number of arguments excluding the params syntax as this apparently looks like a single argument to dlib
i have considered using exec to define a function but it seems overly complicated this is my best solution so far
this puts a function fn into the global namespace and now i can do 
it works but it seems like a very brittle solutions i would really like to improve it
",1,"Dlib has recently released a new method for global optimization, minimizing the number of function calls (http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html)
I would like to use this in Python, and for a simple cost function it works well:
import dlib
import numpy as np

def cost_fn_2_params(x0, x1):
    return np.sum([x0, x1])

dlib.find_max_global(f=cost_fn_2_params, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)

Now, in my particular use-case I have a variable number of parameters going in to my cost function, and find_global_max expects the cost function to have a static number of arguments. For example, defining the cost function in another way:
def cost_fn_unspecified_params(**params):
    return np.sum(params)

dlib.find_max_global(f=cost_fn_unspecified_params, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)

I get the following error:
Failing expression was num == args.size().
The function being optimized takes a number of arguments that doesn't agree with the size of the bounds lists you provided to find_max_global()

My question is: Is there any way to dynamically define functions with a certain number of arguments? Excluding the *params syntax as this apparently looks like a single argument to dlib.
I have considered using exec to define a function, but it seems overly complicated. This is my best solution so far:
def get_fn_n_arguments(n):
    argument_list = [f'x{ii}' for ii in range(n)]
    argument_str = ','.join(argument_list)
    def_str = f'def fn({argument_str}): \n\tparams = [{argument_str}]\n\treturn cost_fn_unspecified_params(params)'
    print(def_str)
    exec(def_str, globals())

This puts a function fn into the global namespace, and now I can do 
get_fn_n_arguments(2)
dlib.find_max_global(f=fn, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)

It works, but it seems like a very brittle solutions. I would really like to improve it.
","Use dlib's `find_max_global` in python, with changing number of parameters
Dlib has recently released a new method for global optimization, minimizing the number of function calls (http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html)
I would like to use this in Python, and for a simple cost function it works well:
import dlib
import numpy as np

def cost_fn_2_params(x0, x1):
    return np.sum([x0, x1])

dlib.find_max_global(f=cost_fn_2_params, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)

Now, in my particular use-case I have a variable number of parameters going in to my cost function, and find_global_max expects the cost function to have a static number of arguments. For example, defining the cost function in another way:
def cost_fn_unspecified_params(**params):
    return np.sum(params)

dlib.find_max_global(f=cost_fn_unspecified_params, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)

I get the following error:
Failing expression was num == args.size().
The function being optimized takes a number of arguments that doesn't agree with the size of the bounds lists you provided to find_max_global()

My question is: Is there any way to dynamically define functions with a certain number of arguments? Excluding the *params syntax as this apparently looks like a single argument to dlib.
I have considered using exec to define a function, but it seems overly complicated. This is my best solution so far:
def get_fn_n_arguments(n):
    argument_list = [f'x{ii}' for ii in range(n)]
    argument_str = ','.join(argument_list)
    def_str = f'def fn({argument_str}): \n\tparams = [{argument_str}]\n\treturn cost_fn_unspecified_params(params)'
    print(def_str)
    exec(def_str, globals())

This puts a function fn into the global namespace, and now I can do 
get_fn_n_arguments(2)
dlib.find_max_global(f=fn, bound1=[-1., -1], 
                     bound2=[1., 1], num_function_calls=10)

It works, but it seems like a very brittle solutions. I would really like to improve it.
",0,1,0,1,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,1,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
74075122,74075122,"The most efficient way rather than using np.setdiff1d and np.in1d, to remove common values of 1D arrays with unique values","<p>I need a much faster code to remove values of an 1D array (array length ~ 10-15) that are common with another 1D array (array length ~ 1e5-5e5 --&gt; <em>rarely</em> up to 7e5), which are index arrays contain integers. There is no duplicate in the arrays, and they are not sorted and the order of the values must be kept in the main array after modification. I know that can be achieved using such <code>np.setdiff1d</code> or <code>np.in1d</code> (<em>which both are not supported for numba jitted in no-python mode</em>), and other similar posts (e.g. <a href=""https://stackoverflow.com/q/27967914/13394817"">this</a>) have not much more efficient way to do so, but performance is important here because all the values in the main index array will be gradually be removed in loops.</p>
<pre><code>import numpy as np
import numba as nb

n = 500000
r = 10
arr1 = np.random.permutation(n)
arr2 = np.random.randint(0, n, r)

# @nb.jit
def setdif1d_np(a, b):
    return np.setdiff1d(a, b, assume_unique=True)


# @nb.jit
def setdif1d_in1d_np(a, b):
    return a[~np.in1d(a, b)]
</code></pre>
<p>There is another related post that proposed by <a href=""https://stackoverflow.com/a/66674679/13394817""><em>norok2</em></a> for 2D arrays, that is ~15 times faster solution (hashing-like way using numba) than usual methods described there. This solution may be the best if it could be prepared for 1D arrays:</p>
<pre><code>@nb.njit
def mul_xor_hash(arr, init=65537, k=37):
    result = init
    for x in arr.view(np.uint64):
        result = (result * k) ^ x
    return result


@nb.njit
def setdiff2d_nb(arr1, arr2):
    # : build `delta` set using hashes
    delta = {mul_xor_hash(arr2[0])}
    for i in range(1, arr2.shape[0]):
        delta.add(mul_xor_hash(arr2[i]))
    # : compute the size of the result
    n = 0
    for i in range(arr1.shape[0]):
        if mul_xor_hash(arr1[i]) not in delta:
            n += 1
    # : build the result
    result = np.empty((n, arr1.shape[-1]), dtype=arr1.dtype)
    j = 0
    for i in range(arr1.shape[0]):
        if mul_xor_hash(arr1[i]) not in delta:
            result[j] = arr1[i]
            j += 1
    return result
</code></pre>
<p>I tried to prepare that for 1D arrays, but I have some problems/question with that.</p>
<ul>
<li>At first, IDU what does <code>mul_xor_hash</code> exactly do, and if <code>init</code> and <code>k</code> are arbitrary selected or not</li>
<li>Why <code>mul_xor_hash</code> will not work without <code>nb.njit</code>:</li>
</ul>
<pre><code>  File &quot;C:/Users/Ali/Desktop/test - Copy - Copy.py&quot;, line 21, in mul_xor_hash
    result = (result * k) ^ x
TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>
<ul>
<li>IDK how to implement <code>mul_xor_hash</code> on 1D arrays (if it could), which I guess may make it faster more than for 2Ds, so I broadcast the input arrays to 2D by <code>[None, :]</code>, which get the following error just for <code>arr2</code>:</li>
</ul>
<pre><code>    print(mul_xor_hash(arr2[0]))
ValueError: new type not compatible with array
</code></pre>
<ul>
<li>and what does <code>delta</code> do</li>
</ul>
<p>I am searching the most efficient way in this regard. In the absence of better method than <em>norok2 solution</em>, how to prepare this solution for 1D arrays?</p>
","i need a much faster code to remove values of an 1d array array length  1015 that are common with another 1d array array length  1e55e5  rarely up to 7e5 which are index arrays contain integers there is no duplicate in the arrays and they are not sorted and the order of the values must be kept in the main array after modification i know that can be achieved using such npsetdiff1d or npin1d which both are not supported for numba jitted in nopython mode and other similar posts eg this have not much more efficient way to do so but performance is important here because all the values in the main index array will be gradually be removed in loops
there is another related post that proposed by norok2 for 2d arrays that is 15 times faster solution hashinglike way using numba than usual methods described there this solution may be the best if it could be prepared for 1d arrays
i tried to prepare that for 1d arrays but i have some problemsquestion with that

at first idu what does mul_xor_hash exactly do and if init and k are arbitrary selected or not
why mul_xor_hash will not work without nbnjit


idk how to implement mul_xor_hash on 1d arrays if it could which i guess may make it faster more than for 2ds so i broadcast the input arrays to 2d by none  which get the following error just for arr2


and what does delta do

i am searching the most efficient way in this regard in the absence of better method than norok2 solution how to prepare this solution for 1d arrays
",1,"I need a much faster code to remove values of an 1D array (array length ~ 10-15) that are common with another 1D array (array length ~ 1e5-5e5 --> rarely up to 7e5), which are index arrays contain integers. There is no duplicate in the arrays, and they are not sorted and the order of the values must be kept in the main array after modification. I know that can be achieved using such np.setdiff1d or np.in1d (which both are not supported for numba jitted in no-python mode), and other similar posts (e.g. this) have not much more efficient way to do so, but performance is important here because all the values in the main index array will be gradually be removed in loops.
import numpy as np
import numba as nb

n = 500000
r = 10
arr1 = np.random.permutation(n)
arr2 = np.random.randint(0, n, r)

# @nb.jit
def setdif1d_np(a, b):
    return np.setdiff1d(a, b, assume_unique=True)


# @nb.jit
def setdif1d_in1d_np(a, b):
    return a[~np.in1d(a, b)]

There is another related post that proposed by norok2 for 2D arrays, that is ~15 times faster solution (hashing-like way using numba) than usual methods described there. This solution may be the best if it could be prepared for 1D arrays:
@nb.njit
def mul_xor_hash(arr, init=65537, k=37):
    result = init
    for x in arr.view(np.uint64):
        result = (result * k) ^ x
    return result


@nb.njit
def setdiff2d_nb(arr1, arr2):
    # : build `delta` set using hashes
    delta = {mul_xor_hash(arr2[0])}
    for i in range(1, arr2.shape[0]):
        delta.add(mul_xor_hash(arr2[i]))
    # : compute the size of the result
    n = 0
    for i in range(arr1.shape[0]):
        if mul_xor_hash(arr1[i]) not in delta:
            n += 1
    # : build the result
    result = np.empty((n, arr1.shape[-1]), dtype=arr1.dtype)
    j = 0
    for i in range(arr1.shape[0]):
        if mul_xor_hash(arr1[i]) not in delta:
            result[j] = arr1[i]
            j += 1
    return result

I tried to prepare that for 1D arrays, but I have some problems/question with that.

At first, IDU what does mul_xor_hash exactly do, and if init and k are arbitrary selected or not
Why mul_xor_hash will not work without nb.njit:

  File ""C:/Users/Ali/Desktop/test - Copy - Copy.py"", line 21, in mul_xor_hash
    result = (result * k) ^ x
TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''


IDK how to implement mul_xor_hash on 1D arrays (if it could), which I guess may make it faster more than for 2Ds, so I broadcast the input arrays to 2D by [None, :], which get the following error just for arr2:

    print(mul_xor_hash(arr2[0]))
ValueError: new type not compatible with array


and what does delta do

I am searching the most efficient way in this regard. In the absence of better method than norok2 solution, how to prepare this solution for 1D arrays?
","The most efficient way rather than using np.setdiff1d and np.in1d, to remove common values of 1D arrays with unique values
I need a much faster code to remove values of an 1D array (array length ~ 10-15) that are common with another 1D array (array length ~ 1e5-5e5 --> rarely up to 7e5), which are index arrays contain integers. There is no duplicate in the arrays, and they are not sorted and the order of the values must be kept in the main array after modification. I know that can be achieved using such np.setdiff1d or np.in1d (which both are not supported for numba jitted in no-python mode), and other similar posts (e.g. this) have not much more efficient way to do so, but performance is important here because all the values in the main index array will be gradually be removed in loops.
import numpy as np
import numba as nb

n = 500000
r = 10
arr1 = np.random.permutation(n)
arr2 = np.random.randint(0, n, r)

# @nb.jit
def setdif1d_np(a, b):
    return np.setdiff1d(a, b, assume_unique=True)


# @nb.jit
def setdif1d_in1d_np(a, b):
    return a[~np.in1d(a, b)]

There is another related post that proposed by norok2 for 2D arrays, that is ~15 times faster solution (hashing-like way using numba) than usual methods described there. This solution may be the best if it could be prepared for 1D arrays:
@nb.njit
def mul_xor_hash(arr, init=65537, k=37):
    result = init
    for x in arr.view(np.uint64):
        result = (result * k) ^ x
    return result


@nb.njit
def setdiff2d_nb(arr1, arr2):
    # : build `delta` set using hashes
    delta = {mul_xor_hash(arr2[0])}
    for i in range(1, arr2.shape[0]):
        delta.add(mul_xor_hash(arr2[i]))
    # : compute the size of the result
    n = 0
    for i in range(arr1.shape[0]):
        if mul_xor_hash(arr1[i]) not in delta:
            n += 1
    # : build the result
    result = np.empty((n, arr1.shape[-1]), dtype=arr1.dtype)
    j = 0
    for i in range(arr1.shape[0]):
        if mul_xor_hash(arr1[i]) not in delta:
            result[j] = arr1[i]
            j += 1
    return result

I tried to prepare that for 1D arrays, but I have some problems/question with that.

At first, IDU what does mul_xor_hash exactly do, and if init and k are arbitrary selected or not
Why mul_xor_hash will not work without nb.njit:

  File ""C:/Users/Ali/Desktop/test - Copy - Copy.py"", line 21, in mul_xor_hash
    result = (result * k) ^ x
TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''


IDK how to implement mul_xor_hash on 1D arrays (if it could), which I guess may make it faster more than for 2Ds, so I broadcast the input arrays to 2D by [None, :], which get the following error just for arr2:

    print(mul_xor_hash(arr2[0]))
ValueError: new type not compatible with array


and what does delta do

I am searching the most efficient way in this regard. In the absence of better method than norok2 solution, how to prepare this solution for 1D arrays?
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
27261404,27261404,"Changing twice ""for"" loop for single ""for"" loop (SORTING)","<p>This is my first post here. I have to sort values that returns True or False. My problem is that, it take too long in two loops but I can't change it to single loop.</p>

<p>Here is code for the sorting: </p>

<pre><code>for (int i = 0; i &lt; size; i++) {
    if (f(arr[i]) == true) {
       for(int j = 0; j &lt; size; j++){
           if(f(arr[j]) == false){
               swap(arr[i],arr[j]);
           }
       }    
} 
</code></pre>

<p>And here is function:</p>

<pre><code>bool isEven(int e) { return e%2 == 0; }
</code></pre>

<p>True vaules must be first in array and false vals on the right side(place that left). I must get rid of that inside loop. Thanks for help and advices. I can't make any new arrays, that must be done with the one in loop (arr[]).</p>

<p>For example for array: 1,2,3,4,5,6 -> 2 4 6 1 5 3.</p>
","this is my first post here i have to sort values that returns true or false my problem is that it take too long in two loops but i cant change it to single loop
here is code for the sorting 
and here is function
true vaules must be first in array and false vals on the right sideplace that left i must get rid of that inside loop thanks for help and advices i cant make any new arrays that must be done with the one in loop arr
for example for array 123456  2 4 6 1 5 3
",0,"This is my first post here. I have to sort values that returns True or False. My problem is that, it take too long in two loops but I can't change it to single loop.
Here is code for the sorting: 
for (int i = 0; i < size; i++) {
    if (f(arr[i]) == true) {
       for(int j = 0; j < size; j++){
           if(f(arr[j]) == false){
               swap(arr[i],arr[j]);
           }
       }    
} 

And here is function:
bool isEven(int e) { return e%2 == 0; }

True vaules must be first in array and false vals on the right side(place that left). I must get rid of that inside loop. Thanks for help and advices. I can't make any new arrays, that must be done with the one in loop (arr[]).
For example for array: 1,2,3,4,5,6 -> 2 4 6 1 5 3.
","Changing twice ""for"" loop for single ""for"" loop (SORTING)
This is my first post here. I have to sort values that returns True or False. My problem is that, it take too long in two loops but I can't change it to single loop.
Here is code for the sorting: 
for (int i = 0; i < size; i++) {
    if (f(arr[i]) == true) {
       for(int j = 0; j < size; j++){
           if(f(arr[j]) == false){
               swap(arr[i],arr[j]);
           }
       }    
} 

And here is function:
bool isEven(int e) { return e%2 == 0; }

True vaules must be first in array and false vals on the right side(place that left). I must get rid of that inside loop. Thanks for help and advices. I can't make any new arrays, that must be done with the one in loop (arr[]).
For example for array: 1,2,3,4,5,6 -> 2 4 6 1 5 3.
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
46699118,46699118,Stream reduction identity vs. idempotent value,"<p>The <a href=""https://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html"" rel=""nofollow noreferrer""><code>java.util.stream</code></a> package documentation gives this definition for <em>identity</em> in the context of reduction:</p>

<blockquote>
  <p>the <code>identity</code> value must be an <em>identity</em> for the combiner function. This means that for all <code>u</code>, <code>combiner.apply(identity, u)</code> is equal to <code>u</code>.</p>
</blockquote>

<p><a href=""https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#reduce-T-java.util.function.BinaryOperator-"" rel=""nofollow noreferrer""><code>Stream.reduce()</code></a> and its primitive counterparts offer a similar definition.</p>

<p>As far as I understand, this definition is necessary to support parallel streams. For example, a non-zero seed value for a sum reduction could be multiplied by the number of parallel processors, distorting the final result unpredictably.</p>

<p>But this definition seems stricter than necessary to support parallelism. Why not require an <a href=""https://en.wikipedia.org/wiki/Idempotence#Idempotent_elements_and_binary_operations"" rel=""nofollow noreferrer""><em>idempotent</em></a> value <code>x</code>, such that <code>combiner.apply(x, x)</code> is equal to <code>x</code>? This would protect functions like <code>Integer::sum</code>, <code>(a, b) -&gt; a * b</code> and <code>String::concat</code> from skewing across multiple processes, while still allowing the use of any seed with idempotent functions like <code>Math::max</code>, <code>(a, b) -&gt; a | b</code> and <a href=""https://google.github.io/guava/releases/snapshot/api/docs/com/google/common/collect/Sets.html#intersection-java.util.Set-java.util.Set-"" rel=""nofollow noreferrer""><code>Sets::intersection</code></a>.</p>

<p>Is there some unique benefit to an identity value that I'm overlooking?</p>
","the javautilstream package documentation gives this definition for identity in the context of reduction

the identity value must be an identity for the combiner function this means that for all u combinerapplyidentity u is equal to u

streamreduce and its primitive counterparts offer a similar definition
as far as i understand this definition is necessary to support parallel streams for example a nonzero seed value for a sum reduction could be multiplied by the number of parallel processors distorting the final result unpredictably
but this definition seems stricter than necessary to support parallelism why not require an idempotent value x such that combinerapplyx x is equal to x this would protect functions like integersum a b  a  b and stringconcat from skewing across multiple processes while still allowing the use of any seed with idempotent functions like mathmax a b  a  b and setsintersection
is there some unique benefit to an identity value that im overlooking
",1,"The java.util.stream package documentation gives this definition for identity in the context of reduction:

the identity value must be an identity for the combiner function. This means that for all u, combiner.apply(identity, u) is equal to u.

Stream.reduce() and its primitive counterparts offer a similar definition.
As far as I understand, this definition is necessary to support parallel streams. For example, a non-zero seed value for a sum reduction could be multiplied by the number of parallel processors, distorting the final result unpredictably.
But this definition seems stricter than necessary to support parallelism. Why not require an idempotent value x, such that combiner.apply(x, x) is equal to x? This would protect functions like Integer::sum, (a, b) -> a * b and String::concat from skewing across multiple processes, while still allowing the use of any seed with idempotent functions like Math::max, (a, b) -> a | b and Sets::intersection.
Is there some unique benefit to an identity value that I'm overlooking?
","Stream reduction identity vs. idempotent value
The java.util.stream package documentation gives this definition for identity in the context of reduction:

the identity value must be an identity for the combiner function. This means that for all u, combiner.apply(identity, u) is equal to u.

Stream.reduce() and its primitive counterparts offer a similar definition.
As far as I understand, this definition is necessary to support parallel streams. For example, a non-zero seed value for a sum reduction could be multiplied by the number of parallel processors, distorting the final result unpredictably.
But this definition seems stricter than necessary to support parallelism. Why not require an idempotent value x, such that combiner.apply(x, x) is equal to x? This would protect functions like Integer::sum, (a, b) -> a * b and String::concat from skewing across multiple processes, while still allowing the use of any seed with idempotent functions like Math::max, (a, b) -> a | b and Sets::intersection.
Is there some unique benefit to an identity value that I'm overlooking?
",0,0,1,1,0,0,0,- Is a theoretical question,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,0,1,1,0,0,,,Documentation,Documentation,,Documentation,Mild,Mild,,Mild
12984140,12984140,Absolute fastest possible Java HTML escape function,"<p>Basically, this post is a challenge.  I've been trying to optimize an HTML escape function today with moderate success.  But I know there are some serious Java hackers out there who can probably do this way better than me and I'd love to learn.</p>

<p>I've been profiling my Java web app and have found that a major hotspot was our String escaping function.  We currently use <a href=""http://commons.apache.org/lang/"" rel=""nofollow"">Apache Commons Lang</a> for this task, calling StringEscapeUtils.escapeHtml().  I assumed since it is so widely used it would be reasonably fast, but even my most naive implementation was significantly faster.</p>

<p>Here's the benchmark code I used along with my Naive implementation.  It tests a strings of varying length, some containing nothing but plain text and some containing HTML that needs escaping.</p>

<pre><code>public class HTMLEscapeBenchmark {

    public static String escapeHtml(String text) {
        if (text == null) return null;

        StringBuilder sb = new StringBuilder();
        for (int i = 0; i &lt; text.length(); i++) {
            char c = text.charAt(i);

            if (c == '&amp;') {
                sb.append(""&amp;amp;"");
            } else if (c == '\'') {
                sb.append(""&amp;#39;"");
            } else if (c == '""') {
                sb.append(""&amp;quot;"");
            } else if (c == '&lt;') {
                sb.append(""&amp;lt;"");
            } else if (c == '&gt;') {
                sb.append(""&amp;gt;"");
            } else {
                sb.append(c);
            }
        }

        return sb.toString();
    }

    /*
    public static String escapeHtml(String text) {
        if (text == null) return null;
        return StringEscapeUtils.escapeHtml(text);
    }
    */


    public static void main(String[] args) {

        final int RUNS = 5;
        final int ITERATIONS = 1000000;


        // Standard lorem ipsum text.
        String loremIpsum = ""Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut "" +
            ""labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut "" +
            ""aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum "" +
            ""dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia "" +
            ""deserunt mollit anim id est laborum. "";
        while (loremIpsum.length() &lt; 1000) loremIpsum += loremIpsum;

        // Add some characters that need HTML escaping.  Bold every 2 and 3 letter word, quote every 5 letter word.
        String loremIpsumHtml = loremIpsum.replaceAll(""[A-Za-z]{2}]"", ""&lt;b&gt;$0&lt;/b&gt;"").replaceAll(""[A-Za-z]{5}"", ""\""$0\"""");

        System.out.print(""\nNormal-10"");
        String text = loremIpsum.substring(0, 10);
        for (int run = 1; run &lt;= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i &lt; ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nNormal-100"");
        text = loremIpsum.substring(0, 100);
        for (int run = 1; run &lt;= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i &lt; ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nNormal-1000"");
        text = loremIpsum.substring(0, 1000);
        for (int run = 1; run &lt;= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i &lt; ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-10"");
        text = loremIpsumHtml.substring(0, 10);
        for (int run = 1; run &lt;= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i &lt; ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-100"");
        text = loremIpsumHtml.substring(0, 100);
        for (int run = 1; run &lt;= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i &lt; ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-1000"");
        text = loremIpsumHtml.substring(0, 1000);
        for (int run = 1; run &lt;= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i &lt; ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }
    }
}
</code></pre>

<p>On my two year old MacBook pro, I get the following results.</p>

<p>Commons Lang StringEscapeUtils.escapeHtml</p>

<pre><code>Normal-10     0.439     0.357     0.351     0.343     0.342
Normal-100     2.244     0.934     0.930     0.932     0.931
Normal-1000     8.993     9.020     9.007     9.043     9.052
Html-10     0.270     0.259     0.258     0.258     0.257
Html-100     1.769     1.753     1.765     1.754     1.759
Html-1000     17.313     17.479     17.347     17.266     17.246
</code></pre>

<p>Naive Implementation</p>

<pre><code>Normal-10    0.111    0.091    0.086     0.084     0.088
Normal-100    0.636     0.627     0.626     0.626     0.627
Normal-1000     5.740     5.755     5.721     5.728     5.720
Html-10     0.145     0.138     0.138     0.138     0.138
Html-100     0.899     0.901     0.896     0.901     0.900
Html-1000     8.249     8.288     8.272     8.262     8.284
</code></pre>

<p>I'll post my own best attempt at optimization as an answer.  So, my question is, can you do better?  What is the fastest possible method for escaping HTML?</p>
","basically this post is a challenge  ive been trying to optimize an html escape function today with moderate success  but i know there are some serious java hackers out there who can probably do this way better than me and id love to learn
ive been profiling my java web app and have found that a major hotspot was our string escaping function  we currently use apache commons lang for this task calling stringescapeutilsescapehtml  i assumed since it is so widely used it would be reasonably fast but even my most naive implementation was significantly faster
heres the benchmark code i used along with my naive implementation  it tests a strings of varying length some containing nothing but plain text and some containing html that needs escaping
on my two year old macbook pro i get the following results
commons lang stringescapeutilsescapehtml
naive implementation
ill post my own best attempt at optimization as an answer  so my question is can you do better  what is the fastest possible method for escaping html
",1,"Basically, this post is a challenge.  I've been trying to optimize an HTML escape function today with moderate success.  But I know there are some serious Java hackers out there who can probably do this way better than me and I'd love to learn.
I've been profiling my Java web app and have found that a major hotspot was our String escaping function.  We currently use Apache Commons Lang for this task, calling StringEscapeUtils.escapeHtml().  I assumed since it is so widely used it would be reasonably fast, but even my most naive implementation was significantly faster.
Here's the benchmark code I used along with my Naive implementation.  It tests a strings of varying length, some containing nothing but plain text and some containing HTML that needs escaping.
public class HTMLEscapeBenchmark {

    public static String escapeHtml(String text) {
        if (text == null) return null;

        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < text.length(); i++) {
            char c = text.charAt(i);

            if (c == '&') {
                sb.append(""&amp;"");
            } else if (c == '\'') {
                sb.append(""&#39;"");
            } else if (c == '""') {
                sb.append(""&quot;"");
            } else if (c == '<') {
                sb.append(""&lt;"");
            } else if (c == '>') {
                sb.append(""&gt;"");
            } else {
                sb.append(c);
            }
        }

        return sb.toString();
    }

    /*
    public static String escapeHtml(String text) {
        if (text == null) return null;
        return StringEscapeUtils.escapeHtml(text);
    }
    */


    public static void main(String[] args) {

        final int RUNS = 5;
        final int ITERATIONS = 1000000;


        // Standard lorem ipsum text.
        String loremIpsum = ""Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut "" +
            ""labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut "" +
            ""aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum "" +
            ""dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia "" +
            ""deserunt mollit anim id est laborum. "";
        while (loremIpsum.length() < 1000) loremIpsum += loremIpsum;

        // Add some characters that need HTML escaping.  Bold every 2 and 3 letter word, quote every 5 letter word.
        String loremIpsumHtml = loremIpsum.replaceAll(""[A-Za-z]{2}]"", ""<b>$0</b>"").replaceAll(""[A-Za-z]{5}"", ""\""$0\"""");

        System.out.print(""\nNormal-10"");
        String text = loremIpsum.substring(0, 10);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nNormal-100"");
        text = loremIpsum.substring(0, 100);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nNormal-1000"");
        text = loremIpsum.substring(0, 1000);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-10"");
        text = loremIpsumHtml.substring(0, 10);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-100"");
        text = loremIpsumHtml.substring(0, 100);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-1000"");
        text = loremIpsumHtml.substring(0, 1000);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }
    }
}

On my two year old MacBook pro, I get the following results.
Commons Lang StringEscapeUtils.escapeHtml
Normal-10     0.439     0.357     0.351     0.343     0.342
Normal-100     2.244     0.934     0.930     0.932     0.931
Normal-1000     8.993     9.020     9.007     9.043     9.052
Html-10     0.270     0.259     0.258     0.258     0.257
Html-100     1.769     1.753     1.765     1.754     1.759
Html-1000     17.313     17.479     17.347     17.266     17.246

Naive Implementation
Normal-10    0.111    0.091    0.086     0.084     0.088
Normal-100    0.636     0.627     0.626     0.626     0.627
Normal-1000     5.740     5.755     5.721     5.728     5.720
Html-10     0.145     0.138     0.138     0.138     0.138
Html-100     0.899     0.901     0.896     0.901     0.900
Html-1000     8.249     8.288     8.272     8.262     8.284

I'll post my own best attempt at optimization as an answer.  So, my question is, can you do better?  What is the fastest possible method for escaping HTML?
","Absolute fastest possible Java HTML escape function
Basically, this post is a challenge.  I've been trying to optimize an HTML escape function today with moderate success.  But I know there are some serious Java hackers out there who can probably do this way better than me and I'd love to learn.
I've been profiling my Java web app and have found that a major hotspot was our String escaping function.  We currently use Apache Commons Lang for this task, calling StringEscapeUtils.escapeHtml().  I assumed since it is so widely used it would be reasonably fast, but even my most naive implementation was significantly faster.
Here's the benchmark code I used along with my Naive implementation.  It tests a strings of varying length, some containing nothing but plain text and some containing HTML that needs escaping.
public class HTMLEscapeBenchmark {

    public static String escapeHtml(String text) {
        if (text == null) return null;

        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < text.length(); i++) {
            char c = text.charAt(i);

            if (c == '&') {
                sb.append(""&amp;"");
            } else if (c == '\'') {
                sb.append(""&#39;"");
            } else if (c == '""') {
                sb.append(""&quot;"");
            } else if (c == '<') {
                sb.append(""&lt;"");
            } else if (c == '>') {
                sb.append(""&gt;"");
            } else {
                sb.append(c);
            }
        }

        return sb.toString();
    }

    /*
    public static String escapeHtml(String text) {
        if (text == null) return null;
        return StringEscapeUtils.escapeHtml(text);
    }
    */


    public static void main(String[] args) {

        final int RUNS = 5;
        final int ITERATIONS = 1000000;


        // Standard lorem ipsum text.
        String loremIpsum = ""Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut "" +
            ""labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut "" +
            ""aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum "" +
            ""dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia "" +
            ""deserunt mollit anim id est laborum. "";
        while (loremIpsum.length() < 1000) loremIpsum += loremIpsum;

        // Add some characters that need HTML escaping.  Bold every 2 and 3 letter word, quote every 5 letter word.
        String loremIpsumHtml = loremIpsum.replaceAll(""[A-Za-z]{2}]"", ""<b>$0</b>"").replaceAll(""[A-Za-z]{5}"", ""\""$0\"""");

        System.out.print(""\nNormal-10"");
        String text = loremIpsum.substring(0, 10);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nNormal-100"");
        text = loremIpsum.substring(0, 100);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nNormal-1000"");
        text = loremIpsum.substring(0, 1000);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-10"");
        text = loremIpsumHtml.substring(0, 10);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-100"");
        text = loremIpsumHtml.substring(0, 100);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }

        System.out.print(""\nHtml-1000"");
        text = loremIpsumHtml.substring(0, 1000);
        for (int run = 1; run <= RUNS; run++) {
            long start = System.nanoTime();
            for (int i = 0; i < ITERATIONS; i++) {
                escapeHtml(text);
            }
            System.out.printf(""\t%.3f"", (System.nanoTime() - start) / 1e9);
        }
    }
}

On my two year old MacBook pro, I get the following results.
Commons Lang StringEscapeUtils.escapeHtml
Normal-10     0.439     0.357     0.351     0.343     0.342
Normal-100     2.244     0.934     0.930     0.932     0.931
Normal-1000     8.993     9.020     9.007     9.043     9.052
Html-10     0.270     0.259     0.258     0.258     0.257
Html-100     1.769     1.753     1.765     1.754     1.759
Html-1000     17.313     17.479     17.347     17.266     17.246

Naive Implementation
Normal-10    0.111    0.091    0.086     0.084     0.088
Normal-100    0.636     0.627     0.626     0.626     0.627
Normal-1000     5.740     5.755     5.721     5.728     5.720
Html-10     0.145     0.138     0.138     0.138     0.138
Html-100     0.899     0.901     0.896     0.901     0.900
Html-1000     8.249     8.288     8.272     8.262     8.284

I'll post my own best attempt at optimization as an answer.  So, my question is, can you do better?  What is the fastest possible method for escaping HTML?
",0,1,0,1,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
10749585,10749585,Converting double to char* in C++ with high performance,"<p>My application needs to convert double values to char* to write to a pipe that accepts only characters. The usual ways of doing this are using the <em>sprintf()</em> function or using <em>ostringstream</em> from iomanip.h header file.</p>

<p>Turns out, both of these have really bad performance. And my application needs to do this conversion so often that it becomes the primary bottleneck.</p>

<p>Is there any other function I could use? What logic can I use to write an efficient conversion function? The only thing I have managed to come up with so far is to get each individual digit out using division and mod operations, and append these digits to a char* to get the entire double value. This doesn't seem like a good approach though, and will likely have bad performance itself.</p>

<p>Thanks in advance for your thoughts.</p>

<p>EDIT: There is some confusion over how the char* will be used. The char* will be an argument to the fwrite function which writes to a pipe.</p>
","my application needs to convert double values to char to write to a pipe that accepts only characters the usual ways of doing this are using the sprintf function or using ostringstream from iomaniph header file
turns out both of these have really bad performance and my application needs to do this conversion so often that it becomes the primary bottleneck
is there any other function i could use what logic can i use to write an efficient conversion function the only thing i have managed to come up with so far is to get each individual digit out using division and mod operations and append these digits to a char to get the entire double value this doesnt seem like a good approach though and will likely have bad performance itself
thanks in advance for your thoughts
edit there is some confusion over how the char will be used the char will be an argument to the fwrite function which writes to a pipe
",1,"My application needs to convert double values to char* to write to a pipe that accepts only characters. The usual ways of doing this are using the sprintf() function or using ostringstream from iomanip.h header file.
Turns out, both of these have really bad performance. And my application needs to do this conversion so often that it becomes the primary bottleneck.
Is there any other function I could use? What logic can I use to write an efficient conversion function? The only thing I have managed to come up with so far is to get each individual digit out using division and mod operations, and append these digits to a char* to get the entire double value. This doesn't seem like a good approach though, and will likely have bad performance itself.
Thanks in advance for your thoughts.
EDIT: There is some confusion over how the char* will be used. The char* will be an argument to the fwrite function which writes to a pipe.
","Converting double to char* in C++ with high performance
My application needs to convert double values to char* to write to a pipe that accepts only characters. The usual ways of doing this are using the sprintf() function or using ostringstream from iomanip.h header file.
Turns out, both of these have really bad performance. And my application needs to do this conversion so often that it becomes the primary bottleneck.
Is there any other function I could use? What logic can I use to write an efficient conversion function? The only thing I have managed to come up with so far is to get each individual digit out using division and mod operations, and append these digits to a char* to get the entire double value. This doesn't seem like a good approach though, and will likely have bad performance itself.
Thanks in advance for your thoughts.
EDIT: There is some confusion over how the char* will be used. The char* will be an argument to the fwrite function which writes to a pipe.
",0,1,0,1,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Code,Code,,Code,Very Severe,Very Severe,,Very Severe
4661963,4661963,"Working with Huge ArrayLists, Java OutOfMemoryError: Java heap space... use db?","<p>I am having a memory problem that I completely understand the cause, but have no idea of a fix.  I have attempted to use the -Xmx2g tag and make the heap size larger, but there seems to be a hidden maximum. (if i used -Xmx512m I run out of space at the same time).</p>

<p>Assume I have 2 objects, an Area and a User.  My Area object holds an ArrayList of users:</p>

<pre><code>public class Area {
      int numUsers;
      ArrayList&lt;User&gt; userList;
}
</code></pre>

<p>My User class holds an ArrayList of friends:</p>

<pre><code>public class User {
      int userID;
      int numFriends;
      ArrayList&lt;User&gt; friends;
}
</code></pre>

<p>Just using a single Area, with 1 million users, and an average of 200 friends per user, I run out of heap space after about 680,000 Users are created.  Obviously if I lower the average number of friends/user to something closer to 100, I can store all of these objects in the heap.</p>

<p>What if I want to simulate 2 million Users in an area?  Or Hundreds of area's?</p>

<p>With this much data, is a database the only feasible way to do simulations using the information?</p>
","i am having a memory problem that i completely understand the cause but have no idea of a fix  i have attempted to use the xmx2g tag and make the heap size larger but there seems to be a hidden maximum if i used xmx512m i run out of space at the same time
assume i have 2 objects an area and a user  my area object holds an arraylist of users
my user class holds an arraylist of friends
just using a single area with 1 million users and an average of 200 friends per user i run out of heap space after about 680000 users are created  obviously if i lower the average number of friendsuser to something closer to 100 i can store all of these objects in the heap
what if i want to simulate 2 million users in an area  or hundreds of areas
with this much data is a database the only feasible way to do simulations using the information
",1,"I am having a memory problem that I completely understand the cause, but have no idea of a fix.  I have attempted to use the -Xmx2g tag and make the heap size larger, but there seems to be a hidden maximum. (if i used -Xmx512m I run out of space at the same time).
Assume I have 2 objects, an Area and a User.  My Area object holds an ArrayList of users:
public class Area {
      int numUsers;
      ArrayList<User> userList;
}

My User class holds an ArrayList of friends:
public class User {
      int userID;
      int numFriends;
      ArrayList<User> friends;
}

Just using a single Area, with 1 million users, and an average of 200 friends per user, I run out of heap space after about 680,000 Users are created.  Obviously if I lower the average number of friends/user to something closer to 100, I can store all of these objects in the heap.
What if I want to simulate 2 million Users in an area?  Or Hundreds of area's?
With this much data, is a database the only feasible way to do simulations using the information?
","Working with Huge ArrayLists, Java OutOfMemoryError: Java heap space... use db?
I am having a memory problem that I completely understand the cause, but have no idea of a fix.  I have attempted to use the -Xmx2g tag and make the heap size larger, but there seems to be a hidden maximum. (if i used -Xmx512m I run out of space at the same time).
Assume I have 2 objects, an Area and a User.  My Area object holds an ArrayList of users:
public class Area {
      int numUsers;
      ArrayList<User> userList;
}

My User class holds an ArrayList of friends:
public class User {
      int userID;
      int numFriends;
      ArrayList<User> friends;
}

Just using a single Area, with 1 million users, and an average of 200 friends per user, I run out of heap space after about 680,000 Users are created.  Obviously if I lower the average number of friends/user to something closer to 100, I can store all of these objects in the heap.
What if I want to simulate 2 million Users in an area?  Or Hundreds of area's?
With this much data, is a database the only feasible way to do simulations using the information?
",1,1,0,0,0,0,1,,,,1,1,Memory Management,Memory Management,,Memory Management,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
35279145,35279145,MPI parallelization help for reading a large amount of data multiple times,"<p>I have been asked to parallelize an existing c program in order to decrease its runtime. 
I only have some (very limited) experience using basic MPI (and all my programming knowledge is self taught, so it is somewhat spotty). I am currently trying to figure out the best parallelization approach.</p>

<p>Currently, during every iteration of the main loop (M = number of iterations), the program sequentially accesses a set of input files (N= number of files)-each of varying length. After all the input files are read the program sorts the data and updates a set of output files. Both N and M are known at the start, and N is always larger than M. In fact, N is too large to read all the input data into memory, so each time the files are read, only the information pertinent to that main loop iteration is kept.</p>

<p>I am confident I can make each main loop iteration independent, but every iteration would still need to access all N files. What would be the best way to use OpenMPI (technically OpenRTE 1.6.2 running on Rocks- i.e. RedHat Linux) to parallelize this program?</p>

<p>My first idea was to simply split up the read-in of the input files across multiple threads- each thread handling a subset of files and then ordering the inputs at the end.</p>

<p>My second idea was to instead split up the main M loop across the threads, which would be a much better utilization of MPI. But would this method require copies all input files in every thread (to avoid reading conflicts)? If so, I am worried copying the files may offset any time gained from parallelizing the main loop. Also, besides building a test program for each approach, is there an easier way to determine which method would be faster?</p>

<p>Edit: The file system is NFS.  </p>

<p>After reading the comments I went back and ran a few tests on the code. The program spends 93% of its runtime reading in data. From what has been said it seems parallelization alone may not be the best solution. At this point it seems necessary to really look into the programâ€™s calculations and try to minimize the read-in requirements. </p>

<p>Thank you very much for the replies. </p>
","i have been asked to parallelize an existing c program in order to decrease its runtime 
i only have some very limited experience using basic mpi and all my programming knowledge is self taught so it is somewhat spotty i am currently trying to figure out the best parallelization approach
currently during every iteration of the main loop m  number of iterations the program sequentially accesses a set of input files n number of fileseach of varying length after all the input files are read the program sorts the data and updates a set of output files both n and m are known at the start and n is always larger than m in fact n is too large to read all the input data into memory so each time the files are read only the information pertinent to that main loop iteration is kept
i am confident i can make each main loop iteration independent but every iteration would still need to access all n files what would be the best way to use openmpi technically openrte 162 running on rocks ie redhat linux to parallelize this program
my first idea was to simply split up the readin of the input files across multiple threads each thread handling a subset of files and then ordering the inputs at the end
my second idea was to instead split up the main m loop across the threads which would be a much better utilization of mpi but would this method require copies all input files in every thread to avoid reading conflicts if so i am worried copying the files may offset any time gained from parallelizing the main loop also besides building a test program for each approach is there an easier way to determine which method would be faster
edit the file system is nfs  
after reading the comments i went back and ran a few tests on the code the program spends 93 of its runtime reading in data from what has been said it seems parallelization alone may not be the best solution at this point it seems necessary to really look into the programs calculations and try to minimize the readin requirements 
thank you very much for the replies 
",1,"I have been asked to parallelize an existing c program in order to decrease its runtime. 
I only have some (very limited) experience using basic MPI (and all my programming knowledge is self taught, so it is somewhat spotty). I am currently trying to figure out the best parallelization approach.
Currently, during every iteration of the main loop (M = number of iterations), the program sequentially accesses a set of input files (N= number of files)-each of varying length. After all the input files are read the program sorts the data and updates a set of output files. Both N and M are known at the start, and N is always larger than M. In fact, N is too large to read all the input data into memory, so each time the files are read, only the information pertinent to that main loop iteration is kept.
I am confident I can make each main loop iteration independent, but every iteration would still need to access all N files. What would be the best way to use OpenMPI (technically OpenRTE 1.6.2 running on Rocks- i.e. RedHat Linux) to parallelize this program?
My first idea was to simply split up the read-in of the input files across multiple threads- each thread handling a subset of files and then ordering the inputs at the end.
My second idea was to instead split up the main M loop across the threads, which would be a much better utilization of MPI. But would this method require copies all input files in every thread (to avoid reading conflicts)? If so, I am worried copying the files may offset any time gained from parallelizing the main loop. Also, besides building a test program for each approach, is there an easier way to determine which method would be faster?
Edit: The file system is NFS.  
After reading the comments I went back and ran a few tests on the code. The program spends 93% of its runtime reading in data. From what has been said it seems parallelization alone may not be the best solution. At this point it seems necessary to really look into the programâ€™s calculations and try to minimize the read-in requirements. 
Thank you very much for the replies. 
","MPI parallelization help for reading a large amount of data multiple times
I have been asked to parallelize an existing c program in order to decrease its runtime. 
I only have some (very limited) experience using basic MPI (and all my programming knowledge is self taught, so it is somewhat spotty). I am currently trying to figure out the best parallelization approach.
Currently, during every iteration of the main loop (M = number of iterations), the program sequentially accesses a set of input files (N= number of files)-each of varying length. After all the input files are read the program sorts the data and updates a set of output files. Both N and M are known at the start, and N is always larger than M. In fact, N is too large to read all the input data into memory, so each time the files are read, only the information pertinent to that main loop iteration is kept.
I am confident I can make each main loop iteration independent, but every iteration would still need to access all N files. What would be the best way to use OpenMPI (technically OpenRTE 1.6.2 running on Rocks- i.e. RedHat Linux) to parallelize this program?
My first idea was to simply split up the read-in of the input files across multiple threads- each thread handling a subset of files and then ordering the inputs at the end.
My second idea was to instead split up the main M loop across the threads, which would be a much better utilization of MPI. But would this method require copies all input files in every thread (to avoid reading conflicts)? If so, I am worried copying the files may offset any time gained from parallelizing the main loop. Also, besides building a test program for each approach, is there an easier way to determine which method would be faster?
Edit: The file system is NFS.  
After reading the comments I went back and ran a few tests on the code. The program spends 93% of its runtime reading in data. From what has been said it seems parallelization alone may not be the best solution. At this point it seems necessary to really look into the programâ€™s calculations and try to minimize the read-in requirements. 
Thank you very much for the replies. 
",0,1,0,1,0,0,1,,,,1,1,Runtime Efficiency,Multithreading and Concurrency,User is looking to parallelize their program ,Runtime Efficiency,0,1,0,1,0,0,,,Infrastructure,Infrastructure,,Infrastructure,Mild,Mild,,Mild
53449792,53449792,Filling DataFrame Pandas Python,"<p>I have a similar dataset, and even though the code gives me the right output; I do not want to use three for loops. Is there a way to do this in a better way?</p>

<pre><code>import pandas as pd

col = [""a"",""b"",""c"",""d""]
index = [""0"",""1"",""2"",""3""]
dict_ = {(""0"",""a""):8,
         (""1"",""a""):3,
         (""3"",""b""):2}

df = pd.DataFrame(columns=col,index=index)
for i in range(len(dict_)):
    for j in range(len(df)):
        for k in range(len(df)):
            if (str(df.index[j]),str(df.columns[k])) == dict_.keys()[i]:
                df.at[df.index[j],df.columns[k]] = dict_.values()[i]

print df
</code></pre>
","i have a similar dataset and even though the code gives me the right output i do not want to use three for loops is there a way to do this in a better way
",1,"I have a similar dataset, and even though the code gives me the right output; I do not want to use three for loops. Is there a way to do this in a better way?
import pandas as pd

col = [""a"",""b"",""c"",""d""]
index = [""0"",""1"",""2"",""3""]
dict_ = {(""0"",""a""):8,
         (""1"",""a""):3,
         (""3"",""b""):2}

df = pd.DataFrame(columns=col,index=index)
for i in range(len(dict_)):
    for j in range(len(df)):
        for k in range(len(df)):
            if (str(df.index[j]),str(df.columns[k])) == dict_.keys()[i]:
                df.at[df.index[j],df.columns[k]] = dict_.values()[i]

print df

","Filling DataFrame Pandas Python
I have a similar dataset, and even though the code gives me the right output; I do not want to use three for loops. Is there a way to do this in a better way?
import pandas as pd

col = [""a"",""b"",""c"",""d""]
index = [""0"",""1"",""2"",""3""]
dict_ = {(""0"",""a""):8,
         (""1"",""a""):3,
         (""3"",""b""):2}

df = pd.DataFrame(columns=col,index=index)
for i in range(len(dict_)):
    for j in range(len(df)):
        for k in range(len(df)):
            if (str(df.index[j]),str(df.columns[k])) == dict_.keys()[i]:
                df.at[df.index[j],df.columns[k]] = dict_.values()[i]

print df

",0,1,0,1,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
72970186,72970186,C Vectorization: Is it possible to do elementwise operation in array like python-vectorization?,"<p>I am moving from python to C, in the hope of faster implementation, and trying to learn vectorization in C equivalent to python vectorization. For example, assume that we have binary array <code>Input_Binary_Array</code>, if I want to multiply each element for the index, say, <code>i</code>, by 2**i and then sum all non-zero, in python-vectorization we do the following:</p>
<pre><code>case 1 : Value = (2. ** (np.nonzero(Input_Binary_Array)[0] + 1)).sum()
</code></pre>
<p>Or if we do <a href=""https://stackoverflow.com/a/509295/3989930"">slicing</a> and do elementwise addition/subtraction/multiplication, we do the following:</p>
<pre><code>case 2 : Array_opr= (Input_Binary_Array[size:] * 2**Size -Input_Binary_Array[:-size])
</code></pre>
<p><code>C</code> is a powerful low-level language, so simple for/while loop is quite faster, but I am not sure that there are no equivalent vectorizations like python.</p>
<p>So, my question is, is there an explicit vectorization code for:</p>
<p>1.</p>
<p>multiplying all elements of an array</p>
<p>with a constant number (scalar)</p>
<p>2.</p>
<p>elementwise addition, subtraction, division for 2 given arrays of same size.</p>
<p>3.</p>
<p>slicing, summing, cumulative summing</p>
<p>or, the simple for, while loop is the only faster option to do above operations like python vectorization (case 1, 2)?</p>
","i am moving from python to c in the hope of faster implementation and trying to learn vectorization in c equivalent to python vectorization for example assume that we have binary array input_binary_array if i want to multiply each element for the index say i by 2i and then sum all nonzero in pythonvectorization we do the following
or if we do slicing and do elementwise additionsubtractionmultiplication we do the following
c is a powerful lowlevel language so simple forwhile loop is quite faster but i am not sure that there are no equivalent vectorizations like python
so my question is is there an explicit vectorization code for
1
multiplying all elements of an array
with a constant number scalar
2
elementwise addition subtraction division for 2 given arrays of same size
3
slicing summing cumulative summing
or the simple for while loop is the only faster option to do above operations like python vectorization case 1 2
",1,"I am moving from python to C, in the hope of faster implementation, and trying to learn vectorization in C equivalent to python vectorization. For example, assume that we have binary array Input_Binary_Array, if I want to multiply each element for the index, say, i, by 2**i and then sum all non-zero, in python-vectorization we do the following:
case 1 : Value = (2. ** (np.nonzero(Input_Binary_Array)[0] + 1)).sum()

Or if we do slicing and do elementwise addition/subtraction/multiplication, we do the following:
case 2 : Array_opr= (Input_Binary_Array[size:] * 2**Size -Input_Binary_Array[:-size])

C is a powerful low-level language, so simple for/while loop is quite faster, but I am not sure that there are no equivalent vectorizations like python.
So, my question is, is there an explicit vectorization code for:
1.
multiplying all elements of an array
with a constant number (scalar)
2.
elementwise addition, subtraction, division for 2 given arrays of same size.
3.
slicing, summing, cumulative summing
or, the simple for, while loop is the only faster option to do above operations like python vectorization (case 1, 2)?
","C Vectorization: Is it possible to do elementwise operation in array like python-vectorization?
I am moving from python to C, in the hope of faster implementation, and trying to learn vectorization in C equivalent to python vectorization. For example, assume that we have binary array Input_Binary_Array, if I want to multiply each element for the index, say, i, by 2**i and then sum all non-zero, in python-vectorization we do the following:
case 1 : Value = (2. ** (np.nonzero(Input_Binary_Array)[0] + 1)).sum()

Or if we do slicing and do elementwise addition/subtraction/multiplication, we do the following:
case 2 : Array_opr= (Input_Binary_Array[size:] * 2**Size -Input_Binary_Array[:-size])

C is a powerful low-level language, so simple for/while loop is quite faster, but I am not sure that there are no equivalent vectorizations like python.
So, my question is, is there an explicit vectorization code for:
1.
multiplying all elements of an array
with a constant number (scalar)
2.
elementwise addition, subtraction, division for 2 given arrays of same size.
3.
slicing, summing, cumulative summing
or, the simple for, while loop is the only faster option to do above operations like python vectorization (case 1, 2)?
",0,1,1,0,0,0,0,- User just doesn't know how to approach a problem,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,1,0,0,0,,,Documentation,Documentation,,Documentation,Mild,Mild,,Mild
14501509,14501509,Winforms performance issue with Numeric up down and list box,"<p>In my Win forms app, i have a Numeric up down and a list box. 
when i increase/change value in numeric up down, that many rows are added in list box.</p>

<p>for ex. when i have value '1' in my numeric up down and i change the value to a larger number say '300', it adds 300 lines in the list box but it takes hell lot of time to do it.</p>

<p>is there any way by which i can reduce this time.
any help would be appreciated. thanks. </p>
","in my win forms app i have a numeric up down and a list box 
when i increasechange value in numeric up down that many rows are added in list box
for ex when i have value 1 in my numeric up down and i change the value to a larger number say 300 it adds 300 lines in the list box but it takes hell lot of time to do it
is there any way by which i can reduce this time
any help would be appreciated thanks 
",0,"In my Win forms app, i have a Numeric up down and a list box. 
when i increase/change value in numeric up down, that many rows are added in list box.
for ex. when i have value '1' in my numeric up down and i change the value to a larger number say '300', it adds 300 lines in the list box but it takes hell lot of time to do it.
is there any way by which i can reduce this time.
any help would be appreciated. thanks. 
","Winforms performance issue with Numeric up down and list box
In my Win forms app, i have a Numeric up down and a list box. 
when i increase/change value in numeric up down, that many rows are added in list box.
for ex. when i have value '1' in my numeric up down and i change the value to a larger number say '300', it adds 300 lines in the list box but it takes hell lot of time to do it.
is there any way by which i can reduce this time.
any help would be appreciated. thanks. 
",1,0,0,0,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,0,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
28896769,28896769,vectorize conditional assignment in pandas dataframe,"<p>If I have a dataframe <code>df</code> with column <code>x</code> and want to create column <code>y</code> based on values of <code>x</code> using this in pseudo code:</p>
<pre><code>if df['x'] &lt; -2 then df['y'] = 1 
else if df['x'] &gt; 2 then df['y'] = -1 
else df['y'] = 0
</code></pre>
<p>How would I achieve this?  I assume <code>np.where</code> is the best way to do this but not sure how to code it correctly.</p>
","if i have a dataframe df with column x and want to create column y based on values of x using this in pseudo code
how would i achieve this  i assume npwhere is the best way to do this but not sure how to code it correctly
",1,"If I have a dataframe df with column x and want to create column y based on values of x using this in pseudo code:
if df['x'] < -2 then df['y'] = 1 
else if df['x'] > 2 then df['y'] = -1 
else df['y'] = 0

How would I achieve this?  I assume np.where is the best way to do this but not sure how to code it correctly.
","vectorize conditional assignment in pandas dataframe
If I have a dataframe df with column x and want to create column y based on values of x using this in pseudo code:
if df['x'] < -2 then df['y'] = 1 
else if df['x'] > 2 then df['y'] = -1 
else df['y'] = 0

How would I achieve this?  I assume np.where is the best way to do this but not sure how to code it correctly.
",0,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
49936190,49936190,Adding two dictionaries in a RDD in Pyspark,"<p>I have created and RDD where every element is a dictionary. (This is a sample. There are 30,000 keys</p>

<pre><code>rdd.take(2)

[{'actor': 'brad',
  'good': 1,
  'bad': 0,
  'average': 0,}
 {'actor': 'tom',
  'good': 0,
  'bad': 1,
  'average': 1,}]
</code></pre>

<p>I am trying to perform arithmetic operations on the data, That is 
'actor': 'brad' and 'actor': 'tom', I want to add the other values having the same key </p>

<p>I did this:</p>

<pre><code>d1=rdd.filter(lambda x: x['actor']=='brad').first()
d2=rdd.filter(lambda x: x['actor']=='tom').first()

dc={key: d1[key] + d2[key] for key in d1.keys() if key not in {'actor'} }
</code></pre>

<p>But since there are around 30,000 [keys] this is taking me lot of time. Can I optimize this by performing the addition using Spark's distributed operations. If so how?</p>

<p>Expected Output:</p>

<pre><code> [{'actor': 'brad',
      'good': 1,
      'bad': 1,
      'average': 1,}]
</code></pre>
","i have created and rdd where every element is a dictionary this is a sample there are 30000 keys
i am trying to perform arithmetic operations on the data that is 
actor brad and actor tom i want to add the other values having the same key 
i did this
but since there are around 30000 keys this is taking me lot of time can i optimize this by performing the addition using sparks distributed operations if so how
expected output
",0,"I have created and RDD where every element is a dictionary. (This is a sample. There are 30,000 keys
rdd.take(2)

[{'actor': 'brad',
  'good': 1,
  'bad': 0,
  'average': 0,}
 {'actor': 'tom',
  'good': 0,
  'bad': 1,
  'average': 1,}]

I am trying to perform arithmetic operations on the data, That is 
'actor': 'brad' and 'actor': 'tom', I want to add the other values having the same key 
I did this:
d1=rdd.filter(lambda x: x['actor']=='brad').first()
d2=rdd.filter(lambda x: x['actor']=='tom').first()

dc={key: d1[key] + d2[key] for key in d1.keys() if key not in {'actor'} }

But since there are around 30,000 [keys] this is taking me lot of time. Can I optimize this by performing the addition using Spark's distributed operations. If so how?
Expected Output:
 [{'actor': 'brad',
      'good': 1,
      'bad': 1,
      'average': 1,}]

","Adding two dictionaries in a RDD in Pyspark
I have created and RDD where every element is a dictionary. (This is a sample. There are 30,000 keys
rdd.take(2)

[{'actor': 'brad',
  'good': 1,
  'bad': 0,
  'average': 0,}
 {'actor': 'tom',
  'good': 0,
  'bad': 1,
  'average': 1,}]

I am trying to perform arithmetic operations on the data, That is 
'actor': 'brad' and 'actor': 'tom', I want to add the other values having the same key 
I did this:
d1=rdd.filter(lambda x: x['actor']=='brad').first()
d2=rdd.filter(lambda x: x['actor']=='tom').first()

dc={key: d1[key] + d2[key] for key in d1.keys() if key not in {'actor'} }

But since there are around 30,000 [keys] this is taking me lot of time. Can I optimize this by performing the addition using Spark's distributed operations. If so how?
Expected Output:
 [{'actor': 'brad',
      'good': 1,
      'bad': 1,
      'average': 1,}]

",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
2486840,2486840,Linux C++: how to profile time wasted due to cache misses?,"<p>I know that I can use gprof to benchmark my code.</p>
<p>However, I have this problem -- I have a smart pointer that has an extra level of indirection (think of it as a proxy object).</p>
<p>As a result, I have this extra layer that effects pretty much all functions, and screws with caching.</p>
<p>Is there a way to measure the time my CPU wastes due to cache misses?</p>
","i know that i can use gprof to benchmark my code
however i have this problem  i have a smart pointer that has an extra level of indirection think of it as a proxy object
as a result i have this extra layer that effects pretty much all functions and screws with caching
is there a way to measure the time my cpu wastes due to cache misses
",1,"I know that I can use gprof to benchmark my code.
However, I have this problem -- I have a smart pointer that has an extra level of indirection (think of it as a proxy object).
As a result, I have this extra layer that effects pretty much all functions, and screws with caching.
Is there a way to measure the time my CPU wastes due to cache misses?
","Linux C++: how to profile time wasted due to cache misses?
I know that I can use gprof to benchmark my code.
However, I have this problem -- I have a smart pointer that has an extra level of indirection (think of it as a proxy object).
As a result, I have this extra layer that effects pretty much all functions, and screws with caching.
Is there a way to measure the time my CPU wastes due to cache misses?
",1,1,0,0,0,0,0,- User just doesn't know how to approach a problem,"Label: 1

Reasoning:

This question expresses technical debt because the user is trying to measure the performance impact of a specific design decision (the extra level of indirection in the smart pointer) on caching. The user acknowledges that this design choice ""effects pretty much all functions"" and ""screws with caching"", implying that the current implementation may be suboptimal and causing performance issues.

The user is not simply asking about how to use a profiling tool (gprof), but rather how to quantify the negative impact of a specific design choice on the performance of their code. This suggests that the user is aware of a potential performance bottleneck and is seeking ways to measure and potentially optimize it.

The mention of cache misses also implies that the user is concerned about the efficiency of their code and is looking for ways to improve it. This is a classic symptom of technical debt, where a design decision or implementation detail is causing performance issues that need to be addressed.

Overall, the question expresses technical debt because it reveals a design flaw that affects performance and the user is seeking ways to measure and potentially refactor the code to improve its efficiency.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
40140942,40140942,Numpy: How to vectorize parameters of a functional form of a function applied to a data set,"<p>Ultimately, I want to remove all explicit loops in the code below to take advantage of numpy vectorization and function calls in C instead of python.</p>

<p>Below is simplified for uses of numpy in python.
I have the following quadratic function:</p>

<pre><code>def quadratic_func(a,b,c,x):
    return a*x*x + b*x + c
</code></pre>

<p>I am trying to optimize choices of a,b,c given input data x and output data y of the same size (of course, this should be done by linear regression...but humor me). Say len(x)=100.  Easy to vectorize with scalars a,b,c to get back a result of length 100.</p>

<p>Let's say that we know a,b,c should be inside of [-10,10] and I optimize by building a grid and picking the point with the min sum square error.</p>

<pre><code>a=np.arange(-10.0, 10.01, 2.0)
nodes=np.array(np.meshgrid(a,a,a)).T.reshape(-1,3) #3-d cartesian product with array of nodes
</code></pre>

<p>For each of the 1331 nodes, I would like to calculate all 1331 of the length 100 return values. </p>

<pre><code>res=[]
x=np.random.uniform(-5.0,5.0, 100)
for node in nodes:
    res.append(quadratic_func(*node, x=x))
</code></pre>

<p>How can I take advantage of broadcasting so as to get my list of 1331 items each with 100 values that are the results of calling quadratic_func on x?  Answer must use vectorization, broadcasting, etc to get the orders of magnitude speed improvements I am looking for.  Also, the answer must use calls to quadratic_func - or more generally, my_func(*node, x=x).</p>

<p>In real life I am optimizing a non-linear function that is not even close to being convex and has many local minimums.  It is a great functional form to use if I can get to the ""right"" local minimum - I already know how to do that, but would like to get there faster!</p>
","ultimately i want to remove all explicit loops in the code below to take advantage of numpy vectorization and function calls in c instead of python
below is simplified for uses of numpy in python
i have the following quadratic function
i am trying to optimize choices of abc given input data x and output data y of the same size of course this should be done by linear regressionbut humor me say lenx100  easy to vectorize with scalars abc to get back a result of length 100
lets say that we know abc should be inside of 1010 and i optimize by building a grid and picking the point with the min sum square error
for each of the 1331 nodes i would like to calculate all 1331 of the length 100 return values 
how can i take advantage of broadcasting so as to get my list of 1331 items each with 100 values that are the results of calling quadratic_func on x  answer must use vectorization broadcasting etc to get the orders of magnitude speed improvements i am looking for  also the answer must use calls to quadratic_func  or more generally my_funcnode xx
in real life i am optimizing a nonlinear function that is not even close to being convex and has many local minimums  it is a great functional form to use if i can get to the right local minimum  i already know how to do that but would like to get there faster
",1,"Ultimately, I want to remove all explicit loops in the code below to take advantage of numpy vectorization and function calls in C instead of python.
Below is simplified for uses of numpy in python.
I have the following quadratic function:
def quadratic_func(a,b,c,x):
    return a*x*x + b*x + c

I am trying to optimize choices of a,b,c given input data x and output data y of the same size (of course, this should be done by linear regression...but humor me). Say len(x)=100.  Easy to vectorize with scalars a,b,c to get back a result of length 100.
Let's say that we know a,b,c should be inside of [-10,10] and I optimize by building a grid and picking the point with the min sum square error.
a=np.arange(-10.0, 10.01, 2.0)
nodes=np.array(np.meshgrid(a,a,a)).T.reshape(-1,3) #3-d cartesian product with array of nodes

For each of the 1331 nodes, I would like to calculate all 1331 of the length 100 return values. 
res=[]
x=np.random.uniform(-5.0,5.0, 100)
for node in nodes:
    res.append(quadratic_func(*node, x=x))

How can I take advantage of broadcasting so as to get my list of 1331 items each with 100 values that are the results of calling quadratic_func on x?  Answer must use vectorization, broadcasting, etc to get the orders of magnitude speed improvements I am looking for.  Also, the answer must use calls to quadratic_func - or more generally, my_func(*node, x=x).
In real life I am optimizing a non-linear function that is not even close to being convex and has many local minimums.  It is a great functional form to use if I can get to the ""right"" local minimum - I already know how to do that, but would like to get there faster!
","Numpy: How to vectorize parameters of a functional form of a function applied to a data set
Ultimately, I want to remove all explicit loops in the code below to take advantage of numpy vectorization and function calls in C instead of python.
Below is simplified for uses of numpy in python.
I have the following quadratic function:
def quadratic_func(a,b,c,x):
    return a*x*x + b*x + c

I am trying to optimize choices of a,b,c given input data x and output data y of the same size (of course, this should be done by linear regression...but humor me). Say len(x)=100.  Easy to vectorize with scalars a,b,c to get back a result of length 100.
Let's say that we know a,b,c should be inside of [-10,10] and I optimize by building a grid and picking the point with the min sum square error.
a=np.arange(-10.0, 10.01, 2.0)
nodes=np.array(np.meshgrid(a,a,a)).T.reshape(-1,3) #3-d cartesian product with array of nodes

For each of the 1331 nodes, I would like to calculate all 1331 of the length 100 return values. 
res=[]
x=np.random.uniform(-5.0,5.0, 100)
for node in nodes:
    res.append(quadratic_func(*node, x=x))

How can I take advantage of broadcasting so as to get my list of 1331 items each with 100 values that are the results of calling quadratic_func on x?  Answer must use vectorization, broadcasting, etc to get the orders of magnitude speed improvements I am looking for.  Also, the answer must use calls to quadratic_func - or more generally, my_func(*node, x=x).
In real life I am optimizing a non-linear function that is not even close to being convex and has many local minimums.  It is a great functional form to use if I can get to the ""right"" local minimum - I already know how to do that, but would like to get there faster!
",0,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
29141252,29141252,Should i use Regex.IsMatch before running Regex.Replace,"<p>I'm trying to use regex and regex does have its performance downside even if it's compiled.</p>

<p>What i want to know is, before actually running the <code>Regex.Replace</code> should i check first if there's a match in the string using <code>Regex.IsMatch</code>?</p>

<p>This question is about optimization and performance.</p>
","im trying to use regex and regex does have its performance downside even if its compiled
what i want to know is before actually running the regexreplace should i check first if theres a match in the string using regexismatch
this question is about optimization and performance
",1,"I'm trying to use regex and regex does have its performance downside even if it's compiled.
What i want to know is, before actually running the Regex.Replace should i check first if there's a match in the string using Regex.IsMatch?
This question is about optimization and performance.
","Should i use Regex.IsMatch before running Regex.Replace
I'm trying to use regex and regex does have its performance downside even if it's compiled.
What i want to know is, before actually running the Regex.Replace should i check first if there's a match in the string using Regex.IsMatch?
This question is about optimization and performance.
",0,1,0,1,0,0,0,- Is a theoretical question,"Label: 1

Reasoning:

This question expresses technical debt because it is concerned with optimizing the performance of a specific piece of code, specifically the use of regular expressions (regex) in string replacements. The user is aware of the performance downsides of using regex, even when compiled, and is seeking to improve the efficiency of their code.

The fact that the user is asking about checking for a match before performing the replacement using `Regex.IsMatch` suggests that they are experiencing performance issues or anticipate that they might occur. This implies that the current implementation is suboptimal and requires optimization to improve its performance.

Furthermore, the user explicitly states that the question is about optimization and performance, which reinforces the notion that the current code is not satisfactory and needs improvement. Therefore, I label this question as expressing technical debt due to its focus on code optimization and performance enhancement.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
37124591,37124591,"c++ Algorithm: searching for a subset of a 'list', non-ordered","<p>UPDATE: MY BAD. this was not the cause of the double slowdown. I had other bugs.</p>

<p>C++ MFC. Visual Studio 12.</p>

<p>I'm trying to optimize performance within a draw loop. I have a list of all my objects(ListAll), lets say it has 300 objects, all with unique ID's. I have a second list(ListNow) of the ID's which need to be rendered, size of 100. all the values in ListNow have associated objects stored in ListAll.</p>

<p>currently, ListAll is a CMap &lt; UINT, UINT, Object*, Object*>, and ListNow is a CArray&lt; UINT,UINT>.</p>

<pre><code>// this is the slower, current method
for (int i = 0; i &lt; ListNow.GetSize(); i++)
{
    UINT id = ListNow.GetAt(i);
    if (ListAll-&gt;Lookup(id, object))
    {
        object-&gt;draw();
    }
}
</code></pre>

<p>in the past I only had ListAll(CMap), and I called draw() on every object in it. It only had the 100 I wanted to draw, and I 'rebuilt' it every time i switched what was being drawn.</p>

<pre><code>// this is the faster, old method
POSITION pos = ListAll-&gt;GetStartPosition();
while (pos)
{
    ListAll-&gt;GetNextAssoc(pos, id, object);
    object-&gt;Draw();
}
</code></pre>

<p>Technically both algorithms perform at O(n) speed...but simply adding the CMap::Lookup function to the loop has doubled the time it takes. I have properly set my CMap size to a prime number larger than the number of objects in the CMap. This slowdown is blatant with lists of size 300,000 and above.</p>

<p>I switched to this system so that I could store all the objects in the draw lists, and could quickly swap between what is being drawn between different windows using the same object lists. This speeds up time when switching drastically but has slowed down each individual draw call. Switching back now is not an option, we knew it would slow down each draw call a bit, but not this much. The slowdown is definitely in the code I show you, because when i switch back to drawing everything(remove the lookup), it cuts time in half.</p>

<p>My only idea to increase performance is to record the LastDrawn object pointers in a list, and inform the function if it needs to change(call lookup()) or if it can simply re-use the last drawn(GetNext()). since 90% of the time, nothing has changed between calls.</p>

<p>Does anyone have a faster solution than this? I'm dreaming of a tricky bit masking solution that somehow produces the object pointers i want, I don't know. Anything would help at this point.</p>
","update my bad this was not the cause of the double slowdown i had other bugs
c mfc visual studio 12
im trying to optimize performance within a draw loop i have a list of all my objectslistall lets say it has 300 objects all with unique ids i have a second listlistnow of the ids which need to be rendered size of 100 all the values in listnow have associated objects stored in listall
currently listall is a cmap  uint uint object object and listnow is a carray uintuint
in the past i only had listallcmap and i called draw on every object in it it only had the 100 i wanted to draw and i rebuilt it every time i switched what was being drawn
technically both algorithms perform at on speedbut simply adding the cmaplookup function to the loop has doubled the time it takes i have properly set my cmap size to a prime number larger than the number of objects in the cmap this slowdown is blatant with lists of size 300000 and above
i switched to this system so that i could store all the objects in the draw lists and could quickly swap between what is being drawn between different windows using the same object lists this speeds up time when switching drastically but has slowed down each individual draw call switching back now is not an option we knew it would slow down each draw call a bit but not this much the slowdown is definitely in the code i show you because when i switch back to drawing everythingremove the lookup it cuts time in half
my only idea to increase performance is to record the lastdrawn object pointers in a list and inform the function if it needs to changecall lookup or if it can simply reuse the last drawngetnext since 90 of the time nothing has changed between calls
does anyone have a faster solution than this im dreaming of a tricky bit masking solution that somehow produces the object pointers i want i dont know anything would help at this point
",1,"UPDATE: MY BAD. this was not the cause of the double slowdown. I had other bugs.
C++ MFC. Visual Studio 12.
I'm trying to optimize performance within a draw loop. I have a list of all my objects(ListAll), lets say it has 300 objects, all with unique ID's. I have a second list(ListNow) of the ID's which need to be rendered, size of 100. all the values in ListNow have associated objects stored in ListAll.
currently, ListAll is a CMap < UINT, UINT, Object*, Object*>, and ListNow is a CArray< UINT,UINT>.
// this is the slower, current method
for (int i = 0; i < ListNow.GetSize(); i++)
{
    UINT id = ListNow.GetAt(i);
    if (ListAll->Lookup(id, object))
    {
        object->draw();
    }
}

in the past I only had ListAll(CMap), and I called draw() on every object in it. It only had the 100 I wanted to draw, and I 'rebuilt' it every time i switched what was being drawn.
// this is the faster, old method
POSITION pos = ListAll->GetStartPosition();
while (pos)
{
    ListAll->GetNextAssoc(pos, id, object);
    object->Draw();
}

Technically both algorithms perform at O(n) speed...but simply adding the CMap::Lookup function to the loop has doubled the time it takes. I have properly set my CMap size to a prime number larger than the number of objects in the CMap. This slowdown is blatant with lists of size 300,000 and above.
I switched to this system so that I could store all the objects in the draw lists, and could quickly swap between what is being drawn between different windows using the same object lists. This speeds up time when switching drastically but has slowed down each individual draw call. Switching back now is not an option, we knew it would slow down each draw call a bit, but not this much. The slowdown is definitely in the code I show you, because when i switch back to drawing everything(remove the lookup), it cuts time in half.
My only idea to increase performance is to record the LastDrawn object pointers in a list, and inform the function if it needs to change(call lookup()) or if it can simply re-use the last drawn(GetNext()). since 90% of the time, nothing has changed between calls.
Does anyone have a faster solution than this? I'm dreaming of a tricky bit masking solution that somehow produces the object pointers i want, I don't know. Anything would help at this point.
","c++ Algorithm: searching for a subset of a 'list', non-ordered
UPDATE: MY BAD. this was not the cause of the double slowdown. I had other bugs.
C++ MFC. Visual Studio 12.
I'm trying to optimize performance within a draw loop. I have a list of all my objects(ListAll), lets say it has 300 objects, all with unique ID's. I have a second list(ListNow) of the ID's which need to be rendered, size of 100. all the values in ListNow have associated objects stored in ListAll.
currently, ListAll is a CMap < UINT, UINT, Object*, Object*>, and ListNow is a CArray< UINT,UINT>.
// this is the slower, current method
for (int i = 0; i < ListNow.GetSize(); i++)
{
    UINT id = ListNow.GetAt(i);
    if (ListAll->Lookup(id, object))
    {
        object->draw();
    }
}

in the past I only had ListAll(CMap), and I called draw() on every object in it. It only had the 100 I wanted to draw, and I 'rebuilt' it every time i switched what was being drawn.
// this is the faster, old method
POSITION pos = ListAll->GetStartPosition();
while (pos)
{
    ListAll->GetNextAssoc(pos, id, object);
    object->Draw();
}

Technically both algorithms perform at O(n) speed...but simply adding the CMap::Lookup function to the loop has doubled the time it takes. I have properly set my CMap size to a prime number larger than the number of objects in the CMap. This slowdown is blatant with lists of size 300,000 and above.
I switched to this system so that I could store all the objects in the draw lists, and could quickly swap between what is being drawn between different windows using the same object lists. This speeds up time when switching drastically but has slowed down each individual draw call. Switching back now is not an option, we knew it would slow down each draw call a bit, but not this much. The slowdown is definitely in the code I show you, because when i switch back to drawing everything(remove the lookup), it cuts time in half.
My only idea to increase performance is to record the LastDrawn object pointers in a list, and inform the function if it needs to change(call lookup()) or if it can simply re-use the last drawn(GetNext()). since 90% of the time, nothing has changed between calls.
Does anyone have a faster solution than this? I'm dreaming of a tricky bit masking solution that somehow produces the object pointers i want, I don't know. Anything would help at this point.
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
50911787,50911787,"OpenMP reduction, variable not private?","<p>I have an array like this (0,0 is bottom left):</p>

<pre><code>0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0 0
0 0 1 0 1 0 1 0 0
0 0 1 1 1 1 1 1 1
1 0 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1
</code></pre>

<p>My goal is to get the index of the higher line who is not completely set to <code>0</code>. For this I made the code below (which works fine):</p>

<pre><code>max=0;
for (i=0 ; i&lt;width ; ++i) {
  for (j=max ; j&lt;height ; ++j) {
    if (array[i*height+j]!=0) {
      max=j;
    }
  }
}
</code></pre>

<p>For the second loop I initialize <code>j</code> to max, because the global maximum cannot be less than a local maximum. And this way I can reduce the number of tests.</p>

<p>The I tried to parallelize it with <code>OpenMp</code>. My code is now:</p>

<pre><code>max=0;
#pragma omp parallel for  default(none)                 \
                          shared(spec, width, height)   \
                          collapse(2)                   \
                          reduction(max:max)
for (i=0 ; i&lt;width ; ++i) {
  for (j=max ; j&lt;height ; ++j) {
    if (array[i*height+j]!=0) {
      max=j;
    }
  }
}
</code></pre>

<p>Which leads to a segmentation fault. In order to make it works, I changed <code>j=max</code> to <code>j=0</code>. So the problem seems to come from the <code>max</code> variable.</p>

<p>I don't understand why, because with the reduction this variable should be private (or lastprivate) between each threads. So why does it make it crash ? And how can I use my ""optimization"" with OpenMP ?</p>
","i have an array like this 00 is bottom left
my goal is to get the index of the higher line who is not completely set to 0 for this i made the code below which works fine
for the second loop i initialize j to max because the global maximum cannot be less than a local maximum and this way i can reduce the number of tests
the i tried to parallelize it with openmp my code is now
which leads to a segmentation fault in order to make it works i changed jmax to j0 so the problem seems to come from the max variable
i dont understand why because with the reduction this variable should be private or lastprivate between each threads so why does it make it crash  and how can i use my optimization with openmp 
",1,"I have an array like this (0,0 is bottom left):
0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0 0
0 0 1 0 1 0 1 0 0
0 0 1 1 1 1 1 1 1
1 0 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1

My goal is to get the index of the higher line who is not completely set to 0. For this I made the code below (which works fine):
max=0;
for (i=0 ; i<width ; ++i) {
  for (j=max ; j<height ; ++j) {
    if (array[i*height+j]!=0) {
      max=j;
    }
  }
}

For the second loop I initialize j to max, because the global maximum cannot be less than a local maximum. And this way I can reduce the number of tests.
The I tried to parallelize it with OpenMp. My code is now:
max=0;
#pragma omp parallel for  default(none)                 \
                          shared(spec, width, height)   \
                          collapse(2)                   \
                          reduction(max:max)
for (i=0 ; i<width ; ++i) {
  for (j=max ; j<height ; ++j) {
    if (array[i*height+j]!=0) {
      max=j;
    }
  }
}

Which leads to a segmentation fault. In order to make it works, I changed j=max to j=0. So the problem seems to come from the max variable.
I don't understand why, because with the reduction this variable should be private (or lastprivate) between each threads. So why does it make it crash ? And how can I use my ""optimization"" with OpenMP ?
","OpenMP reduction, variable not private?
I have an array like this (0,0 is bottom left):
0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0 0
0 0 1 0 1 0 1 0 0
0 0 1 1 1 1 1 1 1
1 0 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1

My goal is to get the index of the higher line who is not completely set to 0. For this I made the code below (which works fine):
max=0;
for (i=0 ; i<width ; ++i) {
  for (j=max ; j<height ; ++j) {
    if (array[i*height+j]!=0) {
      max=j;
    }
  }
}

For the second loop I initialize j to max, because the global maximum cannot be less than a local maximum. And this way I can reduce the number of tests.
The I tried to parallelize it with OpenMp. My code is now:
max=0;
#pragma omp parallel for  default(none)                 \
                          shared(spec, width, height)   \
                          collapse(2)                   \
                          reduction(max:max)
for (i=0 ; i<width ; ++i) {
  for (j=max ; j<height ; ++j) {
    if (array[i*height+j]!=0) {
      max=j;
    }
  }
}

Which leads to a segmentation fault. In order to make it works, I changed j=max to j=0. So the problem seems to come from the max variable.
I don't understand why, because with the reduction this variable should be private (or lastprivate) between each threads. So why does it make it crash ? And how can I use my ""optimization"" with OpenMP ?
",1,1,0,0,0,0,1,,,,1,1,Multithreading and Concurrency,Multithreading and Concurrency,,Multithreading and Concurrency,1,1,0,0,0,0,,,Design,Design,,Design,Moderate,Moderate,,Moderate
14247956,14247956,Graphics.DrawImage speed,"<p>In my program, I'm coding a basic image editor. Part of this allows the user to draw a rectangular region and I pop up a display that shows that region zoomed by 3x or so (which they can adjust further with the mouse wheel). If they right click and drag this image, it will move the zoom region around on the original image, basically acting as a magnifying glass.</p>

<p>The problem is, I'm seeing some serious performance issues even on relatively small bitmaps. If the bitmap showing the zoomed region is around 400x400 it's still updating as fast as mouse can move and is perfectly smooth, but if I mouse wheel the zoom up to around 450x450, it immediately starts chunking, only down to around 2 updates per second, if that. I don't understand why such a small increase incurs such an enormous performance problem... it's like I've hit some internal memory limit or something. It doesn't seem to matter the size of the source bitmap that is being zoomed, just the size of the zoomed bitmap.</p>

<p>The problem is that I'm using Graphics.DrawImage and a PictureBox. Reading around this site, I see that the performance for both of these is typically not very good, but I don't know enough about the inner workings of GDI to improve my speed. I was hoping some of you might know where my bottlenecks are, as I'm likely just using these tools in poor ways or don't know of a better tool to use in its place.</p>

<p>Here are some snippets of my mouse events and related functions.</p>

<pre><code>private void pictureBox_MouseDown(object sender, MouseEventArgs e)
    {

        else if (e.Button == System.Windows.Forms.MouseButtons.Right)
        {
            // slide the zoomed part to look at a different area of the original image
            if (zoomFactor &gt; 1)
            {
                isMovingZoom = true;
                // try saving the graphics object?? are these settings helping at all??
                zoomingGraphics = Graphics.FromImage(displayImage);
                zoomingGraphics.CompositingQuality = System.Drawing.Drawing2D.CompositingQuality.HighSpeed;
                zoomingGraphics.InterpolationMode = System.Drawing.Drawing2D.InterpolationMode.Low;
                zoomingGraphics.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.HighSpeed;
                zoomingGraphics.PixelOffsetMode = PixelOffsetMode.HighSpeed;
            }
        }
    }


private void pictureBox_MouseMove(object sender, MouseEventArgs e)
    {
        if (isMovingZoom)
        {
            // some computation on where they moved mouse ommitted here

            zoomRegion.X = originalZoomRegion.X + delta.X;
            zoomRegion.Y = originalZoomRegion.Y + delta.Y;
            zoomRegionEnlarged = scaleToOriginal(zoomRegion);

            // overwrite the existing displayImage to prevent more Bitmaps being allocated
            createZoomedImage(image.Bitmap, zoomRegionEnlarged, zoomFactor, displayImage, zoomingGraphics);
        }
    }

private void createZoomedImage(Bitmap source, Rectangle srcRegion, float zoom, Bitmap output, Graphics outputGraphics)
    {
        Rectangle destRect = new Rectangle(0, 0, (int)(srcRegion.Width * zoom), (int)(srcRegion.Height * zoom));

            outputGraphics.DrawImage(source, destRect, srcRegion, GraphicsUnit.Pixel);

        if (displayImage != originalDisplayImage &amp;&amp; displayImage != output)
            displayImage.Dispose();
        setImageInBox(output);
    }

// sets the picture box image, as well as resizes the window to fit
    void setImageInBox(Bitmap bmp)
    {
        pictureBox.Image = bmp;
        displayImage = bmp;
        this.Width = pictureBox.Width + okButton.Width + SystemInformation.FrameBorderSize.Width * 2 + 25;
        this.Height = Math.Max(450, pictureBox.Height) + SystemInformation.CaptionHeight + SystemInformation.FrameBorderSize.Height * 2 + 20;
    }

private void pictureBox_MouseUp(object sender, MouseEventArgs e)
    {
        else if (e.Button == System.Windows.Forms.MouseButtons.Right)
        {
            if (isMovingZoom)
            {
                isMovingZoom = false;
                zoomingGraphics.Dispose();
            }
        }
    }
</code></pre>

<p>As you can see, I'm not declaring a new Bitmap every time I want to draw something, I'm reusing an old Bitmap (and the Bitmap's graphics object, though I don't know if there is much cost with calling Graphics.FromImage repeatedly). I tried adding Stopwatches around to benchmark my code, but I think DrawImage passes functionality to another thread so the function claims to be done relatively quickly. I'm trying to <code>Dispose</code> all my Bitmap and Graphics objects when I'm not using them, and avoid repeated calls to allocate/deallocate resources during the <code>MouseMove</code> event. I'm using a <code>PictureBox</code> but I don't think that's the problem here.</p>

<p>Any help to speed up this code or teach me what's happening in DrawImage is appreciated! I've trimmed some excess code to make it more presentable, but if I've accidentally trimmed something important, or don't show how I'm using something which may be causing problems, please let me know and I'll revise the post.</p>
","in my program im coding a basic image editor part of this allows the user to draw a rectangular region and i pop up a display that shows that region zoomed by 3x or so which they can adjust further with the mouse wheel if they right click and drag this image it will move the zoom region around on the original image basically acting as a magnifying glass
the problem is im seeing some serious performance issues even on relatively small bitmaps if the bitmap showing the zoomed region is around 400x400 its still updating as fast as mouse can move and is perfectly smooth but if i mouse wheel the zoom up to around 450x450 it immediately starts chunking only down to around 2 updates per second if that i dont understand why such a small increase incurs such an enormous performance problem its like ive hit some internal memory limit or something it doesnt seem to matter the size of the source bitmap that is being zoomed just the size of the zoomed bitmap
the problem is that im using graphicsdrawimage and a picturebox reading around this site i see that the performance for both of these is typically not very good but i dont know enough about the inner workings of gdi to improve my speed i was hoping some of you might know where my bottlenecks are as im likely just using these tools in poor ways or dont know of a better tool to use in its place
here are some snippets of my mouse events and related functions
as you can see im not declaring a new bitmap every time i want to draw something im reusing an old bitmap and the bitmaps graphics object though i dont know if there is much cost with calling graphicsfromimage repeatedly i tried adding stopwatches around to benchmark my code but i think drawimage passes functionality to another thread so the function claims to be done relatively quickly im trying to dispose all my bitmap and graphics objects when im not using them and avoid repeated calls to allocatedeallocate resources during the mousemove event im using a picturebox but i dont think thats the problem here
any help to speed up this code or teach me whats happening in drawimage is appreciated ive trimmed some excess code to make it more presentable but if ive accidentally trimmed something important or dont show how im using something which may be causing problems please let me know and ill revise the post
",1,"In my program, I'm coding a basic image editor. Part of this allows the user to draw a rectangular region and I pop up a display that shows that region zoomed by 3x or so (which they can adjust further with the mouse wheel). If they right click and drag this image, it will move the zoom region around on the original image, basically acting as a magnifying glass.
The problem is, I'm seeing some serious performance issues even on relatively small bitmaps. If the bitmap showing the zoomed region is around 400x400 it's still updating as fast as mouse can move and is perfectly smooth, but if I mouse wheel the zoom up to around 450x450, it immediately starts chunking, only down to around 2 updates per second, if that. I don't understand why such a small increase incurs such an enormous performance problem... it's like I've hit some internal memory limit or something. It doesn't seem to matter the size of the source bitmap that is being zoomed, just the size of the zoomed bitmap.
The problem is that I'm using Graphics.DrawImage and a PictureBox. Reading around this site, I see that the performance for both of these is typically not very good, but I don't know enough about the inner workings of GDI to improve my speed. I was hoping some of you might know where my bottlenecks are, as I'm likely just using these tools in poor ways or don't know of a better tool to use in its place.
Here are some snippets of my mouse events and related functions.
private void pictureBox_MouseDown(object sender, MouseEventArgs e)
    {

        else if (e.Button == System.Windows.Forms.MouseButtons.Right)
        {
            // slide the zoomed part to look at a different area of the original image
            if (zoomFactor > 1)
            {
                isMovingZoom = true;
                // try saving the graphics object?? are these settings helping at all??
                zoomingGraphics = Graphics.FromImage(displayImage);
                zoomingGraphics.CompositingQuality = System.Drawing.Drawing2D.CompositingQuality.HighSpeed;
                zoomingGraphics.InterpolationMode = System.Drawing.Drawing2D.InterpolationMode.Low;
                zoomingGraphics.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.HighSpeed;
                zoomingGraphics.PixelOffsetMode = PixelOffsetMode.HighSpeed;
            }
        }
    }


private void pictureBox_MouseMove(object sender, MouseEventArgs e)
    {
        if (isMovingZoom)
        {
            // some computation on where they moved mouse ommitted here

            zoomRegion.X = originalZoomRegion.X + delta.X;
            zoomRegion.Y = originalZoomRegion.Y + delta.Y;
            zoomRegionEnlarged = scaleToOriginal(zoomRegion);

            // overwrite the existing displayImage to prevent more Bitmaps being allocated
            createZoomedImage(image.Bitmap, zoomRegionEnlarged, zoomFactor, displayImage, zoomingGraphics);
        }
    }

private void createZoomedImage(Bitmap source, Rectangle srcRegion, float zoom, Bitmap output, Graphics outputGraphics)
    {
        Rectangle destRect = new Rectangle(0, 0, (int)(srcRegion.Width * zoom), (int)(srcRegion.Height * zoom));

            outputGraphics.DrawImage(source, destRect, srcRegion, GraphicsUnit.Pixel);

        if (displayImage != originalDisplayImage && displayImage != output)
            displayImage.Dispose();
        setImageInBox(output);
    }

// sets the picture box image, as well as resizes the window to fit
    void setImageInBox(Bitmap bmp)
    {
        pictureBox.Image = bmp;
        displayImage = bmp;
        this.Width = pictureBox.Width + okButton.Width + SystemInformation.FrameBorderSize.Width * 2 + 25;
        this.Height = Math.Max(450, pictureBox.Height) + SystemInformation.CaptionHeight + SystemInformation.FrameBorderSize.Height * 2 + 20;
    }

private void pictureBox_MouseUp(object sender, MouseEventArgs e)
    {
        else if (e.Button == System.Windows.Forms.MouseButtons.Right)
        {
            if (isMovingZoom)
            {
                isMovingZoom = false;
                zoomingGraphics.Dispose();
            }
        }
    }

As you can see, I'm not declaring a new Bitmap every time I want to draw something, I'm reusing an old Bitmap (and the Bitmap's graphics object, though I don't know if there is much cost with calling Graphics.FromImage repeatedly). I tried adding Stopwatches around to benchmark my code, but I think DrawImage passes functionality to another thread so the function claims to be done relatively quickly. I'm trying to Dispose all my Bitmap and Graphics objects when I'm not using them, and avoid repeated calls to allocate/deallocate resources during the MouseMove event. I'm using a PictureBox but I don't think that's the problem here.
Any help to speed up this code or teach me what's happening in DrawImage is appreciated! I've trimmed some excess code to make it more presentable, but if I've accidentally trimmed something important, or don't show how I'm using something which may be causing problems, please let me know and I'll revise the post.
","Graphics.DrawImage speed
In my program, I'm coding a basic image editor. Part of this allows the user to draw a rectangular region and I pop up a display that shows that region zoomed by 3x or so (which they can adjust further with the mouse wheel). If they right click and drag this image, it will move the zoom region around on the original image, basically acting as a magnifying glass.
The problem is, I'm seeing some serious performance issues even on relatively small bitmaps. If the bitmap showing the zoomed region is around 400x400 it's still updating as fast as mouse can move and is perfectly smooth, but if I mouse wheel the zoom up to around 450x450, it immediately starts chunking, only down to around 2 updates per second, if that. I don't understand why such a small increase incurs such an enormous performance problem... it's like I've hit some internal memory limit or something. It doesn't seem to matter the size of the source bitmap that is being zoomed, just the size of the zoomed bitmap.
The problem is that I'm using Graphics.DrawImage and a PictureBox. Reading around this site, I see that the performance for both of these is typically not very good, but I don't know enough about the inner workings of GDI to improve my speed. I was hoping some of you might know where my bottlenecks are, as I'm likely just using these tools in poor ways or don't know of a better tool to use in its place.
Here are some snippets of my mouse events and related functions.
private void pictureBox_MouseDown(object sender, MouseEventArgs e)
    {

        else if (e.Button == System.Windows.Forms.MouseButtons.Right)
        {
            // slide the zoomed part to look at a different area of the original image
            if (zoomFactor > 1)
            {
                isMovingZoom = true;
                // try saving the graphics object?? are these settings helping at all??
                zoomingGraphics = Graphics.FromImage(displayImage);
                zoomingGraphics.CompositingQuality = System.Drawing.Drawing2D.CompositingQuality.HighSpeed;
                zoomingGraphics.InterpolationMode = System.Drawing.Drawing2D.InterpolationMode.Low;
                zoomingGraphics.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.HighSpeed;
                zoomingGraphics.PixelOffsetMode = PixelOffsetMode.HighSpeed;
            }
        }
    }


private void pictureBox_MouseMove(object sender, MouseEventArgs e)
    {
        if (isMovingZoom)
        {
            // some computation on where they moved mouse ommitted here

            zoomRegion.X = originalZoomRegion.X + delta.X;
            zoomRegion.Y = originalZoomRegion.Y + delta.Y;
            zoomRegionEnlarged = scaleToOriginal(zoomRegion);

            // overwrite the existing displayImage to prevent more Bitmaps being allocated
            createZoomedImage(image.Bitmap, zoomRegionEnlarged, zoomFactor, displayImage, zoomingGraphics);
        }
    }

private void createZoomedImage(Bitmap source, Rectangle srcRegion, float zoom, Bitmap output, Graphics outputGraphics)
    {
        Rectangle destRect = new Rectangle(0, 0, (int)(srcRegion.Width * zoom), (int)(srcRegion.Height * zoom));

            outputGraphics.DrawImage(source, destRect, srcRegion, GraphicsUnit.Pixel);

        if (displayImage != originalDisplayImage && displayImage != output)
            displayImage.Dispose();
        setImageInBox(output);
    }

// sets the picture box image, as well as resizes the window to fit
    void setImageInBox(Bitmap bmp)
    {
        pictureBox.Image = bmp;
        displayImage = bmp;
        this.Width = pictureBox.Width + okButton.Width + SystemInformation.FrameBorderSize.Width * 2 + 25;
        this.Height = Math.Max(450, pictureBox.Height) + SystemInformation.CaptionHeight + SystemInformation.FrameBorderSize.Height * 2 + 20;
    }

private void pictureBox_MouseUp(object sender, MouseEventArgs e)
    {
        else if (e.Button == System.Windows.Forms.MouseButtons.Right)
        {
            if (isMovingZoom)
            {
                isMovingZoom = false;
                zoomingGraphics.Dispose();
            }
        }
    }

As you can see, I'm not declaring a new Bitmap every time I want to draw something, I'm reusing an old Bitmap (and the Bitmap's graphics object, though I don't know if there is much cost with calling Graphics.FromImage repeatedly). I tried adding Stopwatches around to benchmark my code, but I think DrawImage passes functionality to another thread so the function claims to be done relatively quickly. I'm trying to Dispose all my Bitmap and Graphics objects when I'm not using them, and avoid repeated calls to allocate/deallocate resources during the MouseMove event. I'm using a PictureBox but I don't think that's the problem here.
Any help to speed up this code or teach me what's happening in DrawImage is appreciated! I've trimmed some excess code to make it more presentable, but if I've accidentally trimmed something important, or don't show how I'm using something which may be causing problems, please let me know and I'll revise the post.
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
60253557,60253557,Parse data to mappable nested list | React,"<p>I'm building a database where you can add comments to meetings and replies to those meetings. I took inspiration from <a href=""https://phpro.org/tutorials/Creating-Threaded-Comments-With-PHP-And-Postgresql-Recursive-Query.html"" rel=""nofollow noreferrer"">this article</a> since I've never done anything like this before.</p>
<p>I'm fetching my comments from my database and the structure looks like this:</p>
<pre><code>&quot;comments&quot;: [
        {
          &quot;id&quot;: 5,
          &quot;content&quot;: &quot;First comment&quot;,
          &quot;path&quot;: [
            5
          ],
          &quot;depth&quot;: 1
        },
        {
          &quot;id&quot;: 6,
          &quot;content&quot;: &quot;First reply on first comment&quot;,
          &quot;path&quot;: [
            5,
            6
          ],
          &quot;depth&quot;: 2
        },
        {
          &quot;id&quot;: 7,
          &quot;content&quot;: &quot;Second comment&quot;,
          &quot;path&quot;: [
            7
          ],
          &quot;depth&quot;: 1
        },
        {
          &quot;id&quot;: 8,
          &quot;content&quot;: &quot;First reply on second comment&quot;,
          &quot;path&quot;: [
            7,
            8
          ],
          &quot;depth&quot;: 2
        },
        {
          &quot;id&quot;: 9,
          &quot;content&quot;: &quot;Second reply on second comment&quot;,
          &quot;path&quot;: [
            7,
            9
          ],
          &quot;depth&quot;: 2
        },
        {
          &quot;id&quot;: 10,
          &quot;content&quot;: &quot;First reply on second comment second reply&quot;,
          &quot;path&quot;: [
            7,
            9,
            10
          ],
          &quot;depth&quot;: 3
        }
      ]
</code></pre>
<p>Currently, I'm trying to create a nested list with my React application and display the comments in a similar way as StackOverflow does. I have an idea of how the structure will look like, but I'm not too sure how to implement it.</p>
<pre><code>comments: [
    {
        id: 5,
        content: &quot;First comment&quot;,
        replies: [
            {
                id: 6,
                content: &quot;First reply on first comment&quot;,
                depth: 2                               
            }
        ]
    },
    {
        id: 100,
        content: &quot;Comment with no replies&quot;,
        replies: []
    }

]
</code></pre>
<p>Any help on how to get started on this is greatly appreciated and I hope I've described my problem clearly.</p>
<p>Thanks for reading and have a nice day.</p>
","im building a database where you can add comments to meetings and replies to those meetings i took inspiration from this article since ive never done anything like this before
im fetching my comments from my database and the structure looks like this
currently im trying to create a nested list with my react application and display the comments in a similar way as stackoverflow does i have an idea of how the structure will look like but im not too sure how to implement it
any help on how to get started on this is greatly appreciated and i hope ive described my problem clearly
thanks for reading and have a nice day
",1,"I'm building a database where you can add comments to meetings and replies to those meetings. I took inspiration from this article since I've never done anything like this before.
I'm fetching my comments from my database and the structure looks like this:
""comments"": [
        {
          ""id"": 5,
          ""content"": ""First comment"",
          ""path"": [
            5
          ],
          ""depth"": 1
        },
        {
          ""id"": 6,
          ""content"": ""First reply on first comment"",
          ""path"": [
            5,
            6
          ],
          ""depth"": 2
        },
        {
          ""id"": 7,
          ""content"": ""Second comment"",
          ""path"": [
            7
          ],
          ""depth"": 1
        },
        {
          ""id"": 8,
          ""content"": ""First reply on second comment"",
          ""path"": [
            7,
            8
          ],
          ""depth"": 2
        },
        {
          ""id"": 9,
          ""content"": ""Second reply on second comment"",
          ""path"": [
            7,
            9
          ],
          ""depth"": 2
        },
        {
          ""id"": 10,
          ""content"": ""First reply on second comment second reply"",
          ""path"": [
            7,
            9,
            10
          ],
          ""depth"": 3
        }
      ]

Currently, I'm trying to create a nested list with my React application and display the comments in a similar way as StackOverflow does. I have an idea of how the structure will look like, but I'm not too sure how to implement it.
comments: [
    {
        id: 5,
        content: ""First comment"",
        replies: [
            {
                id: 6,
                content: ""First reply on first comment"",
                depth: 2                               
            }
        ]
    },
    {
        id: 100,
        content: ""Comment with no replies"",
        replies: []
    }

]

Any help on how to get started on this is greatly appreciated and I hope I've described my problem clearly.
Thanks for reading and have a nice day.
","Parse data to mappable nested list | React
I'm building a database where you can add comments to meetings and replies to those meetings. I took inspiration from this article since I've never done anything like this before.
I'm fetching my comments from my database and the structure looks like this:
""comments"": [
        {
          ""id"": 5,
          ""content"": ""First comment"",
          ""path"": [
            5
          ],
          ""depth"": 1
        },
        {
          ""id"": 6,
          ""content"": ""First reply on first comment"",
          ""path"": [
            5,
            6
          ],
          ""depth"": 2
        },
        {
          ""id"": 7,
          ""content"": ""Second comment"",
          ""path"": [
            7
          ],
          ""depth"": 1
        },
        {
          ""id"": 8,
          ""content"": ""First reply on second comment"",
          ""path"": [
            7,
            8
          ],
          ""depth"": 2
        },
        {
          ""id"": 9,
          ""content"": ""Second reply on second comment"",
          ""path"": [
            7,
            9
          ],
          ""depth"": 2
        },
        {
          ""id"": 10,
          ""content"": ""First reply on second comment second reply"",
          ""path"": [
            7,
            9,
            10
          ],
          ""depth"": 3
        }
      ]

Currently, I'm trying to create a nested list with my React application and display the comments in a similar way as StackOverflow does. I have an idea of how the structure will look like, but I'm not too sure how to implement it.
comments: [
    {
        id: 5,
        content: ""First comment"",
        replies: [
            {
                id: 6,
                content: ""First reply on first comment"",
                depth: 2                               
            }
        ]
    },
    {
        id: 100,
        content: ""Comment with no replies"",
        replies: []
    }

]

Any help on how to get started on this is greatly appreciated and I hope I've described my problem clearly.
Thanks for reading and have a nice day.
",0,1,0,0,0,0,0,- User just doesn't know how to approach a problem,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
28670440,28670440,"jquery performance issue, action registering","<p>I generate a page containing 100 textfields, each have the same action registered on it, but with different id and values:</p>

<pre><code>&lt;input type=""textfield"" id=""tf1"" value=""tf1"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""


&lt;input type=""textfield"" id=""tf2"" value=""tf2"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""

..

&lt;input type=""textfield"" id=""tf00"" value=""tf100"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""
</code></pre>

<p>I generate the html content using servlet, send to browser using ajax set them to the page using jquery. I have network performance problems, so have to minimize the content sent from servlet. What would be the best solution to minimize the content?</p>

<p>1-using strig compression on servlet side(java) and string decompress on clientjavascript)?, because we have many common templates, the compression can provide up to 90% compress ratio.</p>

<p>2-assinging a class to each textfield, then registering to them using jquery</p>

<pre><code>$('.textfieldclass').onBlur(..)
</code></pre>
","i generate a page containing 100 textfields each have the same action registered on it but with different id and values
i generate the html content using servlet send to browser using ajax set them to the page using jquery i have network performance problems so have to minimize the content sent from servlet what would be the best solution to minimize the content
1using strig compression on servlet sidejava and string decompress on clientjavascript because we have many common templates the compression can provide up to 90 compress ratio
2assinging a class to each textfield then registering to them using jquery
",1,"I generate a page containing 100 textfields, each have the same action registered on it, but with different id and values:
<input type=""textfield"" id=""tf1"" value=""tf1"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""


<input type=""textfield"" id=""tf2"" value=""tf2"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""

..

<input type=""textfield"" id=""tf00"" value=""tf100"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""

I generate the html content using servlet, send to browser using ajax set them to the page using jquery. I have network performance problems, so have to minimize the content sent from servlet. What would be the best solution to minimize the content?
1-using strig compression on servlet side(java) and string decompress on clientjavascript)?, because we have many common templates, the compression can provide up to 90% compress ratio.
2-assinging a class to each textfield, then registering to them using jquery
$('.textfieldclass').onBlur(..)

","jquery performance issue, action registering
I generate a page containing 100 textfields, each have the same action registered on it, but with different id and values:
<input type=""textfield"" id=""tf1"" value=""tf1"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""


<input type=""textfield"" id=""tf2"" value=""tf2"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""

..

<input type=""textfield"" id=""tf00"" value=""tf100"" onClick=""textfieldclicked(this);"" onChange=""textfieldchanged(this);"" onBlur=""textfieldblur(this);"" onFocus=""textfieldfocus(this)""

I generate the html content using servlet, send to browser using ajax set them to the page using jquery. I have network performance problems, so have to minimize the content sent from servlet. What would be the best solution to minimize the content?
1-using strig compression on servlet side(java) and string decompress on clientjavascript)?, because we have many common templates, the compression can provide up to 90% compress ratio.
2-assinging a class to each textfield, then registering to them using jquery
$('.textfieldclass').onBlur(..)

",1,1,0,0,0,0,1,,,,1,1,Web Performance,Web Performance,,Web Performance,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Moderate,Moderate,,Moderate
15595476,15595476,Refactor the foreach loop with continue in it?,"<p>I was trying to refactor a very long <code>foreach</code> loop. I got stuck as at few places I am having <code>continue</code> in it. Here is some dummy code to mimic my original code</p>

<pre><code>IEnumerable&lt;dynamic&gt; allData = FetchAllData();
IEnumerable&lt;dynamic&gt; relativeData = FetchAllRelativData();

foreach (var rdata in relativeData)
{
    IEnumerable&lt;dynamic&gt; dataTobeProcessed = allData.Where(c =&gt; c.Name = rdata.Name);

    //Do something

    //if then Continue

    // do something

    //if then continue

    // do something

    // do something

    // add data to db
}
</code></pre>

<p>Here I have remove all the do something part in to separate function. But again I am having few code portion having <code>continue</code> in it, that is breaking loop and moving forward to next element. So, I was unable to separate that part.</p>

<p>Again, here I am having constraint like steps are in continuation. Means first thing happens then second one and then third one then next... So, I can't move the position of code as of now. Because of that my function got big, around 300 lines and it is now getting hard to change or maintain. </p>

<p>Please provide a good solution for removing <code>continue</code> or another way to <code>break</code> method by changing the way of coding.</p>

<p>Please let me know if any other details are needed. </p>
","i was trying to refactor a very long foreach loop i got stuck as at few places i am having continue in it here is some dummy code to mimic my original code
here i have remove all the do something part in to separate function but again i am having few code portion having continue in it that is breaking loop and moving forward to next element so i was unable to separate that part
again here i am having constraint like steps are in continuation means first thing happens then second one and then third one then next so i cant move the position of code as of now because of that my function got big around 300 lines and it is now getting hard to change or maintain 
please provide a good solution for removing continue or another way to break method by changing the way of coding
please let me know if any other details are needed 
",1,"I was trying to refactor a very long foreach loop. I got stuck as at few places I am having continue in it. Here is some dummy code to mimic my original code
IEnumerable<dynamic> allData = FetchAllData();
IEnumerable<dynamic> relativeData = FetchAllRelativData();

foreach (var rdata in relativeData)
{
    IEnumerable<dynamic> dataTobeProcessed = allData.Where(c => c.Name = rdata.Name);

    //Do something

    //if then Continue

    // do something

    //if then continue

    // do something

    // do something

    // add data to db
}

Here I have remove all the do something part in to separate function. But again I am having few code portion having continue in it, that is breaking loop and moving forward to next element. So, I was unable to separate that part.
Again, here I am having constraint like steps are in continuation. Means first thing happens then second one and then third one then next... So, I can't move the position of code as of now. Because of that my function got big, around 300 lines and it is now getting hard to change or maintain. 
Please provide a good solution for removing continue or another way to break method by changing the way of coding.
Please let me know if any other details are needed. 
","Refactor the foreach loop with continue in it?
I was trying to refactor a very long foreach loop. I got stuck as at few places I am having continue in it. Here is some dummy code to mimic my original code
IEnumerable<dynamic> allData = FetchAllData();
IEnumerable<dynamic> relativeData = FetchAllRelativData();

foreach (var rdata in relativeData)
{
    IEnumerable<dynamic> dataTobeProcessed = allData.Where(c => c.Name = rdata.Name);

    //Do something

    //if then Continue

    // do something

    //if then continue

    // do something

    // do something

    // add data to db
}

Here I have remove all the do something part in to separate function. But again I am having few code portion having continue in it, that is breaking loop and moving forward to next element. So, I was unable to separate that part.
Again, here I am having constraint like steps are in continuation. Means first thing happens then second one and then third one then next... So, I can't move the position of code as of now. Because of that my function got big, around 300 lines and it is now getting hard to change or maintain. 
Please provide a good solution for removing continue or another way to break method by changing the way of coding.
Please let me know if any other details are needed. 
",1,1,0,0,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,1,1,0,0,0,0,,,Code,Code,,Code,Severe,Severe,,Severe
27936988,27936988,jQuery Replicate an existing Div Multiple Times,"<p>I am building a search query which gives me results.</p>

<p>I have a template ready for the item inside a hidden div. What I want to do is replicate the template n number of times using jQuery. </p>

<p>So For example:
I search for flights and <strong>I get 5 search results</strong>, I need to <strong>replicate the below div template</strong> <strong>5 Times</strong> </p>

<pre><code>&lt;div id=""oneWayFlightElement"" class=""displayNone""&gt;
        &lt;div id=""flightIndex1"" class=""flightDetailElement boxShadowTheme""&gt;
                &lt;div id=""flightDetailsLeftPanel1"" class=""flightDetailsLeftPanel marginBottom10""&gt;
                    &lt;div class=""fullWidth marginTop10""&gt;
                        &lt;span id=""flightPriceLabel1"" class=""headerFontStyle fullWidth boldFont""&gt;Rs 9500.00&lt;/span&gt;&lt;hr/&gt;
                           &lt;div id=""homeToDestination1"" class=""flightBlockStyle""&gt;
                            &lt;span id=""flightNumberFromHome1"" class=""fontSize16""&gt;AI-202&lt;/span&gt;&lt;br/&gt;
                               &lt;span id=""flightRouteFromHome1"" class=""fontSize26""&gt;PNQ &gt; DEL&lt;/span&gt;&lt;br/&gt;
                               &lt;span id=""flightDepartTimeFromHome1"" class=""fontSize26""&gt;Depart: 10.00 AM&lt;/span&gt;&lt;br/&gt;
                               &lt;span id=""flightArrivalTimeFromHome1"" class=""fontSize26""&gt;Arrive: 12.00 PM&lt;/span&gt;&lt;br/&gt;
                           &lt;/div&gt;
                           &lt;div id=""destinationToHome1"" class=""flightBlockStyle""&gt;
                            &lt;span id=""flightNumberToHome1"" class=""fontSize16""&gt;AI-202&lt;/span&gt;&lt;br/&gt;
                               &lt;span id=""flightRouteToHome1"" class=""fontSize26""&gt;PNQ &gt; DEL&lt;/span&gt;&lt;br/&gt;
                               &lt;span id=""flightDepartTimeToHome1"" class=""fontSize26""&gt;Depart: 10.00 AM&lt;/span&gt;&lt;br/&gt;
                               &lt;span id=""flightArrivalTimeToHome1"" class=""fontSize26""&gt;Arrive: 12.00 PM&lt;/span&gt;&lt;br/&gt;
                           &lt;/div&gt;
                       &lt;/div&gt;
                   &lt;/div&gt;
                   &lt;div id=""flightDetailsRightPanel1"" class=""flightDetailsRightPanel textAlignRight marginBottom10""&gt;
                    &lt;img src=""images/flightIcon.png"" class=""marginRight10 marginTop10 width40""/&gt;&lt;br/&gt;
                       &lt;button class=""marginRight10 marginBottom10 width40 bookNowButtonStyle""&gt;Book Now&lt;/button&gt;
                   &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
</code></pre>

<p>Inside this div for 5 times</p>

<pre><code>&lt;div id=""searchFlightResultDiv"" class=""fullWidth"" style=""border:solid""&gt;

                &lt;/div&gt;
</code></pre>

<p>Is there a better way to do that rather than string appending in jQuery?</p>

<p>Thanks,
Ankit Tanna</p>
","i am building a search query which gives me results
i have a template ready for the item inside a hidden div what i want to do is replicate the template n number of times using jquery 
so for example
i search for flights and i get 5 search results i need to replicate the below div template 5 times 
inside this div for 5 times
is there a better way to do that rather than string appending in jquery
thanks
ankit tanna
",1,"I am building a search query which gives me results.
I have a template ready for the item inside a hidden div. What I want to do is replicate the template n number of times using jQuery. 
So For example:
I search for flights and I get 5 search results, I need to replicate the below div template 5 Times 
<div id=""oneWayFlightElement"" class=""displayNone"">
        <div id=""flightIndex1"" class=""flightDetailElement boxShadowTheme"">
                <div id=""flightDetailsLeftPanel1"" class=""flightDetailsLeftPanel marginBottom10"">
                    <div class=""fullWidth marginTop10"">
                        <span id=""flightPriceLabel1"" class=""headerFontStyle fullWidth boldFont"">Rs 9500.00</span><hr/>
                           <div id=""homeToDestination1"" class=""flightBlockStyle"">
                            <span id=""flightNumberFromHome1"" class=""fontSize16"">AI-202</span><br/>
                               <span id=""flightRouteFromHome1"" class=""fontSize26"">PNQ > DEL</span><br/>
                               <span id=""flightDepartTimeFromHome1"" class=""fontSize26"">Depart: 10.00 AM</span><br/>
                               <span id=""flightArrivalTimeFromHome1"" class=""fontSize26"">Arrive: 12.00 PM</span><br/>
                           </div>
                           <div id=""destinationToHome1"" class=""flightBlockStyle"">
                            <span id=""flightNumberToHome1"" class=""fontSize16"">AI-202</span><br/>
                               <span id=""flightRouteToHome1"" class=""fontSize26"">PNQ > DEL</span><br/>
                               <span id=""flightDepartTimeToHome1"" class=""fontSize26"">Depart: 10.00 AM</span><br/>
                               <span id=""flightArrivalTimeToHome1"" class=""fontSize26"">Arrive: 12.00 PM</span><br/>
                           </div>
                       </div>
                   </div>
                   <div id=""flightDetailsRightPanel1"" class=""flightDetailsRightPanel textAlignRight marginBottom10"">
                    <img src=""images/flightIcon.png"" class=""marginRight10 marginTop10 width40""/><br/>
                       <button class=""marginRight10 marginBottom10 width40 bookNowButtonStyle"">Book Now</button>
                   </div>
                </div>
    </div>

Inside this div for 5 times
<div id=""searchFlightResultDiv"" class=""fullWidth"" style=""border:solid"">

                </div>

Is there a better way to do that rather than string appending in jQuery?
Thanks,
Ankit Tanna
","jQuery Replicate an existing Div Multiple Times
I am building a search query which gives me results.
I have a template ready for the item inside a hidden div. What I want to do is replicate the template n number of times using jQuery. 
So For example:
I search for flights and I get 5 search results, I need to replicate the below div template 5 Times 
<div id=""oneWayFlightElement"" class=""displayNone"">
        <div id=""flightIndex1"" class=""flightDetailElement boxShadowTheme"">
                <div id=""flightDetailsLeftPanel1"" class=""flightDetailsLeftPanel marginBottom10"">
                    <div class=""fullWidth marginTop10"">
                        <span id=""flightPriceLabel1"" class=""headerFontStyle fullWidth boldFont"">Rs 9500.00</span><hr/>
                           <div id=""homeToDestination1"" class=""flightBlockStyle"">
                            <span id=""flightNumberFromHome1"" class=""fontSize16"">AI-202</span><br/>
                               <span id=""flightRouteFromHome1"" class=""fontSize26"">PNQ > DEL</span><br/>
                               <span id=""flightDepartTimeFromHome1"" class=""fontSize26"">Depart: 10.00 AM</span><br/>
                               <span id=""flightArrivalTimeFromHome1"" class=""fontSize26"">Arrive: 12.00 PM</span><br/>
                           </div>
                           <div id=""destinationToHome1"" class=""flightBlockStyle"">
                            <span id=""flightNumberToHome1"" class=""fontSize16"">AI-202</span><br/>
                               <span id=""flightRouteToHome1"" class=""fontSize26"">PNQ > DEL</span><br/>
                               <span id=""flightDepartTimeToHome1"" class=""fontSize26"">Depart: 10.00 AM</span><br/>
                               <span id=""flightArrivalTimeToHome1"" class=""fontSize26"">Arrive: 12.00 PM</span><br/>
                           </div>
                       </div>
                   </div>
                   <div id=""flightDetailsRightPanel1"" class=""flightDetailsRightPanel textAlignRight marginBottom10"">
                    <img src=""images/flightIcon.png"" class=""marginRight10 marginTop10 width40""/><br/>
                       <button class=""marginRight10 marginBottom10 width40 bookNowButtonStyle"">Book Now</button>
                   </div>
                </div>
    </div>

Inside this div for 5 times
<div id=""searchFlightResultDiv"" class=""fullWidth"" style=""border:solid"">

                </div>

Is there a better way to do that rather than string appending in jQuery?
Thanks,
Ankit Tanna
",0,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Code Refactoring,User wants more succinct code,Algorithm Optimization,0,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
36652314,36652314,"Speeding up large amounts of array related computation, visual studio","<p>I'm wondering what the best approach may be for speeding up heavy amounts of array computation. Lets say I have this scenario:</p>

<pre><code>int template_t[] = {1, 2, 3, 4, 5, 6, ...., 125};
int image[3200][5600];
int template_image[3200][5600];

for(int i = 0; i &lt; 3200; i++) {
    for(int j = 0; j &lt; 5600; j++) {

        // iterate over template to find template value per pixel
        for(int h = 0; h &lt; template_length; h++)
            template_image[i][j] += template_t[h] * image[i][j];

    }
}
</code></pre>

<p>Of course my situation is much more complex, but the same idea applies. I have some large array representing the pixels within an image, and I need to apply some template array to each pixel to calculate a value to be placed in the template image.</p>

<p>I've thought about a couple ways to speed this up:</p>

<ul>
<li>SIMD instructions? However I can't seem to find any resources for writing SIMD specific code in visual studio.</li>
<li>Parallelization - though I already parallalize the entire execution itself, so the program runs X instances of itself based on X cores. The program's input is a large amounts of image files, so those X instances would all be processing separate files.</li>
</ul>

<p>What would give me the most bang for my buck? Thanks for any suggestions!</p>
","im wondering what the best approach may be for speeding up heavy amounts of array computation lets say i have this scenario
of course my situation is much more complex but the same idea applies i have some large array representing the pixels within an image and i need to apply some template array to each pixel to calculate a value to be placed in the template image
ive thought about a couple ways to speed this up

simd instructions however i cant seem to find any resources for writing simd specific code in visual studio
parallelization  though i already parallalize the entire execution itself so the program runs x instances of itself based on x cores the programs input is a large amounts of image files so those x instances would all be processing separate files

what would give me the most bang for my buck thanks for any suggestions
",1,"I'm wondering what the best approach may be for speeding up heavy amounts of array computation. Lets say I have this scenario:
int template_t[] = {1, 2, 3, 4, 5, 6, ...., 125};
int image[3200][5600];
int template_image[3200][5600];

for(int i = 0; i < 3200; i++) {
    for(int j = 0; j < 5600; j++) {

        // iterate over template to find template value per pixel
        for(int h = 0; h < template_length; h++)
            template_image[i][j] += template_t[h] * image[i][j];

    }
}

Of course my situation is much more complex, but the same idea applies. I have some large array representing the pixels within an image, and I need to apply some template array to each pixel to calculate a value to be placed in the template image.
I've thought about a couple ways to speed this up:

SIMD instructions? However I can't seem to find any resources for writing SIMD specific code in visual studio.
Parallelization - though I already parallalize the entire execution itself, so the program runs X instances of itself based on X cores. The program's input is a large amounts of image files, so those X instances would all be processing separate files.

What would give me the most bang for my buck? Thanks for any suggestions!
","Speeding up large amounts of array related computation, visual studio
I'm wondering what the best approach may be for speeding up heavy amounts of array computation. Lets say I have this scenario:
int template_t[] = {1, 2, 3, 4, 5, 6, ...., 125};
int image[3200][5600];
int template_image[3200][5600];

for(int i = 0; i < 3200; i++) {
    for(int j = 0; j < 5600; j++) {

        // iterate over template to find template value per pixel
        for(int h = 0; h < template_length; h++)
            template_image[i][j] += template_t[h] * image[i][j];

    }
}

Of course my situation is much more complex, but the same idea applies. I have some large array representing the pixels within an image, and I need to apply some template array to each pixel to calculate a value to be placed in the template image.
I've thought about a couple ways to speed this up:

SIMD instructions? However I can't seem to find any resources for writing SIMD specific code in visual studio.
Parallelization - though I already parallalize the entire execution itself, so the program runs X instances of itself based on X cores. The program's input is a large amounts of image files, so those X instances would all be processing separate files.

What would give me the most bang for my buck? Thanks for any suggestions!
",0,1,0,1,0,0,0,- User just doesn't know how to approach a problem,"Label: 1

Reasoning: This question expresses technical debt because the user is seeking to optimize the performance of their array computation, which is currently slow. The code provided is suboptimal and requires improvement to achieve better performance. The user has identified the performance bottleneck and is looking for ways to speed up the computation.

The user has already attempted to parallelize the program by running multiple instances of it, but this approach has limitations, and they are seeking alternative solutions. The consideration of SIMD instructions and parallelization indicates that the current implementation is not efficient and requires refactoring to improve performance.

The user's situation is complex, and they are seeking advice on the best approach to optimize the computation. This implies that the current code is not scalable and requires optimization to handle large datasets efficiently.

The technical debt in this question is inherent, as the user is aware of the performance issues and is seeking to refactor the code to improve its efficiency. The debt is related to performance and scalability, and addressing it will require improvements to the code's design and implementation.",Yes,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,1,0,1,0,0,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
72253434,72253434,How to write an algorithm which finds the best possible event distribution?,"<p>I'm trying to write an algorithm that takes a list of events (in this case, screenings of movies - each movie usually has more than one screening, although that's not always the case) and returns the best possible combination of events, i.e. the one that sacrifices the least amount of movies.</p>
<p><a href=""https://i.stack.imgur.com/9ogDl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9ogDl.png"" alt=""Movie schedule"" /></a></p>
<p>I came up with this algorithm which I applied manually, but I'm trying to turn it into code and having some trouble. What I did (manually) was go screening by screening, checking whether it had conflicts with any other screening. If it did, it would resolve the conflicts (let's say it's a conflict between the first screenings of Movie A and Movie B) by choosing one of the two conflicting movies* (Movie A), deleting all other screenings for Movie A, and deleting that screening of Movie B. After doing that, it would look for another screening of Movie B, check whether it had any conflicts, and repeat the process.</p>
<p>* Usually the one I was currently checking, but some criteria could affect the decision, e.g. if one of the two conflicting movies had a free (conflict-less) screening somewhere else, I would just choose that one and resolve the conflict that way. Conversely, if one of the two movies had no other screenings, I would choose that one.</p>
<p>Eventually this &quot;branch&quot; would end up arriving at a free (conflict-less) screening, which would mean the entire branch was solved (i.e. all of the movies involved could be watched without sacrificing any others), or it would arrive at a stalemate, which would mean that this combination of decisions did not lead to a perfect solution. In this case, one would go back up a level of recursion (i.e. go back one decision) and try choosing the other screening. If that didn't work, go back up another level and try again. If there was no combination of decisions in that branch that led to a solved branch, a decision had to be made by the human to sacrifice one of the movies that was causing the issue.</p>
<p>That's pretty much what I'm trying to code, and I wrote an attempt at it in a very stream-of-consciousness way just trying to get it out quickly cause I was excited, not thinking far ahead and just fixing issues as they came up. It worked for the movie festival I was trying it on, which was somewhat small (no more than 4 screenings at the same time), but crashed when I tried it on a larger one (6+ screenings at the same time sometimes). It exceeds the maximum stack size, probably because of a problem in the code although I'm not sure if it might just be running correctly and just getting into those levels of recursion. On this note, not sure whether this would be better solved with a while loop instead of recursively.</p>
<p>Yesterday I decided to scrap the algorithm I wrote and start from scratch in a better planned manner, and found the main 2 issues I see looking into the future are:</p>
<ul>
<li>The ability for the algorithm to &quot;go back&quot; and switch its previous decision when a branch failed. This is a core mechanic of the algorithm but I'm struggling to think of a good way to code it. In my previous one I implemented a &quot;checkpoint&quot; system where, if a branch failed, it would revert back to the last checkpoint and continue on from there, and if it succeeded, it would mark that one as the new checkpoint. But that only worked top-level, and not for &quot;internal&quot; decisions.</li>
<li>The fact that most schedules will have more than one solution. If the solution is perfect (i.e. no movies sacrificed), then the issue is very minor in that either way you won't be missing any movies, and at most you'll be deprived of choosing between a few screenings of the same movies. But if there is no perfect solution, now the choice might be between having to decide between 2 movies that you really wanna watch, and between 2 movies that you don't care about that much. This means that the algorithm would need to go over every single possible combination of screenings to find the best possible one(s), and if there's no perfect solution it would show you the different possibilities. But this would be brute forcing the problem which (unless strictly necessary) is not really what I'm looking for.</li>
</ul>
<p>I'd appreciate any help or guidance in the matter. For reference, I'm writing this in Typescript + React + Next.js, but I think this is a more general CS question so again, any help is appreciated.</p>
<p>The current algorithm as well as mock data can be found here: <a href=""https://jsfiddle.net/procz/82n3jxLb/"" rel=""nofollow noreferrer"">https://jsfiddle.net/procz/82n3jxLb/</a></p>
<pre><code>const findBestSchedule = (
  screeningData: IScreening[],
  movies: IMovie[]
): IScreening[] =&gt; {
  const debugLog = (type: string, args) =&gt; {
    if (!DEBUG_LOG) return;
    if (type === &quot;process&quot;) {
      return console.log(
        `processing ${findMovie(
          movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs - called from ${
          args.calledFrom
        }, currentArray:`,
        args.screenings
      );
    }
    if (type === &quot;conflict&quot;) {
      return console.log(
        `conflict between ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} ${args.screening1.date.getDate()} ${args.screening1.date.getHours()}hs and ${findMovie(
          movies,
          args.screening2
        ).name.toUpperCase()} ${args.screening2.date.getDate()} ${args.screening2.date.getHours()}hs`
      );
    }

    if (type === &quot;free&quot;) {
      return console.log(
        `${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs is free, deleting all other ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()}`
      );
    }

    if (type === &quot;recursion&quot;) {
      return console.log(args.recursionLevel, args.array);
    }

    if (type === &quot;unsolvable&quot;) {
      return console.log(
        `unsolvable conflict between ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} and ${findMovie(
          args.movies,
          args.screening2
        ).name.toUpperCase()}, backtracking`
      );
    }

    if (type === &quot;decision&quot;) {
      return console.log(
        `recursion level: ${args.recursionLevel} - choosing ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} ${args.screening1.date.getDate()} ${args.screening1.date.getHours()}hs, deleting ${findMovie(
          args.movies,
          args.screening2
        ).name.toUpperCase()} ${args.screening2.date.getDate()} ${args.screening2.date.getHours()}hs, deleting all other ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()}`
      );
    }
    if (type === &quot;noConflict&quot;) {
      return console.log(
        `no conflicts found for ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs, deleting all other ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()}`
      );
    }
  };
  const sortedScreenings = [...screeningData].sort(
    (a, b) =&gt; b.date.getTime() - a.date.getTime()
  );

  let latestScreeningArray = sortedScreenings;
  let recursionLevel = 0;
  let lastSafeArray = latestScreeningArray;
  let lastSafeRecursionLevel = recursionLevel;

  const processScreening = (
    screenings: IScreening[],
    screening: IScreening,
    calledFrom ? : string
  ): boolean =&gt; {
    debugLog(&quot;process&quot;, {
      screening,
      calledFrom,
      screenings
    });
    const findConflictingScreenings = (
      screenings: IScreening[],
      screening: IScreening
    ): IScreening[] =&gt; {
      if (!screenings) return [];
      return [...screenings].filter(
        (otherScreening) =&gt;
        screening.id !== otherScreening ? .id &amp;&amp;
        screeningsOverlap(screening, otherScreening, movies)
      );
    };

    const conflictingScreenings = findConflictingScreenings(
      screenings,
      screening
    );

    if (conflictingScreenings.length) {
      const resolveConflict = (
        screening: IScreening,
        otherScreening: IScreening
      ): boolean =&gt; {
        debugLog(&quot;conflict&quot;, {
          screening1: screening,
          screening2: otherScreening,
          movies,
        });

        const findNextScreenings = (
          screenings: IScreening[],
          screening: IScreening
        ): IScreening[] =&gt; {
          return [...screenings]
            .reverse()
            .filter(
              (currentScreening) =&gt;
              currentScreening.id !== screening.id &amp;&amp;
              findMovie(movies, currentScreening).id ===
              findMovie(movies, screening).id
            );
        };

        const findFreeScreening = (
          screenings: IScreening[],
          screening: IScreening
        ): IScreening =&gt; {
          return findNextScreenings(screenings, screening).find(
            (nextScreening) =&gt;
            !findConflictingScreenings(screenings, nextScreening).length
          );
        };

        const freeOtherScreening = findFreeScreening(
          latestScreeningArray,
          otherScreening
        );
        const freeScreening = findFreeScreening(
          latestScreeningArray,
          screening
        );

        if (freeOtherScreening || freeScreening) {
          /* FREE SCREENING */
          debugLog(&quot;free&quot;, {
            screening: freeOtherScreening || freeScreening,
            movies,
          });
          latestScreeningArray = deleteAllOtherScreenings(
            latestScreeningArray,
            freeOtherScreening || freeScreening,
            movies
          );
          return true;
        } else {
          /* NO FREE SCREENINGS */

          const decideAndMoveToNextStep = (
            screenings: IScreening[],
            screening1: IScreening,
            screening2: IScreening
          ): boolean =&gt; {
            latestScreeningArray = removeScreening(
              deleteAllOtherScreenings(screenings, screening1, movies),
              screening2
            );
            debugLog(&quot;recursion&quot;, {
              recursionLevel,
              array: latestScreeningArray,
            });
            recursionLevel++;
            const nextStepOfBranchIsSolvable = processScreening(
              latestScreeningArray,
              findNextScreenings(screenings, screening2)[0],
              `decideAndMoveToNextStep`
            );
            if (!nextStepOfBranchIsSolvable) {
              debugLog(&quot;unsolvable&quot;, {
                movies,
                screening1,
                screening2
              });
              latestScreeningArray = screenings;
            }
            return nextStepOfBranchIsSolvable;
          };

          const nextScreeningOfOther = findNextScreenings(
            latestScreeningArray,
            otherScreening
          )[0];

          if (nextScreeningOfOther) {
            debugLog(&quot;decision&quot;, {
              recursionLevel,
              movies,
              screening1: screening,
              screening2: otherScreening,
            });
            return decideAndMoveToNextStep(
              latestScreeningArray,
              screening,
              otherScreening
            );
          } else {
            const nextScreening = findNextScreenings(
              latestScreeningArray,
              screening
            )[0];
            if (nextScreening) {
              debugLog(&quot;decision&quot;, {
                recursionLevel,
                movies,
                screening1: otherScreening,
                screening2: screening,
              });
              return decideAndMoveToNextStep(
                latestScreeningArray,
                otherScreening,
                screening
              );
            } else {
              debugLog(&quot;unsolvable&quot;, {
                movies,
                screening1: screening,
                screening2: otherScreening,
              });
              return false;
            }
          }
        }
      };

      for (let i = conflictingScreenings.length - 1; i &gt;= 0; i--) {
        const branchResolved = resolveConflict(
          screening,
          [...conflictingScreenings].reverse()[i]
        );
        if (!branchResolved) {
          latestScreeningArray = lastSafeArray;
          break;
        }
      }
      debugLog(&quot;noConflicts&quot;, {
        movies,
        screening
      });
      debugLog(&quot;recursion&quot;, {
        recursionLevel,
        array: latestScreeningArray
      });
      if (recursionLevel &gt; 0) {
        recursionLevel--;
      }
      return true;
    } else {
      const updatedScreenings = deleteAllOtherScreenings(
        screenings,
        screening,
        movies
      );
      latestScreeningArray = updatedScreenings;
      if (recursionLevel &gt; 0) {
        recursionLevel--;
      }
      return false;
    }
  };

  for (let i = latestScreeningArray.length - 1; i &gt;= 0; i--) {
    if (!!latestScreeningArray[i]) {
      lastSafeArray = latestScreeningArray;
      lastSafeRecursionLevel = recursionLevel;
      processScreening(
        latestScreeningArray,
        latestScreeningArray[i],
        &quot;overarching&quot;
      );
    }
  }
  return latestScreeningArray;
};
</code></pre>
","im trying to write an algorithm that takes a list of events in this case screenings of movies  each movie usually has more than one screening although thats not always the case and returns the best possible combination of events ie the one that sacrifices the least amount of movies

i came up with this algorithm which i applied manually but im trying to turn it into code and having some trouble what i did manually was go screening by screening checking whether it had conflicts with any other screening if it did it would resolve the conflicts lets say its a conflict between the first screenings of movie a and movie b by choosing one of the two conflicting movies movie a deleting all other screenings for movie a and deleting that screening of movie b after doing that it would look for another screening of movie b check whether it had any conflicts and repeat the process
 usually the one i was currently checking but some criteria could affect the decision eg if one of the two conflicting movies had a free conflictless screening somewhere else i would just choose that one and resolve the conflict that way conversely if one of the two movies had no other screenings i would choose that one
eventually this branch would end up arriving at a free conflictless screening which would mean the entire branch was solved ie all of the movies involved could be watched without sacrificing any others or it would arrive at a stalemate which would mean that this combination of decisions did not lead to a perfect solution in this case one would go back up a level of recursion ie go back one decision and try choosing the other screening if that didnt work go back up another level and try again if there was no combination of decisions in that branch that led to a solved branch a decision had to be made by the human to sacrifice one of the movies that was causing the issue
thats pretty much what im trying to code and i wrote an attempt at it in a very streamofconsciousness way just trying to get it out quickly cause i was excited not thinking far ahead and just fixing issues as they came up it worked for the movie festival i was trying it on which was somewhat small no more than 4 screenings at the same time but crashed when i tried it on a larger one 6 screenings at the same time sometimes it exceeds the maximum stack size probably because of a problem in the code although im not sure if it might just be running correctly and just getting into those levels of recursion on this note not sure whether this would be better solved with a while loop instead of recursively
yesterday i decided to scrap the algorithm i wrote and start from scratch in a better planned manner and found the main 2 issues i see looking into the future are

the ability for the algorithm to go back and switch its previous decision when a branch failed this is a core mechanic of the algorithm but im struggling to think of a good way to code it in my previous one i implemented a checkpoint system where if a branch failed it would revert back to the last checkpoint and continue on from there and if it succeeded it would mark that one as the new checkpoint but that only worked toplevel and not for internal decisions
the fact that most schedules will have more than one solution if the solution is perfect ie no movies sacrificed then the issue is very minor in that either way you wont be missing any movies and at most youll be deprived of choosing between a few screenings of the same movies but if there is no perfect solution now the choice might be between having to decide between 2 movies that you really wanna watch and between 2 movies that you dont care about that much this means that the algorithm would need to go over every single possible combination of screenings to find the best possible ones and if theres no perfect solution it would show you the different possibilities but this would be brute forcing the problem which unless strictly necessary is not really what im looking for

id appreciate any help or guidance in the matter for reference im writing this in typescript  react  nextjs but i think this is a more general cs question so again any help is appreciated
the current algorithm as well as mock data can be found here httpsjsfiddlenetprocz82n3jxlb
",0,"I'm trying to write an algorithm that takes a list of events (in this case, screenings of movies - each movie usually has more than one screening, although that's not always the case) and returns the best possible combination of events, i.e. the one that sacrifices the least amount of movies.

I came up with this algorithm which I applied manually, but I'm trying to turn it into code and having some trouble. What I did (manually) was go screening by screening, checking whether it had conflicts with any other screening. If it did, it would resolve the conflicts (let's say it's a conflict between the first screenings of Movie A and Movie B) by choosing one of the two conflicting movies* (Movie A), deleting all other screenings for Movie A, and deleting that screening of Movie B. After doing that, it would look for another screening of Movie B, check whether it had any conflicts, and repeat the process.
* Usually the one I was currently checking, but some criteria could affect the decision, e.g. if one of the two conflicting movies had a free (conflict-less) screening somewhere else, I would just choose that one and resolve the conflict that way. Conversely, if one of the two movies had no other screenings, I would choose that one.
Eventually this ""branch"" would end up arriving at a free (conflict-less) screening, which would mean the entire branch was solved (i.e. all of the movies involved could be watched without sacrificing any others), or it would arrive at a stalemate, which would mean that this combination of decisions did not lead to a perfect solution. In this case, one would go back up a level of recursion (i.e. go back one decision) and try choosing the other screening. If that didn't work, go back up another level and try again. If there was no combination of decisions in that branch that led to a solved branch, a decision had to be made by the human to sacrifice one of the movies that was causing the issue.
That's pretty much what I'm trying to code, and I wrote an attempt at it in a very stream-of-consciousness way just trying to get it out quickly cause I was excited, not thinking far ahead and just fixing issues as they came up. It worked for the movie festival I was trying it on, which was somewhat small (no more than 4 screenings at the same time), but crashed when I tried it on a larger one (6+ screenings at the same time sometimes). It exceeds the maximum stack size, probably because of a problem in the code although I'm not sure if it might just be running correctly and just getting into those levels of recursion. On this note, not sure whether this would be better solved with a while loop instead of recursively.
Yesterday I decided to scrap the algorithm I wrote and start from scratch in a better planned manner, and found the main 2 issues I see looking into the future are:

The ability for the algorithm to ""go back"" and switch its previous decision when a branch failed. This is a core mechanic of the algorithm but I'm struggling to think of a good way to code it. In my previous one I implemented a ""checkpoint"" system where, if a branch failed, it would revert back to the last checkpoint and continue on from there, and if it succeeded, it would mark that one as the new checkpoint. But that only worked top-level, and not for ""internal"" decisions.
The fact that most schedules will have more than one solution. If the solution is perfect (i.e. no movies sacrificed), then the issue is very minor in that either way you won't be missing any movies, and at most you'll be deprived of choosing between a few screenings of the same movies. But if there is no perfect solution, now the choice might be between having to decide between 2 movies that you really wanna watch, and between 2 movies that you don't care about that much. This means that the algorithm would need to go over every single possible combination of screenings to find the best possible one(s), and if there's no perfect solution it would show you the different possibilities. But this would be brute forcing the problem which (unless strictly necessary) is not really what I'm looking for.

I'd appreciate any help or guidance in the matter. For reference, I'm writing this in Typescript + React + Next.js, but I think this is a more general CS question so again, any help is appreciated.
The current algorithm as well as mock data can be found here: https://jsfiddle.net/procz/82n3jxLb/
const findBestSchedule = (
  screeningData: IScreening[],
  movies: IMovie[]
): IScreening[] => {
  const debugLog = (type: string, args) => {
    if (!DEBUG_LOG) return;
    if (type === ""process"") {
      return console.log(
        `processing ${findMovie(
          movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs - called from ${
          args.calledFrom
        }, currentArray:`,
        args.screenings
      );
    }
    if (type === ""conflict"") {
      return console.log(
        `conflict between ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} ${args.screening1.date.getDate()} ${args.screening1.date.getHours()}hs and ${findMovie(
          movies,
          args.screening2
        ).name.toUpperCase()} ${args.screening2.date.getDate()} ${args.screening2.date.getHours()}hs`
      );
    }

    if (type === ""free"") {
      return console.log(
        `${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs is free, deleting all other ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()}`
      );
    }

    if (type === ""recursion"") {
      return console.log(args.recursionLevel, args.array);
    }

    if (type === ""unsolvable"") {
      return console.log(
        `unsolvable conflict between ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} and ${findMovie(
          args.movies,
          args.screening2
        ).name.toUpperCase()}, backtracking`
      );
    }

    if (type === ""decision"") {
      return console.log(
        `recursion level: ${args.recursionLevel} - choosing ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} ${args.screening1.date.getDate()} ${args.screening1.date.getHours()}hs, deleting ${findMovie(
          args.movies,
          args.screening2
        ).name.toUpperCase()} ${args.screening2.date.getDate()} ${args.screening2.date.getHours()}hs, deleting all other ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()}`
      );
    }
    if (type === ""noConflict"") {
      return console.log(
        `no conflicts found for ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs, deleting all other ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()}`
      );
    }
  };
  const sortedScreenings = [...screeningData].sort(
    (a, b) => b.date.getTime() - a.date.getTime()
  );

  let latestScreeningArray = sortedScreenings;
  let recursionLevel = 0;
  let lastSafeArray = latestScreeningArray;
  let lastSafeRecursionLevel = recursionLevel;

  const processScreening = (
    screenings: IScreening[],
    screening: IScreening,
    calledFrom ? : string
  ): boolean => {
    debugLog(""process"", {
      screening,
      calledFrom,
      screenings
    });
    const findConflictingScreenings = (
      screenings: IScreening[],
      screening: IScreening
    ): IScreening[] => {
      if (!screenings) return [];
      return [...screenings].filter(
        (otherScreening) =>
        screening.id !== otherScreening ? .id &&
        screeningsOverlap(screening, otherScreening, movies)
      );
    };

    const conflictingScreenings = findConflictingScreenings(
      screenings,
      screening
    );

    if (conflictingScreenings.length) {
      const resolveConflict = (
        screening: IScreening,
        otherScreening: IScreening
      ): boolean => {
        debugLog(""conflict"", {
          screening1: screening,
          screening2: otherScreening,
          movies,
        });

        const findNextScreenings = (
          screenings: IScreening[],
          screening: IScreening
        ): IScreening[] => {
          return [...screenings]
            .reverse()
            .filter(
              (currentScreening) =>
              currentScreening.id !== screening.id &&
              findMovie(movies, currentScreening).id ===
              findMovie(movies, screening).id
            );
        };

        const findFreeScreening = (
          screenings: IScreening[],
          screening: IScreening
        ): IScreening => {
          return findNextScreenings(screenings, screening).find(
            (nextScreening) =>
            !findConflictingScreenings(screenings, nextScreening).length
          );
        };

        const freeOtherScreening = findFreeScreening(
          latestScreeningArray,
          otherScreening
        );
        const freeScreening = findFreeScreening(
          latestScreeningArray,
          screening
        );

        if (freeOtherScreening || freeScreening) {
          /* FREE SCREENING */
          debugLog(""free"", {
            screening: freeOtherScreening || freeScreening,
            movies,
          });
          latestScreeningArray = deleteAllOtherScreenings(
            latestScreeningArray,
            freeOtherScreening || freeScreening,
            movies
          );
          return true;
        } else {
          /* NO FREE SCREENINGS */

          const decideAndMoveToNextStep = (
            screenings: IScreening[],
            screening1: IScreening,
            screening2: IScreening
          ): boolean => {
            latestScreeningArray = removeScreening(
              deleteAllOtherScreenings(screenings, screening1, movies),
              screening2
            );
            debugLog(""recursion"", {
              recursionLevel,
              array: latestScreeningArray,
            });
            recursionLevel++;
            const nextStepOfBranchIsSolvable = processScreening(
              latestScreeningArray,
              findNextScreenings(screenings, screening2)[0],
              `decideAndMoveToNextStep`
            );
            if (!nextStepOfBranchIsSolvable) {
              debugLog(""unsolvable"", {
                movies,
                screening1,
                screening2
              });
              latestScreeningArray = screenings;
            }
            return nextStepOfBranchIsSolvable;
          };

          const nextScreeningOfOther = findNextScreenings(
            latestScreeningArray,
            otherScreening
          )[0];

          if (nextScreeningOfOther) {
            debugLog(""decision"", {
              recursionLevel,
              movies,
              screening1: screening,
              screening2: otherScreening,
            });
            return decideAndMoveToNextStep(
              latestScreeningArray,
              screening,
              otherScreening
            );
          } else {
            const nextScreening = findNextScreenings(
              latestScreeningArray,
              screening
            )[0];
            if (nextScreening) {
              debugLog(""decision"", {
                recursionLevel,
                movies,
                screening1: otherScreening,
                screening2: screening,
              });
              return decideAndMoveToNextStep(
                latestScreeningArray,
                otherScreening,
                screening
              );
            } else {
              debugLog(""unsolvable"", {
                movies,
                screening1: screening,
                screening2: otherScreening,
              });
              return false;
            }
          }
        }
      };

      for (let i = conflictingScreenings.length - 1; i >= 0; i--) {
        const branchResolved = resolveConflict(
          screening,
          [...conflictingScreenings].reverse()[i]
        );
        if (!branchResolved) {
          latestScreeningArray = lastSafeArray;
          break;
        }
      }
      debugLog(""noConflicts"", {
        movies,
        screening
      });
      debugLog(""recursion"", {
        recursionLevel,
        array: latestScreeningArray
      });
      if (recursionLevel > 0) {
        recursionLevel--;
      }
      return true;
    } else {
      const updatedScreenings = deleteAllOtherScreenings(
        screenings,
        screening,
        movies
      );
      latestScreeningArray = updatedScreenings;
      if (recursionLevel > 0) {
        recursionLevel--;
      }
      return false;
    }
  };

  for (let i = latestScreeningArray.length - 1; i >= 0; i--) {
    if (!!latestScreeningArray[i]) {
      lastSafeArray = latestScreeningArray;
      lastSafeRecursionLevel = recursionLevel;
      processScreening(
        latestScreeningArray,
        latestScreeningArray[i],
        ""overarching""
      );
    }
  }
  return latestScreeningArray;
};

","How to write an algorithm which finds the best possible event distribution?
I'm trying to write an algorithm that takes a list of events (in this case, screenings of movies - each movie usually has more than one screening, although that's not always the case) and returns the best possible combination of events, i.e. the one that sacrifices the least amount of movies.

I came up with this algorithm which I applied manually, but I'm trying to turn it into code and having some trouble. What I did (manually) was go screening by screening, checking whether it had conflicts with any other screening. If it did, it would resolve the conflicts (let's say it's a conflict between the first screenings of Movie A and Movie B) by choosing one of the two conflicting movies* (Movie A), deleting all other screenings for Movie A, and deleting that screening of Movie B. After doing that, it would look for another screening of Movie B, check whether it had any conflicts, and repeat the process.
* Usually the one I was currently checking, but some criteria could affect the decision, e.g. if one of the two conflicting movies had a free (conflict-less) screening somewhere else, I would just choose that one and resolve the conflict that way. Conversely, if one of the two movies had no other screenings, I would choose that one.
Eventually this ""branch"" would end up arriving at a free (conflict-less) screening, which would mean the entire branch was solved (i.e. all of the movies involved could be watched without sacrificing any others), or it would arrive at a stalemate, which would mean that this combination of decisions did not lead to a perfect solution. In this case, one would go back up a level of recursion (i.e. go back one decision) and try choosing the other screening. If that didn't work, go back up another level and try again. If there was no combination of decisions in that branch that led to a solved branch, a decision had to be made by the human to sacrifice one of the movies that was causing the issue.
That's pretty much what I'm trying to code, and I wrote an attempt at it in a very stream-of-consciousness way just trying to get it out quickly cause I was excited, not thinking far ahead and just fixing issues as they came up. It worked for the movie festival I was trying it on, which was somewhat small (no more than 4 screenings at the same time), but crashed when I tried it on a larger one (6+ screenings at the same time sometimes). It exceeds the maximum stack size, probably because of a problem in the code although I'm not sure if it might just be running correctly and just getting into those levels of recursion. On this note, not sure whether this would be better solved with a while loop instead of recursively.
Yesterday I decided to scrap the algorithm I wrote and start from scratch in a better planned manner, and found the main 2 issues I see looking into the future are:

The ability for the algorithm to ""go back"" and switch its previous decision when a branch failed. This is a core mechanic of the algorithm but I'm struggling to think of a good way to code it. In my previous one I implemented a ""checkpoint"" system where, if a branch failed, it would revert back to the last checkpoint and continue on from there, and if it succeeded, it would mark that one as the new checkpoint. But that only worked top-level, and not for ""internal"" decisions.
The fact that most schedules will have more than one solution. If the solution is perfect (i.e. no movies sacrificed), then the issue is very minor in that either way you won't be missing any movies, and at most you'll be deprived of choosing between a few screenings of the same movies. But if there is no perfect solution, now the choice might be between having to decide between 2 movies that you really wanna watch, and between 2 movies that you don't care about that much. This means that the algorithm would need to go over every single possible combination of screenings to find the best possible one(s), and if there's no perfect solution it would show you the different possibilities. But this would be brute forcing the problem which (unless strictly necessary) is not really what I'm looking for.

I'd appreciate any help or guidance in the matter. For reference, I'm writing this in Typescript + React + Next.js, but I think this is a more general CS question so again, any help is appreciated.
The current algorithm as well as mock data can be found here: https://jsfiddle.net/procz/82n3jxLb/
const findBestSchedule = (
  screeningData: IScreening[],
  movies: IMovie[]
): IScreening[] => {
  const debugLog = (type: string, args) => {
    if (!DEBUG_LOG) return;
    if (type === ""process"") {
      return console.log(
        `processing ${findMovie(
          movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs - called from ${
          args.calledFrom
        }, currentArray:`,
        args.screenings
      );
    }
    if (type === ""conflict"") {
      return console.log(
        `conflict between ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} ${args.screening1.date.getDate()} ${args.screening1.date.getHours()}hs and ${findMovie(
          movies,
          args.screening2
        ).name.toUpperCase()} ${args.screening2.date.getDate()} ${args.screening2.date.getHours()}hs`
      );
    }

    if (type === ""free"") {
      return console.log(
        `${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs is free, deleting all other ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()}`
      );
    }

    if (type === ""recursion"") {
      return console.log(args.recursionLevel, args.array);
    }

    if (type === ""unsolvable"") {
      return console.log(
        `unsolvable conflict between ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} and ${findMovie(
          args.movies,
          args.screening2
        ).name.toUpperCase()}, backtracking`
      );
    }

    if (type === ""decision"") {
      return console.log(
        `recursion level: ${args.recursionLevel} - choosing ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()} ${args.screening1.date.getDate()} ${args.screening1.date.getHours()}hs, deleting ${findMovie(
          args.movies,
          args.screening2
        ).name.toUpperCase()} ${args.screening2.date.getDate()} ${args.screening2.date.getHours()}hs, deleting all other ${findMovie(
          args.movies,
          args.screening1
        ).name.toUpperCase()}`
      );
    }
    if (type === ""noConflict"") {
      return console.log(
        `no conflicts found for ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()} ${args.screening.date.getDate()} ${args.screening.date.getHours()}hs, deleting all other ${findMovie(
          args.movies,
          args.screening
        ).name.toUpperCase()}`
      );
    }
  };
  const sortedScreenings = [...screeningData].sort(
    (a, b) => b.date.getTime() - a.date.getTime()
  );

  let latestScreeningArray = sortedScreenings;
  let recursionLevel = 0;
  let lastSafeArray = latestScreeningArray;
  let lastSafeRecursionLevel = recursionLevel;

  const processScreening = (
    screenings: IScreening[],
    screening: IScreening,
    calledFrom ? : string
  ): boolean => {
    debugLog(""process"", {
      screening,
      calledFrom,
      screenings
    });
    const findConflictingScreenings = (
      screenings: IScreening[],
      screening: IScreening
    ): IScreening[] => {
      if (!screenings) return [];
      return [...screenings].filter(
        (otherScreening) =>
        screening.id !== otherScreening ? .id &&
        screeningsOverlap(screening, otherScreening, movies)
      );
    };

    const conflictingScreenings = findConflictingScreenings(
      screenings,
      screening
    );

    if (conflictingScreenings.length) {
      const resolveConflict = (
        screening: IScreening,
        otherScreening: IScreening
      ): boolean => {
        debugLog(""conflict"", {
          screening1: screening,
          screening2: otherScreening,
          movies,
        });

        const findNextScreenings = (
          screenings: IScreening[],
          screening: IScreening
        ): IScreening[] => {
          return [...screenings]
            .reverse()
            .filter(
              (currentScreening) =>
              currentScreening.id !== screening.id &&
              findMovie(movies, currentScreening).id ===
              findMovie(movies, screening).id
            );
        };

        const findFreeScreening = (
          screenings: IScreening[],
          screening: IScreening
        ): IScreening => {
          return findNextScreenings(screenings, screening).find(
            (nextScreening) =>
            !findConflictingScreenings(screenings, nextScreening).length
          );
        };

        const freeOtherScreening = findFreeScreening(
          latestScreeningArray,
          otherScreening
        );
        const freeScreening = findFreeScreening(
          latestScreeningArray,
          screening
        );

        if (freeOtherScreening || freeScreening) {
          /* FREE SCREENING */
          debugLog(""free"", {
            screening: freeOtherScreening || freeScreening,
            movies,
          });
          latestScreeningArray = deleteAllOtherScreenings(
            latestScreeningArray,
            freeOtherScreening || freeScreening,
            movies
          );
          return true;
        } else {
          /* NO FREE SCREENINGS */

          const decideAndMoveToNextStep = (
            screenings: IScreening[],
            screening1: IScreening,
            screening2: IScreening
          ): boolean => {
            latestScreeningArray = removeScreening(
              deleteAllOtherScreenings(screenings, screening1, movies),
              screening2
            );
            debugLog(""recursion"", {
              recursionLevel,
              array: latestScreeningArray,
            });
            recursionLevel++;
            const nextStepOfBranchIsSolvable = processScreening(
              latestScreeningArray,
              findNextScreenings(screenings, screening2)[0],
              `decideAndMoveToNextStep`
            );
            if (!nextStepOfBranchIsSolvable) {
              debugLog(""unsolvable"", {
                movies,
                screening1,
                screening2
              });
              latestScreeningArray = screenings;
            }
            return nextStepOfBranchIsSolvable;
          };

          const nextScreeningOfOther = findNextScreenings(
            latestScreeningArray,
            otherScreening
          )[0];

          if (nextScreeningOfOther) {
            debugLog(""decision"", {
              recursionLevel,
              movies,
              screening1: screening,
              screening2: otherScreening,
            });
            return decideAndMoveToNextStep(
              latestScreeningArray,
              screening,
              otherScreening
            );
          } else {
            const nextScreening = findNextScreenings(
              latestScreeningArray,
              screening
            )[0];
            if (nextScreening) {
              debugLog(""decision"", {
                recursionLevel,
                movies,
                screening1: otherScreening,
                screening2: screening,
              });
              return decideAndMoveToNextStep(
                latestScreeningArray,
                otherScreening,
                screening
              );
            } else {
              debugLog(""unsolvable"", {
                movies,
                screening1: screening,
                screening2: otherScreening,
              });
              return false;
            }
          }
        }
      };

      for (let i = conflictingScreenings.length - 1; i >= 0; i--) {
        const branchResolved = resolveConflict(
          screening,
          [...conflictingScreenings].reverse()[i]
        );
        if (!branchResolved) {
          latestScreeningArray = lastSafeArray;
          break;
        }
      }
      debugLog(""noConflicts"", {
        movies,
        screening
      });
      debugLog(""recursion"", {
        recursionLevel,
        array: latestScreeningArray
      });
      if (recursionLevel > 0) {
        recursionLevel--;
      }
      return true;
    } else {
      const updatedScreenings = deleteAllOtherScreenings(
        screenings,
        screening,
        movies
      );
      latestScreeningArray = updatedScreenings;
      if (recursionLevel > 0) {
        recursionLevel--;
      }
      return false;
    }
  };

  for (let i = latestScreeningArray.length - 1; i >= 0; i--) {
    if (!!latestScreeningArray[i]) {
      lastSafeArray = latestScreeningArray;
      lastSafeRecursionLevel = recursionLevel;
      processScreening(
        latestScreeningArray,
        latestScreeningArray[i],
        ""overarching""
      );
    }
  }
  return latestScreeningArray;
};

",0,0,1,0,0,0,1,- Suboptimal code ,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,0,0,1,0,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
19551134,19551134,Are there any graph algorithm implementations in Python that are not coupled with a particular implementation?,"<p>I need some graph algorithms for a project. I could implement them from scratch, but before I do so I wanted to see if there was something out there that I could use. I've looked at <a href=""http://networkx.github.io/"" rel=""nofollow"">networkx</a> and <a href=""http://igraph.sourceforge.net/"" rel=""nofollow"">igraph</a>. Networkx turns out to be memory inefficient for the size of the graphs I have to deal with. I think I'm using igraph incorrectly because it seems to just hang. I'll get more information on igraph if anyone's interested.</p>

<p>Networkx seemed promising because it is implemented in pure Python. The problem is that building the graph eats up all the memory. I have the graph in an SQLite database as an adjacency list.
So what I wanted to do was modify the code so that it can work directly with the SQLite representation instead of the networkx representation. Has anybody ever tried this successfully?</p>

<p>If that does not work, it goes back to the question in the title. Are there any graph implementations in Python that are reasonably decoupled from the graph representation?</p>
","i need some graph algorithms for a project i could implement them from scratch but before i do so i wanted to see if there was something out there that i could use ive looked at networkx and igraph networkx turns out to be memory inefficient for the size of the graphs i have to deal with i think im using igraph incorrectly because it seems to just hang ill get more information on igraph if anyones interested
networkx seemed promising because it is implemented in pure python the problem is that building the graph eats up all the memory i have the graph in an sqlite database as an adjacency list
so what i wanted to do was modify the code so that it can work directly with the sqlite representation instead of the networkx representation has anybody ever tried this successfully
if that does not work it goes back to the question in the title are there any graph implementations in python that are reasonably decoupled from the graph representation
",1,"I need some graph algorithms for a project. I could implement them from scratch, but before I do so I wanted to see if there was something out there that I could use. I've looked at networkx and igraph. Networkx turns out to be memory inefficient for the size of the graphs I have to deal with. I think I'm using igraph incorrectly because it seems to just hang. I'll get more information on igraph if anyone's interested.
Networkx seemed promising because it is implemented in pure Python. The problem is that building the graph eats up all the memory. I have the graph in an SQLite database as an adjacency list.
So what I wanted to do was modify the code so that it can work directly with the SQLite representation instead of the networkx representation. Has anybody ever tried this successfully?
If that does not work, it goes back to the question in the title. Are there any graph implementations in Python that are reasonably decoupled from the graph representation?
","Are there any graph algorithm implementations in Python that are not coupled with a particular implementation?
I need some graph algorithms for a project. I could implement them from scratch, but before I do so I wanted to see if there was something out there that I could use. I've looked at networkx and igraph. Networkx turns out to be memory inefficient for the size of the graphs I have to deal with. I think I'm using igraph incorrectly because it seems to just hang. I'll get more information on igraph if anyone's interested.
Networkx seemed promising because it is implemented in pure Python. The problem is that building the graph eats up all the memory. I have the graph in an SQLite database as an adjacency list.
So what I wanted to do was modify the code so that it can work directly with the SQLite representation instead of the networkx representation. Has anybody ever tried this successfully?
If that does not work, it goes back to the question in the title. Are there any graph implementations in Python that are reasonably decoupled from the graph representation?
",0,1,0,0,0,1,1,,,,1,1,Memory Management,Memory Management,,Memory Management,0,1,0,0,0,1,,,Architecture,Architecture,,Architecture,Severe,Severe,,Severe
2278494,2278494,Succinctly storing atomic model data for OpenGL program,"<p>I want to be able to store an atomic model in an OpenGL program I'm writing. Nothing fancy; just constant mesh vertex values stored as <code>GLfloat[3]</code>, plus simple textures. I also want the model to be able to move and rotate freely and as a single object. Here's what I have so far:</p>

<pre><code>typedef struct _coordnode {
 GLfloat    *pts;           /* XYZ (vertex) or XY (texture) */
 struct _coordnode *next;
} coordnode;

typedef struct _facenode {
 GLfloat    *norm;          /* XYZ */
 coordnode  *vertices;      /* head of linked list */
 GLfloat    *color;         /* RGBA */
 coordnode  *textures;      /* head of linked list */
 struct _facenode *next;
} facenode;

typedef struct _model {
 GLenum     mode;
 facenode   *faces;         /* head of linked list */
 GLfloat    *rot;           /* delta-XYZ from Theta-origin */
 GLfloat    *rot_delta;     /* delta-delta-XYZ */
 GLfloat    *trans;         /* delta-XYZ from origin */
 GLfloat    *trans_delta;   /* delta-delta-XYZ from origin */
} model;
</code></pre>

<p>This sets itself up in such a way that the model has a linked lists of <code>facenode</code>, each of which has two linked lists of its vertexes and its texture coordinates, respectively.</p>

<p>Since this is my first C program, my question to seasoned programmers is whether this particular method presents any inconsistencies or inefficiencies, or whether it stores enough data.</p>

<p>More information, not necessarily relevant:</p>

<ul>
<li>There will only be a few objects in memory, and a two of them will be involved in collision detection.</li>
<li>One model will have a partial transparency.</li>
<li>One model will have raised rendered text applied to the model's faces, and move according to a gravity vector.</li>
<li>Two models will rotate as one, based on external input.</li>
</ul>
","i want to be able to store an atomic model in an opengl program im writing nothing fancy just constant mesh vertex values stored as glfloat3 plus simple textures i also want the model to be able to move and rotate freely and as a single object heres what i have so far
this sets itself up in such a way that the model has a linked lists of facenode each of which has two linked lists of its vertexes and its texture coordinates respectively
since this is my first c program my question to seasoned programmers is whether this particular method presents any inconsistencies or inefficiencies or whether it stores enough data
more information not necessarily relevant

there will only be a few objects in memory and a two of them will be involved in collision detection
one model will have a partial transparency
one model will have raised rendered text applied to the models faces and move according to a gravity vector
two models will rotate as one based on external input

",1,"I want to be able to store an atomic model in an OpenGL program I'm writing. Nothing fancy; just constant mesh vertex values stored as GLfloat[3], plus simple textures. I also want the model to be able to move and rotate freely and as a single object. Here's what I have so far:
typedef struct _coordnode {
 GLfloat    *pts;           /* XYZ (vertex) or XY (texture) */
 struct _coordnode *next;
} coordnode;

typedef struct _facenode {
 GLfloat    *norm;          /* XYZ */
 coordnode  *vertices;      /* head of linked list */
 GLfloat    *color;         /* RGBA */
 coordnode  *textures;      /* head of linked list */
 struct _facenode *next;
} facenode;

typedef struct _model {
 GLenum     mode;
 facenode   *faces;         /* head of linked list */
 GLfloat    *rot;           /* delta-XYZ from Theta-origin */
 GLfloat    *rot_delta;     /* delta-delta-XYZ */
 GLfloat    *trans;         /* delta-XYZ from origin */
 GLfloat    *trans_delta;   /* delta-delta-XYZ from origin */
} model;

This sets itself up in such a way that the model has a linked lists of facenode, each of which has two linked lists of its vertexes and its texture coordinates, respectively.
Since this is my first C program, my question to seasoned programmers is whether this particular method presents any inconsistencies or inefficiencies, or whether it stores enough data.
More information, not necessarily relevant:

There will only be a few objects in memory, and a two of them will be involved in collision detection.
One model will have a partial transparency.
One model will have raised rendered text applied to the model's faces, and move according to a gravity vector.
Two models will rotate as one, based on external input.

","Succinctly storing atomic model data for OpenGL program
I want to be able to store an atomic model in an OpenGL program I'm writing. Nothing fancy; just constant mesh vertex values stored as GLfloat[3], plus simple textures. I also want the model to be able to move and rotate freely and as a single object. Here's what I have so far:
typedef struct _coordnode {
 GLfloat    *pts;           /* XYZ (vertex) or XY (texture) */
 struct _coordnode *next;
} coordnode;

typedef struct _facenode {
 GLfloat    *norm;          /* XYZ */
 coordnode  *vertices;      /* head of linked list */
 GLfloat    *color;         /* RGBA */
 coordnode  *textures;      /* head of linked list */
 struct _facenode *next;
} facenode;

typedef struct _model {
 GLenum     mode;
 facenode   *faces;         /* head of linked list */
 GLfloat    *rot;           /* delta-XYZ from Theta-origin */
 GLfloat    *rot_delta;     /* delta-delta-XYZ */
 GLfloat    *trans;         /* delta-XYZ from origin */
 GLfloat    *trans_delta;   /* delta-delta-XYZ from origin */
} model;

This sets itself up in such a way that the model has a linked lists of facenode, each of which has two linked lists of its vertexes and its texture coordinates, respectively.
Since this is my first C program, my question to seasoned programmers is whether this particular method presents any inconsistencies or inefficiencies, or whether it stores enough data.
More information, not necessarily relevant:

There will only be a few objects in memory, and a two of them will be involved in collision detection.
One model will have a partial transparency.
One model will have raised rendered text applied to the model's faces, and move according to a gravity vector.
Two models will rotate as one, based on external input.

",0,1,1,1,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,1,1,0,0,,,Design,Design,,Design,Very Mild,Very Mild,,Very Mild
59596851,59596851,Why is casting with dynamic faster than with object,"<p>I am really confused, take the following code:</p>

<pre class=""lang-cs prettyprint-override""><code>[Benchmark]
public TEnum DynamicCast()
{
    return (TEnum)(dynamic)0;
}

[Benchmark]
public TEnum ObjectCast()
{
    return (TEnum)(object)0;
}

[Benchmark]
public TEnum DirectCast()
{
    return (TEnum)0;
}

public enum TEnum
{
    Foo,
    Bar
}
</code></pre>

<p>And yes I know that I could directly cast the integer to the given Enum, but this is a simplified version of my actual code which includes the work with generic extensions.</p>

<p>Anyhow I ran a few performance tests, because I was curious which one is faster. I am using <code>BenchmarkDotNet</code> and see there:</p>

<pre><code>BenchmarkDotNet=v0.12.0, OS=Windows 10.0.18362
Intel Core i7-6700HQ CPU 2.60GHz (Skylake), 1 CPU, 8 logical and 4 physical cores
.NET Core SDK=3.1.100
  [Host]     : .NET Core 3.1.0 (CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404), X64 RyuJIT
  DefaultJob : .NET Core 3.1.0 (CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404), X64 RyuJIT


|      Method |       Mean |     Error |    StdDev |     Median |  Gen 0 | Gen 1 | Gen 2 | Allocated |
|------------ |-----------:|----------:|----------:|-----------:|-------:|------:|------:|----------:|
| DynamicCast |  8.2124 ns | 0.0803 ns | 0.0671 ns |  8.2134 ns | 0.0076 |     - |     - |      24 B |
|  ObjectCast | 13.9178 ns | 0.4822 ns | 0.5922 ns | 13.6714 ns | 0.0076 |     - |     - |      24 B |
|  DirectCast |  0.0538 ns | 0.0422 ns | 0.0534 ns |  0.0311 ns |      - |     - |     - |         - |
</code></pre>

<p>The dynamic version is actually faster and not just by a bit. I ran the test multiple times and I can't wrap my head around it.</p>

<p>I looked at the compiled version, if there might be any optimization, but see <a href=""https://sharplab.io/#v2:EYLgxg9gTgpgtADwGwBYA0ATEBqAPgAQCYBGAWACh8BmAAiJoGEaBvCimju2gFQFEA7AK4BbGgBEAnvwCGwgJZgG0gM4AXABQBKdp1blOBugHYa6vkOGb1GKbIWaADAG4dHAL6uan6jXMiaAPLAAFYwYKpKalqeeoac+CZmAiJWECFhqo4u+pweORzetDAWvsnCMZ4GAGIQEGiVnABC0lCeeW5AA"" rel=""nofollow noreferrer"">here</a>. </p>

<p>Can anyone explain this to me?</p>
","i am really confused take the following code
benchmark
public tenum dynamiccast

    return tenumdynamic0


benchmark
public tenum objectcast

    return tenumobject0


benchmark
public tenum directcast

    return tenum0


public enum tenum

    foo
    bar


and yes i know that i could directly cast the integer to the given enum but this is a simplified version of my actual code which includes the work with generic extensions
anyhow i ran a few performance tests because i was curious which one is faster i am using benchmarkdotnet and see there
the dynamic version is actually faster and not just by a bit i ran the test multiple times and i cant wrap my head around it
i looked at the compiled version if there might be any optimization but see here 
can anyone explain this to me
",1,"I am really confused, take the following code:
[Benchmark]
public TEnum DynamicCast()
{
    return (TEnum)(dynamic)0;
}

[Benchmark]
public TEnum ObjectCast()
{
    return (TEnum)(object)0;
}

[Benchmark]
public TEnum DirectCast()
{
    return (TEnum)0;
}

public enum TEnum
{
    Foo,
    Bar
}

And yes I know that I could directly cast the integer to the given Enum, but this is a simplified version of my actual code which includes the work with generic extensions.
Anyhow I ran a few performance tests, because I was curious which one is faster. I am using BenchmarkDotNet and see there:
BenchmarkDotNet=v0.12.0, OS=Windows 10.0.18362
Intel Core i7-6700HQ CPU 2.60GHz (Skylake), 1 CPU, 8 logical and 4 physical cores
.NET Core SDK=3.1.100
  [Host]     : .NET Core 3.1.0 (CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404), X64 RyuJIT
  DefaultJob : .NET Core 3.1.0 (CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404), X64 RyuJIT


|      Method |       Mean |     Error |    StdDev |     Median |  Gen 0 | Gen 1 | Gen 2 | Allocated |
|------------ |-----------:|----------:|----------:|-----------:|-------:|------:|------:|----------:|
| DynamicCast |  8.2124 ns | 0.0803 ns | 0.0671 ns |  8.2134 ns | 0.0076 |     - |     - |      24 B |
|  ObjectCast | 13.9178 ns | 0.4822 ns | 0.5922 ns | 13.6714 ns | 0.0076 |     - |     - |      24 B |
|  DirectCast |  0.0538 ns | 0.0422 ns | 0.0534 ns |  0.0311 ns |      - |     - |     - |         - |

The dynamic version is actually faster and not just by a bit. I ran the test multiple times and I can't wrap my head around it.
I looked at the compiled version, if there might be any optimization, but see here. 
Can anyone explain this to me?
","Why is casting with dynamic faster than with object
I am really confused, take the following code:
[Benchmark]
public TEnum DynamicCast()
{
    return (TEnum)(dynamic)0;
}

[Benchmark]
public TEnum ObjectCast()
{
    return (TEnum)(object)0;
}

[Benchmark]
public TEnum DirectCast()
{
    return (TEnum)0;
}

public enum TEnum
{
    Foo,
    Bar
}

And yes I know that I could directly cast the integer to the given Enum, but this is a simplified version of my actual code which includes the work with generic extensions.
Anyhow I ran a few performance tests, because I was curious which one is faster. I am using BenchmarkDotNet and see there:
BenchmarkDotNet=v0.12.0, OS=Windows 10.0.18362
Intel Core i7-6700HQ CPU 2.60GHz (Skylake), 1 CPU, 8 logical and 4 physical cores
.NET Core SDK=3.1.100
  [Host]     : .NET Core 3.1.0 (CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404), X64 RyuJIT
  DefaultJob : .NET Core 3.1.0 (CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404), X64 RyuJIT


|      Method |       Mean |     Error |    StdDev |     Median |  Gen 0 | Gen 1 | Gen 2 | Allocated |
|------------ |-----------:|----------:|----------:|-----------:|-------:|------:|------:|----------:|
| DynamicCast |  8.2124 ns | 0.0803 ns | 0.0671 ns |  8.2134 ns | 0.0076 |     - |     - |      24 B |
|  ObjectCast | 13.9178 ns | 0.4822 ns | 0.5922 ns | 13.6714 ns | 0.0076 |     - |     - |      24 B |
|  DirectCast |  0.0538 ns | 0.0422 ns | 0.0534 ns |  0.0311 ns |      - |     - |     - |         - |

The dynamic version is actually faster and not just by a bit. I ran the test multiple times and I can't wrap my head around it.
I looked at the compiled version, if there might be any optimization, but see here. 
Can anyone explain this to me?
",0,0,1,0,0,0,0,- Is a comparison question,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
237876,237876,"Refactoring ""to hit"" values for a game","<p>I'm making a game and one of the methods calculates a character's base hit numbers based on skill values. The method currently calculates each value individually, since each skill can be used at short, medium, and long range.</p>

<p>I originally thought I could combine the skills into a tuple and iterate over it, dynamically creating each hit number. But I don't know if it's actually possible, since I currently have each hit number assigned to it's own variable.</p>

<p>I also thought about creating a method for each range, and passing the tuple as an argument. I could create a new tuple or list with the resulting values and then assign them to the individual variables, but I don't see how it would be any better than do it this way, except that it won't look so copy &amp; pasted.</p>

<p>Here's what I currently have:</p>

<pre><code>    def calcBaseHitNumbers(self, dict):
        """"""Calculate character's base hit numbers depending on skill level.""""""

        self.skill_dict = dict

        self.rifle = self.skill_dict.get('CRM', 0)
        self.pistol = self.skill_dict.get('PST', 0)
        self.big_gun = self.skill_dict.get('LCG', 0)
        self.heavy_weapon = self.skill_dict.get('HW', 0)
        self.bow = self.skill_dict.get('LB', 0)
        #self.skill_tuple = (self.rifle, self.pistol, self.big_gun, self.heavy_weapon,
        #    self.bow)

#---Short range
##        for skill in self.skill_tuple:
##            self.base_hit_short = skill * 0.6
        self.charAttribs.bhCRM_short = self.rifle * 0.6
        self.charAttribs.bhPST_short = self.pistol * 0.6
        self.charAttribs.bhHW_short = self.heavy_weapon * 0.6
        self.charAttribs.bhLCG_short = self.big_gun * 0.6
        self.charAttribs.bhLB_short = self.bow * 0.6
#---Med range
        self.charAttribs.bhCRM_med = self.rifle * 0.3
        self.charAttribs.bhPST_med = self.pistol * 0.3
        self.charAttribs.bhHW_med = self.heavy_weapon * 0.3
        self.charAttribs.bhLCG_med = self.big_gun * 0.3
        self.charAttribs.bhLB_med = self.bow * 0.3
#---Long range
        self.charAttribs.bhCRM_long = self.rifle * 0.1
        self.charAttribs.bhPST_long = self.pistol * 0.1
        self.charAttribs.bhHW_long = self.heavy_weapon * 0.1
        self.charAttribs.bhLCG_long = self.big_gun * 0.1
        self.charAttribs.bhLB_long = self.bow * 0.1
</code></pre>

<p>How would you refactor this so it's more dynamic?</p>

<hr>

<p><strong>Edit:</strong> I guess what I want to do is something like this:
Have a tuple (like the one I commented out) and iterate over it 3 times, each time making a new value (for each skill) based on the modifier for each particular range. The resulting value is then automatically assigned to it's respective variable.</p>

<p>In my head, it makes sense. But when I actually try to code it, I get lost. The problem, I think, is that this is the first ""real"" program I've written; all I've done before are small scripts.</p>

<p>This is only the 0.1 version of my program, so it's not critical to refactor it now. However, it seems very un-Pythonic to do this manually and I also want to ""future-proof"" this in case things change down the road.</p>
","im making a game and one of the methods calculates a characters base hit numbers based on skill values the method currently calculates each value individually since each skill can be used at short medium and long range
i originally thought i could combine the skills into a tuple and iterate over it dynamically creating each hit number but i dont know if its actually possible since i currently have each hit number assigned to its own variable
i also thought about creating a method for each range and passing the tuple as an argument i could create a new tuple or list with the resulting values and then assign them to the individual variables but i dont see how it would be any better than do it this way except that it wont look so copy  pasted
heres what i currently have
how would you refactor this so its more dynamic

edit i guess what i want to do is something like this
have a tuple like the one i commented out and iterate over it 3 times each time making a new value for each skill based on the modifier for each particular range the resulting value is then automatically assigned to its respective variable
in my head it makes sense but when i actually try to code it i get lost the problem i think is that this is the first real program ive written all ive done before are small scripts
this is only the 01 version of my program so its not critical to refactor it now however it seems very unpythonic to do this manually and i also want to futureproof this in case things change down the road
",1,"I'm making a game and one of the methods calculates a character's base hit numbers based on skill values. The method currently calculates each value individually, since each skill can be used at short, medium, and long range.
I originally thought I could combine the skills into a tuple and iterate over it, dynamically creating each hit number. But I don't know if it's actually possible, since I currently have each hit number assigned to it's own variable.
I also thought about creating a method for each range, and passing the tuple as an argument. I could create a new tuple or list with the resulting values and then assign them to the individual variables, but I don't see how it would be any better than do it this way, except that it won't look so copy & pasted.
Here's what I currently have:
    def calcBaseHitNumbers(self, dict):
        """"""Calculate character's base hit numbers depending on skill level.""""""

        self.skill_dict = dict

        self.rifle = self.skill_dict.get('CRM', 0)
        self.pistol = self.skill_dict.get('PST', 0)
        self.big_gun = self.skill_dict.get('LCG', 0)
        self.heavy_weapon = self.skill_dict.get('HW', 0)
        self.bow = self.skill_dict.get('LB', 0)
        #self.skill_tuple = (self.rifle, self.pistol, self.big_gun, self.heavy_weapon,
        #    self.bow)

#---Short range
##        for skill in self.skill_tuple:
##            self.base_hit_short = skill * 0.6
        self.charAttribs.bhCRM_short = self.rifle * 0.6
        self.charAttribs.bhPST_short = self.pistol * 0.6
        self.charAttribs.bhHW_short = self.heavy_weapon * 0.6
        self.charAttribs.bhLCG_short = self.big_gun * 0.6
        self.charAttribs.bhLB_short = self.bow * 0.6
#---Med range
        self.charAttribs.bhCRM_med = self.rifle * 0.3
        self.charAttribs.bhPST_med = self.pistol * 0.3
        self.charAttribs.bhHW_med = self.heavy_weapon * 0.3
        self.charAttribs.bhLCG_med = self.big_gun * 0.3
        self.charAttribs.bhLB_med = self.bow * 0.3
#---Long range
        self.charAttribs.bhCRM_long = self.rifle * 0.1
        self.charAttribs.bhPST_long = self.pistol * 0.1
        self.charAttribs.bhHW_long = self.heavy_weapon * 0.1
        self.charAttribs.bhLCG_long = self.big_gun * 0.1
        self.charAttribs.bhLB_long = self.bow * 0.1

How would you refactor this so it's more dynamic?

Edit: I guess what I want to do is something like this:
Have a tuple (like the one I commented out) and iterate over it 3 times, each time making a new value (for each skill) based on the modifier for each particular range. The resulting value is then automatically assigned to it's respective variable.
In my head, it makes sense. But when I actually try to code it, I get lost. The problem, I think, is that this is the first ""real"" program I've written; all I've done before are small scripts.
This is only the 0.1 version of my program, so it's not critical to refactor it now. However, it seems very un-Pythonic to do this manually and I also want to ""future-proof"" this in case things change down the road.
","Refactoring ""to hit"" values for a game
I'm making a game and one of the methods calculates a character's base hit numbers based on skill values. The method currently calculates each value individually, since each skill can be used at short, medium, and long range.
I originally thought I could combine the skills into a tuple and iterate over it, dynamically creating each hit number. But I don't know if it's actually possible, since I currently have each hit number assigned to it's own variable.
I also thought about creating a method for each range, and passing the tuple as an argument. I could create a new tuple or list with the resulting values and then assign them to the individual variables, but I don't see how it would be any better than do it this way, except that it won't look so copy & pasted.
Here's what I currently have:
    def calcBaseHitNumbers(self, dict):
        """"""Calculate character's base hit numbers depending on skill level.""""""

        self.skill_dict = dict

        self.rifle = self.skill_dict.get('CRM', 0)
        self.pistol = self.skill_dict.get('PST', 0)
        self.big_gun = self.skill_dict.get('LCG', 0)
        self.heavy_weapon = self.skill_dict.get('HW', 0)
        self.bow = self.skill_dict.get('LB', 0)
        #self.skill_tuple = (self.rifle, self.pistol, self.big_gun, self.heavy_weapon,
        #    self.bow)

#---Short range
##        for skill in self.skill_tuple:
##            self.base_hit_short = skill * 0.6
        self.charAttribs.bhCRM_short = self.rifle * 0.6
        self.charAttribs.bhPST_short = self.pistol * 0.6
        self.charAttribs.bhHW_short = self.heavy_weapon * 0.6
        self.charAttribs.bhLCG_short = self.big_gun * 0.6
        self.charAttribs.bhLB_short = self.bow * 0.6
#---Med range
        self.charAttribs.bhCRM_med = self.rifle * 0.3
        self.charAttribs.bhPST_med = self.pistol * 0.3
        self.charAttribs.bhHW_med = self.heavy_weapon * 0.3
        self.charAttribs.bhLCG_med = self.big_gun * 0.3
        self.charAttribs.bhLB_med = self.bow * 0.3
#---Long range
        self.charAttribs.bhCRM_long = self.rifle * 0.1
        self.charAttribs.bhPST_long = self.pistol * 0.1
        self.charAttribs.bhHW_long = self.heavy_weapon * 0.1
        self.charAttribs.bhLCG_long = self.big_gun * 0.1
        self.charAttribs.bhLB_long = self.bow * 0.1

How would you refactor this so it's more dynamic?

Edit: I guess what I want to do is something like this:
Have a tuple (like the one I commented out) and iterate over it 3 times, each time making a new value (for each skill) based on the modifier for each particular range. The resulting value is then automatically assigned to it's respective variable.
In my head, it makes sense. But when I actually try to code it, I get lost. The problem, I think, is that this is the first ""real"" program I've written; all I've done before are small scripts.
This is only the 0.1 version of my program, so it's not critical to refactor it now. However, it seems very un-Pythonic to do this manually and I also want to ""future-proof"" this in case things change down the road.
",0,1,0,1,0,0,1,,,,1,1,Code Refactoring,Code Refactoring,,Code Refactoring,0,1,0,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
53255283,53255283,"Increase performance of simple push pop ""stack""","<p>I was asked to do an interview question on Hackerrank to implement a stack. One of the methods of the stack was 'incr' which increments by the bottom of the stacks elements by a value. Although I got the basic test cases right, a few of them ""timed out"". </p>

<pre><code>class Stack:
    def __init__(self):
        self.stack = []
    def push(self, element):
        self.stack.append(element)
    def pop(self):
        if self.stack:
            self.stack.pop()
    def incr(self, num_elements, value):
        '''Increments bottom num_elements by value'''
        for idx in range(num_elements):
            self.stack[idx] += value
    def print_top(self):
        print(self.stack[-1])
</code></pre>

<p>I'm guessing my implementation was too slow. I'm not sure how to make it faster. I tried to replace the 'incr' with the following list comprension but it didn't make a significant difference when I timed it with a million elements.</p>

<pre><code>[x + value for x in self.stack[:num_elements]].extend(self.stack[num_elements:])
</code></pre>

<p>So I'm not exactly sure why I got it wrong.. maybe it was due to the push and the pop?</p>
","i was asked to do an interview question on hackerrank to implement a stack one of the methods of the stack was incr which increments by the bottom of the stacks elements by a value although i got the basic test cases right a few of them timed out 
im guessing my implementation was too slow im not sure how to make it faster i tried to replace the incr with the following list comprension but it didnt make a significant difference when i timed it with a million elements
so im not exactly sure why i got it wrong maybe it was due to the push and the pop
",0,"I was asked to do an interview question on Hackerrank to implement a stack. One of the methods of the stack was 'incr' which increments by the bottom of the stacks elements by a value. Although I got the basic test cases right, a few of them ""timed out"". 
class Stack:
    def __init__(self):
        self.stack = []
    def push(self, element):
        self.stack.append(element)
    def pop(self):
        if self.stack:
            self.stack.pop()
    def incr(self, num_elements, value):
        '''Increments bottom num_elements by value'''
        for idx in range(num_elements):
            self.stack[idx] += value
    def print_top(self):
        print(self.stack[-1])

I'm guessing my implementation was too slow. I'm not sure how to make it faster. I tried to replace the 'incr' with the following list comprension but it didn't make a significant difference when I timed it with a million elements.
[x + value for x in self.stack[:num_elements]].extend(self.stack[num_elements:])

So I'm not exactly sure why I got it wrong.. maybe it was due to the push and the pop?
","Increase performance of simple push pop ""stack""
I was asked to do an interview question on Hackerrank to implement a stack. One of the methods of the stack was 'incr' which increments by the bottom of the stacks elements by a value. Although I got the basic test cases right, a few of them ""timed out"". 
class Stack:
    def __init__(self):
        self.stack = []
    def push(self, element):
        self.stack.append(element)
    def pop(self):
        if self.stack:
            self.stack.pop()
    def incr(self, num_elements, value):
        '''Increments bottom num_elements by value'''
        for idx in range(num_elements):
            self.stack[idx] += value
    def print_top(self):
        print(self.stack[-1])

I'm guessing my implementation was too slow. I'm not sure how to make it faster. I tried to replace the 'incr' with the following list comprension but it didn't make a significant difference when I timed it with a million elements.
[x + value for x in self.stack[:num_elements]].extend(self.stack[num_elements:])

So I'm not exactly sure why I got it wrong.. maybe it was due to the push and the pop?
",0,1,0,0,0,0,1,- Suboptimal code ,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,0,0,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
72192607,72192607,How to optimize a method that transforms a byte[] to a 4d tensor (or array)?,"<p>I am working on a program that removes a user's background in real-time using a webcam.</p>
<p>To do the background subtraction, I am using an ML model. The input of this model is a 4D tensor in the shape BCHW (batch, channel, height, width), where the RGB values range between 0 and 1. I am only using one image at the same time, so batch = 1, and it effectively makes it a 3D tensor.</p>
<p>So, I have to write some code to transform a byte array, the input image from the webcam, to the tensor.</p>
<p>Say you have an <code>byte[]</code> like this: <code>RGB RGB RGB RGB RGB RGB RGB RGB RGB</code>,</p>
<p>then the code should transform it to: <code>RRRRRRRRR GGGGGGGGG BBBBBBBBB</code>.</p>
<p>To do this I have come up with the following method:</p>
<pre><code>public Tensor&lt;float&gt; ImageToTensor(byte[] pixels, int height, int width)
{
    var tensorSize = 1 * 3 * height * width;

    float[] data = new float[tensorSize];

    var channelSize = width * height;
    var redChannel = 0;
    var greenChannel = channelSize;
    var blueChannel = (channelSize * 2);

    Parallel.For(0, height, y =&gt;
    {
        int rowStart = y * width * 3;
        int rowWidth = width * 3;

        var row = new Span&lt;byte&gt;(pixels, rowStart, rowWidth);

        for (int i = 0; i &lt; row.Length; i += 3)
        {
            byte red = row[i];
            byte green = row[i + 1];
            byte blue = row[i + 2];

            var index = i / 3 + y * width;

            data[redChannel + index] = red / 255f;
            data[greenChannel + index] = green / 255f;
            data[blueChannel + index] = blue / 255f;
        }
    });

    Memory&lt;float&gt; memory = new Memory&lt;float&gt;(data);

    return new DenseTensor&lt;float&gt;(memory, new[] { 1, 3, height, width });
}
</code></pre>
<p>I have benchmarked this with an 1920 x 1080 image using Benchmarkdotnet, and it takes about 5 ms.</p>
<p>While this is not a lot, the ML model takes about 8 ms, and I also have a post-processing step.</p>
<p>The goal is to run the video stream at 60 FPS. This means the whole process can only take 16.6ms. Since the ML model takes 8ms, I only have about 8 to 9 ms left for the pre-processing and post-processing steps.</p>
<p>The post and pre-processing steps are roughly the same, but the post-processing step is a bit more complex and takes about double the time. So, I am currently at <code>5ms + 8ms + 10ms = 23ms</code>.</p>
<p>That's 6 - 7ms too much.</p>
<p>So, my question is, how do I optimize the code above?
I have already tried multiple things, but nothing really seems to improve it by a lot.</p>
<h2>Improved code:</h2>
<pre><code>public Tensor&lt;float&gt; ImageToTensor(byte[] pixels, int height, int width)
{
    var tensorSize = 1 * 3 * height * width;

    float[] data = new float[tensorSize];

    var channelSize = width * height;
    var redChannel = 0;
    var greenChannel = channelSize;
    var blueChannel = (channelSize * 2);

    int floatSlots = Vector&lt;float&gt;.Count;

    Parallel.For(0, height, y =&gt;
    {
        int rowStart = y * width * 3;
        int rowWidth = width * 3;

        var row = new Span&lt;byte&gt;(pixels, rowStart, rowWidth);

        int numOfVectors = row.Length / (floatSlots * 3);
        int ceiling = numOfVectors * (floatSlots * 3);

        var reds = new float[floatSlots];
        var greens = new float[floatSlots];
        var blues = new float[floatSlots];

        for (int i = 0; i &lt; ceiling; i += (floatSlots * 3))
        {
            for (int j = 0; j &lt; (3 * floatSlots); j += 3)
            {
                reds[j / 3] = row[j] / 255f;
                greens[j / 3] = row[j + 1] / 255f;
                blues[j / 3] = row[j + 2] / 255f;
            }

            var vecRed = new Vector&lt;float&gt;(reds);
            var vecGreen = new Vector&lt;float&gt;(greens);
            var vecBlue = new Vector&lt;float&gt;(blues);

            vecRed.CopyTo(data, i + redChannel);
            vecGreen.CopyTo(data, i + greenChannel);
            vecBlue.CopyTo(data, i + blueChannel);
        }

        for (int i = ceiling; i &lt; row.Length; i += 3)
        {
            byte red = row[i];
            byte green = row[i + 1];
            byte blue = row[i + 2];

            var index = i / 3 + y * width;

            data[redChannel + index] = red / 255f;
            data[greenChannel + index] = green / 255f;
            data[blueChannel + index] = blue / 255f;
        }
    });

    Memory&lt;float&gt; memory = new Memory&lt;float&gt;(data);

    return new DenseTensor&lt;float&gt;(memory, new[] { 1, 3, height, width });
}
</code></pre>
<p>I have improved my code by using SIMD Vectors and this made the code 20% faster. It went from 5ms to 4ms.</p>
","i am working on a program that removes a users background in realtime using a webcam
to do the background subtraction i am using an ml model the input of this model is a 4d tensor in the shape bchw batch channel height width where the rgb values range between 0 and 1 i am only using one image at the same time so batch  1 and it effectively makes it a 3d tensor
so i have to write some code to transform a byte array the input image from the webcam to the tensor
say you have an byte like this rgb rgb rgb rgb rgb rgb rgb rgb rgb
then the code should transform it to rrrrrrrrr ggggggggg bbbbbbbbb
to do this i have come up with the following method
i have benchmarked this with an 1920 x 1080 image using benchmarkdotnet and it takes about 5 ms
while this is not a lot the ml model takes about 8 ms and i also have a postprocessing step
the goal is to run the video stream at 60 fps this means the whole process can only take 166ms since the ml model takes 8ms i only have about 8 to 9 ms left for the preprocessing and postprocessing steps
the post and preprocessing steps are roughly the same but the postprocessing step is a bit more complex and takes about double the time so i am currently at 5ms  8ms  10ms  23ms
thats 6  7ms too much
so my question is how do i optimize the code above
i have already tried multiple things but nothing really seems to improve it by a lot
improved code
i have improved my code by using simd vectors and this made the code 20 faster it went from 5ms to 4ms
",1,"I am working on a program that removes a user's background in real-time using a webcam.
To do the background subtraction, I am using an ML model. The input of this model is a 4D tensor in the shape BCHW (batch, channel, height, width), where the RGB values range between 0 and 1. I am only using one image at the same time, so batch = 1, and it effectively makes it a 3D tensor.
So, I have to write some code to transform a byte array, the input image from the webcam, to the tensor.
Say you have an byte[] like this: RGB RGB RGB RGB RGB RGB RGB RGB RGB,
then the code should transform it to: RRRRRRRRR GGGGGGGGG BBBBBBBBB.
To do this I have come up with the following method:
public Tensor<float> ImageToTensor(byte[] pixels, int height, int width)
{
    var tensorSize = 1 * 3 * height * width;

    float[] data = new float[tensorSize];

    var channelSize = width * height;
    var redChannel = 0;
    var greenChannel = channelSize;
    var blueChannel = (channelSize * 2);

    Parallel.For(0, height, y =>
    {
        int rowStart = y * width * 3;
        int rowWidth = width * 3;

        var row = new Span<byte>(pixels, rowStart, rowWidth);

        for (int i = 0; i < row.Length; i += 3)
        {
            byte red = row[i];
            byte green = row[i + 1];
            byte blue = row[i + 2];

            var index = i / 3 + y * width;

            data[redChannel + index] = red / 255f;
            data[greenChannel + index] = green / 255f;
            data[blueChannel + index] = blue / 255f;
        }
    });

    Memory<float> memory = new Memory<float>(data);

    return new DenseTensor<float>(memory, new[] { 1, 3, height, width });
}

I have benchmarked this with an 1920 x 1080 image using Benchmarkdotnet, and it takes about 5 ms.
While this is not a lot, the ML model takes about 8 ms, and I also have a post-processing step.
The goal is to run the video stream at 60 FPS. This means the whole process can only take 16.6ms. Since the ML model takes 8ms, I only have about 8 to 9 ms left for the pre-processing and post-processing steps.
The post and pre-processing steps are roughly the same, but the post-processing step is a bit more complex and takes about double the time. So, I am currently at 5ms + 8ms + 10ms = 23ms.
That's 6 - 7ms too much.
So, my question is, how do I optimize the code above?
I have already tried multiple things, but nothing really seems to improve it by a lot.
Improved code:
public Tensor<float> ImageToTensor(byte[] pixels, int height, int width)
{
    var tensorSize = 1 * 3 * height * width;

    float[] data = new float[tensorSize];

    var channelSize = width * height;
    var redChannel = 0;
    var greenChannel = channelSize;
    var blueChannel = (channelSize * 2);

    int floatSlots = Vector<float>.Count;

    Parallel.For(0, height, y =>
    {
        int rowStart = y * width * 3;
        int rowWidth = width * 3;

        var row = new Span<byte>(pixels, rowStart, rowWidth);

        int numOfVectors = row.Length / (floatSlots * 3);
        int ceiling = numOfVectors * (floatSlots * 3);

        var reds = new float[floatSlots];
        var greens = new float[floatSlots];
        var blues = new float[floatSlots];

        for (int i = 0; i < ceiling; i += (floatSlots * 3))
        {
            for (int j = 0; j < (3 * floatSlots); j += 3)
            {
                reds[j / 3] = row[j] / 255f;
                greens[j / 3] = row[j + 1] / 255f;
                blues[j / 3] = row[j + 2] / 255f;
            }

            var vecRed = new Vector<float>(reds);
            var vecGreen = new Vector<float>(greens);
            var vecBlue = new Vector<float>(blues);

            vecRed.CopyTo(data, i + redChannel);
            vecGreen.CopyTo(data, i + greenChannel);
            vecBlue.CopyTo(data, i + blueChannel);
        }

        for (int i = ceiling; i < row.Length; i += 3)
        {
            byte red = row[i];
            byte green = row[i + 1];
            byte blue = row[i + 2];

            var index = i / 3 + y * width;

            data[redChannel + index] = red / 255f;
            data[greenChannel + index] = green / 255f;
            data[blueChannel + index] = blue / 255f;
        }
    });

    Memory<float> memory = new Memory<float>(data);

    return new DenseTensor<float>(memory, new[] { 1, 3, height, width });
}

I have improved my code by using SIMD Vectors and this made the code 20% faster. It went from 5ms to 4ms.
","How to optimize a method that transforms a byte[] to a 4d tensor (or array)?
I am working on a program that removes a user's background in real-time using a webcam.
To do the background subtraction, I am using an ML model. The input of this model is a 4D tensor in the shape BCHW (batch, channel, height, width), where the RGB values range between 0 and 1. I am only using one image at the same time, so batch = 1, and it effectively makes it a 3D tensor.
So, I have to write some code to transform a byte array, the input image from the webcam, to the tensor.
Say you have an byte[] like this: RGB RGB RGB RGB RGB RGB RGB RGB RGB,
then the code should transform it to: RRRRRRRRR GGGGGGGGG BBBBBBBBB.
To do this I have come up with the following method:
public Tensor<float> ImageToTensor(byte[] pixels, int height, int width)
{
    var tensorSize = 1 * 3 * height * width;

    float[] data = new float[tensorSize];

    var channelSize = width * height;
    var redChannel = 0;
    var greenChannel = channelSize;
    var blueChannel = (channelSize * 2);

    Parallel.For(0, height, y =>
    {
        int rowStart = y * width * 3;
        int rowWidth = width * 3;

        var row = new Span<byte>(pixels, rowStart, rowWidth);

        for (int i = 0; i < row.Length; i += 3)
        {
            byte red = row[i];
            byte green = row[i + 1];
            byte blue = row[i + 2];

            var index = i / 3 + y * width;

            data[redChannel + index] = red / 255f;
            data[greenChannel + index] = green / 255f;
            data[blueChannel + index] = blue / 255f;
        }
    });

    Memory<float> memory = new Memory<float>(data);

    return new DenseTensor<float>(memory, new[] { 1, 3, height, width });
}

I have benchmarked this with an 1920 x 1080 image using Benchmarkdotnet, and it takes about 5 ms.
While this is not a lot, the ML model takes about 8 ms, and I also have a post-processing step.
The goal is to run the video stream at 60 FPS. This means the whole process can only take 16.6ms. Since the ML model takes 8ms, I only have about 8 to 9 ms left for the pre-processing and post-processing steps.
The post and pre-processing steps are roughly the same, but the post-processing step is a bit more complex and takes about double the time. So, I am currently at 5ms + 8ms + 10ms = 23ms.
That's 6 - 7ms too much.
So, my question is, how do I optimize the code above?
I have already tried multiple things, but nothing really seems to improve it by a lot.
Improved code:
public Tensor<float> ImageToTensor(byte[] pixels, int height, int width)
{
    var tensorSize = 1 * 3 * height * width;

    float[] data = new float[tensorSize];

    var channelSize = width * height;
    var redChannel = 0;
    var greenChannel = channelSize;
    var blueChannel = (channelSize * 2);

    int floatSlots = Vector<float>.Count;

    Parallel.For(0, height, y =>
    {
        int rowStart = y * width * 3;
        int rowWidth = width * 3;

        var row = new Span<byte>(pixels, rowStart, rowWidth);

        int numOfVectors = row.Length / (floatSlots * 3);
        int ceiling = numOfVectors * (floatSlots * 3);

        var reds = new float[floatSlots];
        var greens = new float[floatSlots];
        var blues = new float[floatSlots];

        for (int i = 0; i < ceiling; i += (floatSlots * 3))
        {
            for (int j = 0; j < (3 * floatSlots); j += 3)
            {
                reds[j / 3] = row[j] / 255f;
                greens[j / 3] = row[j + 1] / 255f;
                blues[j / 3] = row[j + 2] / 255f;
            }

            var vecRed = new Vector<float>(reds);
            var vecGreen = new Vector<float>(greens);
            var vecBlue = new Vector<float>(blues);

            vecRed.CopyTo(data, i + redChannel);
            vecGreen.CopyTo(data, i + greenChannel);
            vecBlue.CopyTo(data, i + blueChannel);
        }

        for (int i = ceiling; i < row.Length; i += 3)
        {
            byte red = row[i];
            byte green = row[i + 1];
            byte blue = row[i + 2];

            var index = i / 3 + y * width;

            data[redChannel + index] = red / 255f;
            data[greenChannel + index] = green / 255f;
            data[blueChannel + index] = blue / 255f;
        }
    });

    Memory<float> memory = new Memory<float>(data);

    return new DenseTensor<float>(memory, new[] { 1, 3, height, width });
}

I have improved my code by using SIMD Vectors and this made the code 20% faster. It went from 5ms to 4ms.
",1,1,0,0,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,1,1,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
76560743,76560743,Why is a System.Collections.List<T>.Add faster than manual memory management?,"<p>Hey there I am currently experimenting with very low-level C# mainly to learn and bring my skills to the next level for a performance heavy project.
I tried to create sth like a NativeList so a List without any automatic garbage collection.
I want to use structs and manual pointer allocation to create a memory and cache-line aligned List implementation. This may look familiar for the Unity people out here.</p>
<p>The issue I am facing right now that I get some unexpected results after benchmarking using BenchmarkDotnet.
Reads are as expected a bit faster mainly because I use pointers directly (I assume) and array access is pretty fast in C# anyway.
Writes are a lot faster ~70% actually which is confusing but may be caused by the memory alignment.
Add however is ~5% slower and I can't figure out why.</p>
<p>Please note that there are zero allocations involved in the benchmark because both Lists keep their internal buffer alive. The only part left are the if conditions which are very similar to what List does internally.</p>
<p>Do you have some suggestions to make this even faster or can tell me why my list has slower Adds?</p>
<h2>Benchmark results:</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Method</th>
<th>Size</th>
<th style=""text-align: right;"">Mean</th>
<th style=""text-align: right;"">Error</th>
<th style=""text-align: right;"">StdDev</th>
<th style=""text-align: right;"">Ratio</th>
<th style=""text-align: right;"">RatioSD</th>
<th style=""text-align: right;"">CacheMisses/Op</th>
</tr>
</thead>
<tbody>
<tr>
<td>Native_Add</td>
<td>100</td>
<td style=""text-align: right;"">934.59 ns</td>
<td style=""text-align: right;"">5.828 ns</td>
<td style=""text-align: right;"">4.866 ns</td>
<td style=""text-align: right;"">1.04</td>
<td style=""text-align: right;"">0.01</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td>List_Add</td>
<td>100</td>
<td style=""text-align: right;"">894.44 ns</td>
<td style=""text-align: right;"">7.063 ns</td>
<td style=""text-align: right;"">6.607 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Add</td>
<td>1000</td>
<td style=""text-align: right;"">9,463.38 ns</td>
<td style=""text-align: right;"">183.577 ns</td>
<td style=""text-align: right;"">162.736 ns</td>
<td style=""text-align: right;"">1.03</td>
<td style=""text-align: right;"">0.03</td>
<td style=""text-align: right;"">16</td>
</tr>
<tr>
<td>List_Add</td>
<td>1000</td>
<td style=""text-align: right;"">9,196.11 ns</td>
<td style=""text-align: right;"">87.191 ns</td>
<td style=""text-align: right;"">81.558 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">17</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Add</td>
<td>10000</td>
<td style=""text-align: right;"">93,361.52 ns</td>
<td style=""text-align: right;"">876.444 ns</td>
<td style=""text-align: right;"">776.945 ns</td>
<td style=""text-align: right;"">1.05</td>
<td style=""text-align: right;"">0.01</td>
<td style=""text-align: right;"">112</td>
</tr>
<tr>
<td>List_Add</td>
<td>10000</td>
<td style=""text-align: right;"">88,501.43 ns</td>
<td style=""text-align: right;"">362.486 ns</td>
<td style=""text-align: right;"">302.692 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">53</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Get</td>
<td>100</td>
<td style=""text-align: right;"">41.38 ns</td>
<td style=""text-align: right;"">0.371 ns</td>
<td style=""text-align: right;"">0.347 ns</td>
<td style=""text-align: right;"">0.94</td>
<td style=""text-align: right;"">0.02</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td>List_Get</td>
<td>100</td>
<td style=""text-align: right;"">43.93 ns</td>
<td style=""text-align: right;"">0.849 ns</td>
<td style=""text-align: right;"">0.794 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Get</td>
<td>1000</td>
<td style=""text-align: right;"">366.97 ns</td>
<td style=""text-align: right;"">4.445 ns</td>
<td style=""text-align: right;"">3.940 ns</td>
<td style=""text-align: right;"">0.98</td>
<td style=""text-align: right;"">0.02</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td>List_Get</td>
<td>1000</td>
<td style=""text-align: right;"">374.97 ns</td>
<td style=""text-align: right;"">3.053 ns</td>
<td style=""text-align: right;"">2.550 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Get</td>
<td>10000</td>
<td style=""text-align: right;"">3,674.04 ns</td>
<td style=""text-align: right;"">60.222 ns</td>
<td style=""text-align: right;"">56.331 ns</td>
<td style=""text-align: right;"">0.98</td>
<td style=""text-align: right;"">0.02</td>
<td style=""text-align: right;"">5</td>
</tr>
<tr>
<td>List_Get</td>
<td>10000</td>
<td style=""text-align: right;"">3,733.41 ns</td>
<td style=""text-align: right;"">35.220 ns</td>
<td style=""text-align: right;"">32.945 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">4</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Set</td>
<td>100</td>
<td style=""text-align: right;"">56.57 ns</td>
<td style=""text-align: right;"">0.357 ns</td>
<td style=""text-align: right;"">0.279 ns</td>
<td style=""text-align: right;"">0.34</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td>List_Set</td>
<td>100</td>
<td style=""text-align: right;"">164.29 ns</td>
<td style=""text-align: right;"">0.936 ns</td>
<td style=""text-align: right;"">0.875 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Set</td>
<td>1000</td>
<td style=""text-align: right;"">493.52 ns</td>
<td style=""text-align: right;"">8.668 ns</td>
<td style=""text-align: right;"">8.108 ns</td>
<td style=""text-align: right;"">0.30</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td>List_Set</td>
<td>1000</td>
<td style=""text-align: right;"">1,665.81 ns</td>
<td style=""text-align: right;"">10.073 ns</td>
<td style=""text-align: right;"">7.865 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td></td>
<td></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td>Native_Set</td>
<td>10000</td>
<td style=""text-align: right;"">4,984.44 ns</td>
<td style=""text-align: right;"">94.866 ns</td>
<td style=""text-align: right;"">88.737 ns</td>
<td style=""text-align: right;"">0.30</td>
<td style=""text-align: right;"">0.01</td>
<td style=""text-align: right;"">10</td>
</tr>
<tr>
<td>List_Set</td>
<td>10000</td>
<td style=""text-align: right;"">16,699.32 ns</td>
<td style=""text-align: right;"">59.226 ns</td>
<td style=""text-align: right;"">49.456 ns</td>
<td style=""text-align: right;"">1.00</td>
<td style=""text-align: right;"">0.00</td>
<td style=""text-align: right;"">18</td>
</tr>
</tbody>
</table>
</div><h2>Struct:</h2>
<pre class=""lang-cs prettyprint-override""><code>public unsafe struct NativeList&lt;T&gt; : IDisposable where T : unmanaged {
    private static readonly int MinCapacity = UnsafeUtils.CacheLineSize / sizeof(T);
    private T* _buffer;
    private int _capacity;
    private int _count;

    public int Capacity =&gt; _capacity;
    public int Count =&gt; _count;
    public T this[int index] {
        get {
            if((uint)index &gt;= (uint)_count) throw new ArgumentOutOfRangeException(nameof(index));
            return _buffer[index];
        }
        set {
            if((uint)index &gt;= (uint)_count) throw new ArgumentOutOfRangeException(nameof(index));
            _buffer[index] = value;
        }
    }

    public NativeList() {
    }

    public NativeList(int capacity) {
        if(capacity != 0) Resize(capacity);
    }

    public void Add(T value) {
        EnsureCapacity(1);
        _buffer[_count] = value;
        _count++;
    }

    public void AddRange(T* values, int count) {
        if(count == 0) return;
        EnsureCapacity(count);
        UnsafeUtils.Copy&lt;T&gt;(values, 0, _buffer, _count, count);
        _count += count;
    }

    public bool Remove(T value) {
        var index = IndexOf(value);
        if(index &lt; 0) return false;
        RemoveAt(index);
        return true;
    }

    public void RemoveAt(int index) {
        if(index &gt;= _count) throw new ArgumentOutOfRangeException(nameof(index));

        _count--;
        if(_count != index) {
            UnsafeUtils.Copy&lt;T&gt;(_buffer, index + 1, _buffer, index, _count - index);
        }
    }
    public void RemoveRangeAt(int index, int count) {
        if(index &lt; 0 || index &gt;= _count) throw new ArgumentOutOfRangeException(nameof(index));
        if(count &lt; 0 || index + count &gt; _count) throw new ArgumentOutOfRangeException(nameof(count));
        if(count == 0) return;

        _count -= count;
        if(_count != index) {
            UnsafeUtils.Copy&lt;T&gt;(_buffer, index + count, _buffer, index, _count - index);
        }
        _count -= count;
    }

    public bool Contains(T value) {
        return IndexOf(value, EqualityComparer&lt;T&gt;.Default) &gt;= 0;
    }
    public bool Contains(T value, IEqualityComparer&lt;T&gt; equalityComparer) {
        return IndexOf(value, equalityComparer) &gt;= 0;
    }
    
    public int IndexOf(T value) {
        return IndexOf(value, EqualityComparer&lt;T&gt;.Default);
    }
    public int IndexOf(T value, IEqualityComparer&lt;T&gt; equalityComparer) {
        for(var i = 0; i &lt; _count; i++) {
            if(equalityComparer.Equals(((T*)_buffer)[i], value)) return i;
        }
        return -1;
    }
    
    public void Clear() {
        _count = 0;
    }
    
    public Span&lt;T&gt; AsSpan() {
        return UnsafeUtils.CreateSpan&lt;T&gt;(_buffer, _count);
    }

    public Enumerator GetEnumerator() {
        return new Enumerator(_buffer, _count);
    }

    private void EnsureCapacity(int count) {
        var minCapacity = _count + count;
        if(minCapacity &gt; _capacity) Resize(minCapacity);
    }

    private void Resize(int capacity) {
        capacity = Math.Max(MinCapacity, (int)BitOperations.RoundUpToPowerOf2((uint)capacity));
        _buffer = (T*)UnsafeUtils.AlignedRealloc&lt;T&gt;(_buffer, capacity);
        _capacity = capacity;
    }

    public void Dispose() {
        if(_buffer is null) return;
        UnsafeUtils.AlignedFree(_buffer);
        _capacity = 0;
        _buffer = null;
    }

    public struct Enumerator : IEnumerator&lt;T&gt; {
        private readonly void* _buffer;
        private readonly int _count;
        private int _index;
        
        public T Current { get; private set; }
        object IEnumerator.Current =&gt; Current;

        public Enumerator(void* buffer, int count) {
            _buffer = buffer;
            _count = count;
            _index = -1;
        }

        public bool MoveNext() {
            if(++_index &gt;= _count) return false;
            Current = ((T*)_buffer)[_index];
            return true;
        }

        public void Reset() {
            _index = -1;
        }
        public void Dispose() {}
    }
}
</code></pre>
<h2>Benchmark:</h2>
<pre class=""lang-cs prettyprint-override""><code>[SimpleJob]
[MemoryDiagnoser]
[HardwareCounters(HardwareCounter.CacheMisses)]
[GroupBenchmarksBy(BenchmarkLogicalGroupRule.ByCategory, BenchmarkLogicalGroupRule.ByParams)]
public class NativeListBenchmark {
    private NativeList&lt;int&gt; _fullNativeList = new();
    private List&lt;int&gt; _fullList = new();
    private Random _random;
    
    private NativeList&lt;int&gt; _nativeList = new();
    private List&lt;int&gt; _list = new();

    [Params(100, 1000, 10_000)]
    public int Size { get; set; }

    [GlobalSetup]
    public void Setup() {
        _random = new Random(1337);
        for(var i = 0; i &lt; Size; i++) {
            _fullNativeList.Add(i);
        }
        for(var i = 0; i &lt; Size; i++) {
            _fullList.Add(i);
        }
    }

    [GlobalCleanup]
    public void Cleanup() {
        _nativeList.Dispose();
        _fullNativeList.Dispose();
    }

    [Benchmark]
    [BenchmarkCategory(&quot;Add&quot;)]
    public void Native_Add() {
        _nativeList.Clear();
        for(var i = 0; i &lt; Size; i++) {
            _nativeList.Add(_random.Next());
        }
    }
    [Benchmark]
    [BenchmarkCategory(&quot;Get&quot;)]
    public void Native_Get() {
        for(var i = 0; i &lt; Size; i++) {
            _ = _fullNativeList[i];
        }
    }

    [Benchmark]
    [BenchmarkCategory(&quot;Set&quot;)]
    public void Native_Set() {
        for(var i = 0; i &lt; Size; i++) {
            _fullNativeList[i] = i;
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(&quot;Add&quot;)]
    public void List_Add() {
        _list.Clear();
        for(var i = 0; i &lt; Size; i++) {
            _list.Add(_random.Next());
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(&quot;Get&quot;)]
    public void List_Get() {
        for(var i = 0; i &lt; Size; i++) {
            _ = _fullList[i];
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(&quot;Set&quot;)]
    public void List_Set() {
        for(var i = 0; i &lt; Size; i++) {
            _fullList[i] = i;
        }
    }
}
</code></pre>
","hey there i am currently experimenting with very lowlevel c mainly to learn and bring my skills to the next level for a performance heavy project
i tried to create sth like a nativelist so a list without any automatic garbage collection
i want to use structs and manual pointer allocation to create a memory and cacheline aligned list implementation this may look familiar for the unity people out here
the issue i am facing right now that i get some unexpected results after benchmarking using benchmarkdotnet
reads are as expected a bit faster mainly because i use pointers directly i assume and array access is pretty fast in c anyway
writes are a lot faster 70 actually which is confusing but may be caused by the memory alignment
add however is 5 slower and i cant figure out why
please note that there are zero allocations involved in the benchmark because both lists keep their internal buffer alive the only part left are the if conditions which are very similar to what list does internally
do you have some suggestions to make this even faster or can tell me why my list has slower adds
benchmark results




method
size
mean
error
stddev
ratio
ratiosd
cachemissesop




native_add
100
93459 ns
5828 ns
4866 ns
104
001
2


list_add
100
89444 ns
7063 ns
6607 ns
100
000
1












native_add
1000
946338 ns
183577 ns
162736 ns
103
003
16


list_add
1000
919611 ns
87191 ns
81558 ns
100
000
17












native_add
10000
9336152 ns
876444 ns
776945 ns
105
001
112


list_add
10000
8850143 ns
362486 ns
302692 ns
100
000
53












native_get
100
4138 ns
0371 ns
0347 ns
094
002
0


list_get
100
4393 ns
0849 ns
0794 ns
100
000
0












native_get
1000
36697 ns
4445 ns
3940 ns
098
002
0


list_get
1000
37497 ns
3053 ns
2550 ns
100
000
0












native_get
10000
367404 ns
60222 ns
56331 ns
098
002
5


list_get
10000
373341 ns
35220 ns
32945 ns
100
000
4












native_set
100
5657 ns
0357 ns
0279 ns
034
000
0


list_set
100
16429 ns
0936 ns
0875 ns
100
000
0












native_set
1000
49352 ns
8668 ns
8108 ns
030
000
1


list_set
1000
166581 ns
10073 ns
7865 ns
100
000
2












native_set
10000
498444 ns
94866 ns
88737 ns
030
001
10


list_set
10000
1669932 ns
59226 ns
49456 ns
100
000
18



struct
public unsafe struct nativelistt  idisposable where t  unmanaged 
    private static readonly int mincapacity  unsafeutilscachelinesize  sizeoft
    private t _buffer
    private int _capacity
    private int _count

    public int capacity  _capacity
    public int count  _count
    public t thisint index 
        get 
            ifuintindex  uint_count throw new argumentoutofrangeexceptionnameofindex
            return _bufferindex
        
        set 
            ifuintindex  uint_count throw new argumentoutofrangeexceptionnameofindex
            _bufferindex  value
        
    

    public nativelist 
    

    public nativelistint capacity 
        ifcapacity  0 resizecapacity
    

    public void addt value 
        ensurecapacity1
        _buffer_count  value
        _count
    

    public void addranget values int count 
        ifcount  0 return
        ensurecapacitycount
        unsafeutilscopytvalues 0 _buffer _count count
        _count  count
    

    public bool removet value 
        var index  indexofvalue
        ifindex  0 return false
        removeatindex
        return true
    

    public void removeatint index 
        ifindex  _count throw new argumentoutofrangeexceptionnameofindex

        _count
        if_count  index 
            unsafeutilscopyt_buffer index  1 _buffer index _count  index
        
    
    public void removerangeatint index int count 
        ifindex  0  index  _count throw new argumentoutofrangeexceptionnameofindex
        ifcount  0  index  count  _count throw new argumentoutofrangeexceptionnameofcount
        ifcount  0 return

        _count  count
        if_count  index 
            unsafeutilscopyt_buffer index  count _buffer index _count  index
        
        _count  count
    

    public bool containst value 
        return indexofvalue equalitycomparertdefault  0
    
    public bool containst value iequalitycomparert equalitycomparer 
        return indexofvalue equalitycomparer  0
    
    
    public int indexoft value 
        return indexofvalue equalitycomparertdefault
    
    public int indexoft value iequalitycomparert equalitycomparer 
        forvar i  0 i  _count i 
            ifequalitycomparerequalst_bufferi value return i
        
        return 1
    
    
    public void clear 
        _count  0
    
    
    public spant asspan 
        return unsafeutilscreatespant_buffer _count
    

    public enumerator getenumerator 
        return new enumerator_buffer _count
    

    private void ensurecapacityint count 
        var mincapacity  _count  count
        ifmincapacity  _capacity resizemincapacity
    

    private void resizeint capacity 
        capacity  mathmaxmincapacity intbitoperationsrounduptopowerof2uintcapacity
        _buffer  tunsafeutilsalignedrealloct_buffer capacity
        _capacity  capacity
    

    public void dispose 
        if_buffer is null return
        unsafeutilsalignedfree_buffer
        _capacity  0
        _buffer  null
    

    public struct enumerator  ienumeratort 
        private readonly void _buffer
        private readonly int _count
        private int _index
        
        public t current  get private set 
        object ienumeratorcurrent  current

        public enumeratorvoid buffer int count 
            _buffer  buffer
            _count  count
            _index  1
        

        public bool movenext 
            if_index  _count return false
            current  t_buffer_index
            return true
        

        public void reset 
            _index  1
        
        public void dispose 
    


benchmark
simplejob
memorydiagnoser
hardwarecountershardwarecountercachemisses
groupbenchmarksbybenchmarklogicalgrouprulebycategory benchmarklogicalgrouprulebyparams
public class nativelistbenchmark 
    private nativelistint _fullnativelist  new
    private listint _fulllist  new
    private random _random
    
    private nativelistint _nativelist  new
    private listint _list  new

    params100 1000 10_000
    public int size  get set 

    globalsetup
    public void setup 
        _random  new random1337
        forvar i  0 i  size i 
            _fullnativelistaddi
        
        forvar i  0 i  size i 
            _fulllistaddi
        
    

    globalcleanup
    public void cleanup 
        _nativelistdispose
        _fullnativelistdispose
    

    benchmark
    benchmarkcategoryadd
    public void native_add 
        _nativelistclear
        forvar i  0 i  size i 
            _nativelistadd_randomnext
        
    
    benchmark
    benchmarkcategoryget
    public void native_get 
        forvar i  0 i  size i 
            _  _fullnativelisti
        
    

    benchmark
    benchmarkcategoryset
    public void native_set 
        forvar i  0 i  size i 
            _fullnativelisti  i
        
    

    benchmarkbaseline  true
    benchmarkcategoryadd
    public void list_add 
        _listclear
        forvar i  0 i  size i 
            _listadd_randomnext
        
    

    benchmarkbaseline  true
    benchmarkcategoryget
    public void list_get 
        forvar i  0 i  size i 
            _  _fulllisti
        
    

    benchmarkbaseline  true
    benchmarkcategoryset
    public void list_set 
        forvar i  0 i  size i 
            _fulllisti  i
        
    


",1,"Hey there I am currently experimenting with very low-level C# mainly to learn and bring my skills to the next level for a performance heavy project.
I tried to create sth like a NativeList so a List without any automatic garbage collection.
I want to use structs and manual pointer allocation to create a memory and cache-line aligned List implementation. This may look familiar for the Unity people out here.
The issue I am facing right now that I get some unexpected results after benchmarking using BenchmarkDotnet.
Reads are as expected a bit faster mainly because I use pointers directly (I assume) and array access is pretty fast in C# anyway.
Writes are a lot faster ~70% actually which is confusing but may be caused by the memory alignment.
Add however is ~5% slower and I can't figure out why.
Please note that there are zero allocations involved in the benchmark because both Lists keep their internal buffer alive. The only part left are the if conditions which are very similar to what List does internally.
Do you have some suggestions to make this even faster or can tell me why my list has slower Adds?
Benchmark results:




Method
Size
Mean
Error
StdDev
Ratio
RatioSD
CacheMisses/Op




Native_Add
100
934.59 ns
5.828 ns
4.866 ns
1.04
0.01
2


List_Add
100
894.44 ns
7.063 ns
6.607 ns
1.00
0.00
1












Native_Add
1000
9,463.38 ns
183.577 ns
162.736 ns
1.03
0.03
16


List_Add
1000
9,196.11 ns
87.191 ns
81.558 ns
1.00
0.00
17












Native_Add
10000
93,361.52 ns
876.444 ns
776.945 ns
1.05
0.01
112


List_Add
10000
88,501.43 ns
362.486 ns
302.692 ns
1.00
0.00
53












Native_Get
100
41.38 ns
0.371 ns
0.347 ns
0.94
0.02
0


List_Get
100
43.93 ns
0.849 ns
0.794 ns
1.00
0.00
0












Native_Get
1000
366.97 ns
4.445 ns
3.940 ns
0.98
0.02
0


List_Get
1000
374.97 ns
3.053 ns
2.550 ns
1.00
0.00
0












Native_Get
10000
3,674.04 ns
60.222 ns
56.331 ns
0.98
0.02
5


List_Get
10000
3,733.41 ns
35.220 ns
32.945 ns
1.00
0.00
4












Native_Set
100
56.57 ns
0.357 ns
0.279 ns
0.34
0.00
0


List_Set
100
164.29 ns
0.936 ns
0.875 ns
1.00
0.00
0












Native_Set
1000
493.52 ns
8.668 ns
8.108 ns
0.30
0.00
1


List_Set
1000
1,665.81 ns
10.073 ns
7.865 ns
1.00
0.00
2












Native_Set
10000
4,984.44 ns
94.866 ns
88.737 ns
0.30
0.01
10


List_Set
10000
16,699.32 ns
59.226 ns
49.456 ns
1.00
0.00
18



Struct:
public unsafe struct NativeList<T> : IDisposable where T : unmanaged {
    private static readonly int MinCapacity = UnsafeUtils.CacheLineSize / sizeof(T);
    private T* _buffer;
    private int _capacity;
    private int _count;

    public int Capacity => _capacity;
    public int Count => _count;
    public T this[int index] {
        get {
            if((uint)index >= (uint)_count) throw new ArgumentOutOfRangeException(nameof(index));
            return _buffer[index];
        }
        set {
            if((uint)index >= (uint)_count) throw new ArgumentOutOfRangeException(nameof(index));
            _buffer[index] = value;
        }
    }

    public NativeList() {
    }

    public NativeList(int capacity) {
        if(capacity != 0) Resize(capacity);
    }

    public void Add(T value) {
        EnsureCapacity(1);
        _buffer[_count] = value;
        _count++;
    }

    public void AddRange(T* values, int count) {
        if(count == 0) return;
        EnsureCapacity(count);
        UnsafeUtils.Copy<T>(values, 0, _buffer, _count, count);
        _count += count;
    }

    public bool Remove(T value) {
        var index = IndexOf(value);
        if(index < 0) return false;
        RemoveAt(index);
        return true;
    }

    public void RemoveAt(int index) {
        if(index >= _count) throw new ArgumentOutOfRangeException(nameof(index));

        _count--;
        if(_count != index) {
            UnsafeUtils.Copy<T>(_buffer, index + 1, _buffer, index, _count - index);
        }
    }
    public void RemoveRangeAt(int index, int count) {
        if(index < 0 || index >= _count) throw new ArgumentOutOfRangeException(nameof(index));
        if(count < 0 || index + count > _count) throw new ArgumentOutOfRangeException(nameof(count));
        if(count == 0) return;

        _count -= count;
        if(_count != index) {
            UnsafeUtils.Copy<T>(_buffer, index + count, _buffer, index, _count - index);
        }
        _count -= count;
    }

    public bool Contains(T value) {
        return IndexOf(value, EqualityComparer<T>.Default) >= 0;
    }
    public bool Contains(T value, IEqualityComparer<T> equalityComparer) {
        return IndexOf(value, equalityComparer) >= 0;
    }
    
    public int IndexOf(T value) {
        return IndexOf(value, EqualityComparer<T>.Default);
    }
    public int IndexOf(T value, IEqualityComparer<T> equalityComparer) {
        for(var i = 0; i < _count; i++) {
            if(equalityComparer.Equals(((T*)_buffer)[i], value)) return i;
        }
        return -1;
    }
    
    public void Clear() {
        _count = 0;
    }
    
    public Span<T> AsSpan() {
        return UnsafeUtils.CreateSpan<T>(_buffer, _count);
    }

    public Enumerator GetEnumerator() {
        return new Enumerator(_buffer, _count);
    }

    private void EnsureCapacity(int count) {
        var minCapacity = _count + count;
        if(minCapacity > _capacity) Resize(minCapacity);
    }

    private void Resize(int capacity) {
        capacity = Math.Max(MinCapacity, (int)BitOperations.RoundUpToPowerOf2((uint)capacity));
        _buffer = (T*)UnsafeUtils.AlignedRealloc<T>(_buffer, capacity);
        _capacity = capacity;
    }

    public void Dispose() {
        if(_buffer is null) return;
        UnsafeUtils.AlignedFree(_buffer);
        _capacity = 0;
        _buffer = null;
    }

    public struct Enumerator : IEnumerator<T> {
        private readonly void* _buffer;
        private readonly int _count;
        private int _index;
        
        public T Current { get; private set; }
        object IEnumerator.Current => Current;

        public Enumerator(void* buffer, int count) {
            _buffer = buffer;
            _count = count;
            _index = -1;
        }

        public bool MoveNext() {
            if(++_index >= _count) return false;
            Current = ((T*)_buffer)[_index];
            return true;
        }

        public void Reset() {
            _index = -1;
        }
        public void Dispose() {}
    }
}

Benchmark:
[SimpleJob]
[MemoryDiagnoser]
[HardwareCounters(HardwareCounter.CacheMisses)]
[GroupBenchmarksBy(BenchmarkLogicalGroupRule.ByCategory, BenchmarkLogicalGroupRule.ByParams)]
public class NativeListBenchmark {
    private NativeList<int> _fullNativeList = new();
    private List<int> _fullList = new();
    private Random _random;
    
    private NativeList<int> _nativeList = new();
    private List<int> _list = new();

    [Params(100, 1000, 10_000)]
    public int Size { get; set; }

    [GlobalSetup]
    public void Setup() {
        _random = new Random(1337);
        for(var i = 0; i < Size; i++) {
            _fullNativeList.Add(i);
        }
        for(var i = 0; i < Size; i++) {
            _fullList.Add(i);
        }
    }

    [GlobalCleanup]
    public void Cleanup() {
        _nativeList.Dispose();
        _fullNativeList.Dispose();
    }

    [Benchmark]
    [BenchmarkCategory(""Add"")]
    public void Native_Add() {
        _nativeList.Clear();
        for(var i = 0; i < Size; i++) {
            _nativeList.Add(_random.Next());
        }
    }
    [Benchmark]
    [BenchmarkCategory(""Get"")]
    public void Native_Get() {
        for(var i = 0; i < Size; i++) {
            _ = _fullNativeList[i];
        }
    }

    [Benchmark]
    [BenchmarkCategory(""Set"")]
    public void Native_Set() {
        for(var i = 0; i < Size; i++) {
            _fullNativeList[i] = i;
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(""Add"")]
    public void List_Add() {
        _list.Clear();
        for(var i = 0; i < Size; i++) {
            _list.Add(_random.Next());
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(""Get"")]
    public void List_Get() {
        for(var i = 0; i < Size; i++) {
            _ = _fullList[i];
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(""Set"")]
    public void List_Set() {
        for(var i = 0; i < Size; i++) {
            _fullList[i] = i;
        }
    }
}

","Why is a System.Collections.List<T>.Add faster than manual memory management?
Hey there I am currently experimenting with very low-level C# mainly to learn and bring my skills to the next level for a performance heavy project.
I tried to create sth like a NativeList so a List without any automatic garbage collection.
I want to use structs and manual pointer allocation to create a memory and cache-line aligned List implementation. This may look familiar for the Unity people out here.
The issue I am facing right now that I get some unexpected results after benchmarking using BenchmarkDotnet.
Reads are as expected a bit faster mainly because I use pointers directly (I assume) and array access is pretty fast in C# anyway.
Writes are a lot faster ~70% actually which is confusing but may be caused by the memory alignment.
Add however is ~5% slower and I can't figure out why.
Please note that there are zero allocations involved in the benchmark because both Lists keep their internal buffer alive. The only part left are the if conditions which are very similar to what List does internally.
Do you have some suggestions to make this even faster or can tell me why my list has slower Adds?
Benchmark results:




Method
Size
Mean
Error
StdDev
Ratio
RatioSD
CacheMisses/Op




Native_Add
100
934.59 ns
5.828 ns
4.866 ns
1.04
0.01
2


List_Add
100
894.44 ns
7.063 ns
6.607 ns
1.00
0.00
1












Native_Add
1000
9,463.38 ns
183.577 ns
162.736 ns
1.03
0.03
16


List_Add
1000
9,196.11 ns
87.191 ns
81.558 ns
1.00
0.00
17












Native_Add
10000
93,361.52 ns
876.444 ns
776.945 ns
1.05
0.01
112


List_Add
10000
88,501.43 ns
362.486 ns
302.692 ns
1.00
0.00
53












Native_Get
100
41.38 ns
0.371 ns
0.347 ns
0.94
0.02
0


List_Get
100
43.93 ns
0.849 ns
0.794 ns
1.00
0.00
0












Native_Get
1000
366.97 ns
4.445 ns
3.940 ns
0.98
0.02
0


List_Get
1000
374.97 ns
3.053 ns
2.550 ns
1.00
0.00
0












Native_Get
10000
3,674.04 ns
60.222 ns
56.331 ns
0.98
0.02
5


List_Get
10000
3,733.41 ns
35.220 ns
32.945 ns
1.00
0.00
4












Native_Set
100
56.57 ns
0.357 ns
0.279 ns
0.34
0.00
0


List_Set
100
164.29 ns
0.936 ns
0.875 ns
1.00
0.00
0












Native_Set
1000
493.52 ns
8.668 ns
8.108 ns
0.30
0.00
1


List_Set
1000
1,665.81 ns
10.073 ns
7.865 ns
1.00
0.00
2












Native_Set
10000
4,984.44 ns
94.866 ns
88.737 ns
0.30
0.01
10


List_Set
10000
16,699.32 ns
59.226 ns
49.456 ns
1.00
0.00
18



Struct:
public unsafe struct NativeList<T> : IDisposable where T : unmanaged {
    private static readonly int MinCapacity = UnsafeUtils.CacheLineSize / sizeof(T);
    private T* _buffer;
    private int _capacity;
    private int _count;

    public int Capacity => _capacity;
    public int Count => _count;
    public T this[int index] {
        get {
            if((uint)index >= (uint)_count) throw new ArgumentOutOfRangeException(nameof(index));
            return _buffer[index];
        }
        set {
            if((uint)index >= (uint)_count) throw new ArgumentOutOfRangeException(nameof(index));
            _buffer[index] = value;
        }
    }

    public NativeList() {
    }

    public NativeList(int capacity) {
        if(capacity != 0) Resize(capacity);
    }

    public void Add(T value) {
        EnsureCapacity(1);
        _buffer[_count] = value;
        _count++;
    }

    public void AddRange(T* values, int count) {
        if(count == 0) return;
        EnsureCapacity(count);
        UnsafeUtils.Copy<T>(values, 0, _buffer, _count, count);
        _count += count;
    }

    public bool Remove(T value) {
        var index = IndexOf(value);
        if(index < 0) return false;
        RemoveAt(index);
        return true;
    }

    public void RemoveAt(int index) {
        if(index >= _count) throw new ArgumentOutOfRangeException(nameof(index));

        _count--;
        if(_count != index) {
            UnsafeUtils.Copy<T>(_buffer, index + 1, _buffer, index, _count - index);
        }
    }
    public void RemoveRangeAt(int index, int count) {
        if(index < 0 || index >= _count) throw new ArgumentOutOfRangeException(nameof(index));
        if(count < 0 || index + count > _count) throw new ArgumentOutOfRangeException(nameof(count));
        if(count == 0) return;

        _count -= count;
        if(_count != index) {
            UnsafeUtils.Copy<T>(_buffer, index + count, _buffer, index, _count - index);
        }
        _count -= count;
    }

    public bool Contains(T value) {
        return IndexOf(value, EqualityComparer<T>.Default) >= 0;
    }
    public bool Contains(T value, IEqualityComparer<T> equalityComparer) {
        return IndexOf(value, equalityComparer) >= 0;
    }
    
    public int IndexOf(T value) {
        return IndexOf(value, EqualityComparer<T>.Default);
    }
    public int IndexOf(T value, IEqualityComparer<T> equalityComparer) {
        for(var i = 0; i < _count; i++) {
            if(equalityComparer.Equals(((T*)_buffer)[i], value)) return i;
        }
        return -1;
    }
    
    public void Clear() {
        _count = 0;
    }
    
    public Span<T> AsSpan() {
        return UnsafeUtils.CreateSpan<T>(_buffer, _count);
    }

    public Enumerator GetEnumerator() {
        return new Enumerator(_buffer, _count);
    }

    private void EnsureCapacity(int count) {
        var minCapacity = _count + count;
        if(minCapacity > _capacity) Resize(minCapacity);
    }

    private void Resize(int capacity) {
        capacity = Math.Max(MinCapacity, (int)BitOperations.RoundUpToPowerOf2((uint)capacity));
        _buffer = (T*)UnsafeUtils.AlignedRealloc<T>(_buffer, capacity);
        _capacity = capacity;
    }

    public void Dispose() {
        if(_buffer is null) return;
        UnsafeUtils.AlignedFree(_buffer);
        _capacity = 0;
        _buffer = null;
    }

    public struct Enumerator : IEnumerator<T> {
        private readonly void* _buffer;
        private readonly int _count;
        private int _index;
        
        public T Current { get; private set; }
        object IEnumerator.Current => Current;

        public Enumerator(void* buffer, int count) {
            _buffer = buffer;
            _count = count;
            _index = -1;
        }

        public bool MoveNext() {
            if(++_index >= _count) return false;
            Current = ((T*)_buffer)[_index];
            return true;
        }

        public void Reset() {
            _index = -1;
        }
        public void Dispose() {}
    }
}

Benchmark:
[SimpleJob]
[MemoryDiagnoser]
[HardwareCounters(HardwareCounter.CacheMisses)]
[GroupBenchmarksBy(BenchmarkLogicalGroupRule.ByCategory, BenchmarkLogicalGroupRule.ByParams)]
public class NativeListBenchmark {
    private NativeList<int> _fullNativeList = new();
    private List<int> _fullList = new();
    private Random _random;
    
    private NativeList<int> _nativeList = new();
    private List<int> _list = new();

    [Params(100, 1000, 10_000)]
    public int Size { get; set; }

    [GlobalSetup]
    public void Setup() {
        _random = new Random(1337);
        for(var i = 0; i < Size; i++) {
            _fullNativeList.Add(i);
        }
        for(var i = 0; i < Size; i++) {
            _fullList.Add(i);
        }
    }

    [GlobalCleanup]
    public void Cleanup() {
        _nativeList.Dispose();
        _fullNativeList.Dispose();
    }

    [Benchmark]
    [BenchmarkCategory(""Add"")]
    public void Native_Add() {
        _nativeList.Clear();
        for(var i = 0; i < Size; i++) {
            _nativeList.Add(_random.Next());
        }
    }
    [Benchmark]
    [BenchmarkCategory(""Get"")]
    public void Native_Get() {
        for(var i = 0; i < Size; i++) {
            _ = _fullNativeList[i];
        }
    }

    [Benchmark]
    [BenchmarkCategory(""Set"")]
    public void Native_Set() {
        for(var i = 0; i < Size; i++) {
            _fullNativeList[i] = i;
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(""Add"")]
    public void List_Add() {
        _list.Clear();
        for(var i = 0; i < Size; i++) {
            _list.Add(_random.Next());
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(""Get"")]
    public void List_Get() {
        for(var i = 0; i < Size; i++) {
            _ = _fullList[i];
        }
    }

    [Benchmark(Baseline = true)]
    [BenchmarkCategory(""Set"")]
    public void List_Set() {
        for(var i = 0; i < Size; i++) {
            _fullList[i] = i;
        }
    }
}

",0,1,1,0,0,0,0,- Is a comparison question,"Label: 1

Reasoning:

This question expresses technical debt because the user is trying to optimize their custom implementation of a `NativeList` in C# to make it more performant, specifically for adding elements, which is slower than the standard `List` implementation. They have already implemented a custom list with manual pointer allocation and memory alignment, which is a non-trivial task.

The user is seeking suggestions to further improve the performance of their implementation, which indicates that they are aware of the limitations and potential issues with their current code. The benchmark results provided show that the `Add` operation is slower in their implementation compared to the standard `List`, which is a clear indication of technical debt.

The user's efforts to optimize their implementation and the benchmarking results demonstrate that they are concerned about the performance and efficiency of their code, which is a key aspect of technical debt. The fact that they are seeking guidance from others to improve their implementation further supports the notion that this question expresses technical debt.

Additionally, the user's implementation is quite complex, involving manual memory management, pointer arithmetic, and custom structs, which can be error-prone and difficult to maintain. This complexity adds to the technical debt, as it may require significant refactoring or rewriting to achieve optimal performance.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,1,0,0,0,,,Design,Design,,Design,Very Mild,Very Mild,,Very Mild
21055946,21055946,Why does tree vectorization make this sorting algorithm 2x slower?,"<p>The sorting algorithm of <a href=""https://stackoverflow.com/q/21050130/341970"">this question</a> becomes twice faster(!) if <code>-fprofile-arcs</code> is enabled in gcc (4.7.2). The heavily simplified C code of that question (it turned out that I can initialize the array with all zeros, the weird performance behavior remains but it makes the reasoning much much simpler):</p>

<pre><code>#include &lt;time.h&gt;
#include &lt;stdio.h&gt;

#define ELEMENTS 100000

int main() {
  int a[ELEMENTS] = { 0 };
  clock_t start = clock();
  for (int i = 0; i &lt; ELEMENTS; ++i) {
    int lowerElementIndex = i;
    for (int j = i+1; j &lt; ELEMENTS; ++j) {
      if (a[j] &lt; a[lowerElementIndex]) {
        lowerElementIndex = j;
      }
    }
    int tmp = a[i];
    a[i] = a[lowerElementIndex];
    a[lowerElementIndex] = tmp;
  } 
  clock_t end = clock();
  float timeExec = (float)(end - start) / CLOCKS_PER_SEC;
  printf(""Time: %2.3f\n"", timeExec);
  printf(""ignore this line %d\n"", a[ELEMENTS-1]);
}
</code></pre>

<p>After playing with the optimization flags for a long while, it turned out that <code>-ftree-vectorize</code> also yields this weird behavior so we can take <code>-fprofile-arcs</code> out of the question. After profiling with <code>perf</code> I have found that <strong>the only relevant difference is:</strong></p>

<p>Fast case <code>gcc -std=c99 -O2 simp.c</code> (runs in 3.1s)</p>

<pre><code>    cmpl    %esi, %ecx
    jge .L3
    movl    %ecx, %esi
    movslq  %edx, %rdi
.L3:
</code></pre>

<p>Slow case <code>gcc -std=c99 -O2 -ftree-vectorize simp.c</code> (runs in 6.1s)</p>

<pre><code>    cmpl    %ecx, %esi
    cmovl   %edx, %edi
    cmovl   %esi, %ecx
</code></pre>

<p>As for the first snippet: Given that the array only contains zeros, we always jump to <code>.L3</code>. It can greatly benefit from branch prediction. </p>

<p>I guess the <code>cmovl</code> instructions cannot benefit from branch prediction. </p>

<hr>

<p><strong>Questions:</strong></p>

<ol>
<li><p>Are all my above guesses correct? Does this make the algorithm slow?</p></li>
<li><p>If yes, how can I prevent gcc from emitting this instruction (other than the trivial <code>-fno-tree-vectorization</code> workaround of course) but still doing as much optimizations as possible? </p></li>
<li><p>What is this <code>-ftree-vectorization</code>? <a href=""http://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html"" rel=""nofollow noreferrer"">The documentation</a> is quite
vague, I would need a little more explanation to understand what's happening.</p></li>
</ol>

<hr>

<p><strong>Update:</strong> Since it came up in comments: The weird performance behavior w.r.t. the <code>-ftree-vectorize</code> flag remains with random data. <strong><a href=""https://stackoverflow.com/questions/21055946/why-does-tree-vectorization-make-this-sorting-algorithm-2x-slower#comment31663424_21055946"">As Yakk points out</a>, for selection sort, it is actually hard to create a dataset that would result in a lot of branch mispredictions.</strong></p>

<p>Since it also came up: I have a Core i5 CPU.</p>

<hr>

<p><a href=""https://stackoverflow.com/questions/21055946/why-does-tree-vectorization-make-this-sorting-algorithm-2x-slower#comment31663424_21055946"">Based on Yakk's comment</a>, I created a test. The code below (<a href=""http://coliru.stacked-crooked.com/a/7f8bd41e406cc7c6"" rel=""nofollow noreferrer"">online without boost</a>) is of course no longer a sorting algorithm; I only took out the inner loop. Its only goal is to examine the effect of branch prediction: <strong>We skip the <code>if</code> branch in the <code>for</code> loop with probability <code>p</code>.</strong></p>

<pre><code>#include &lt;algorithm&gt;
#include &lt;cstdio&gt;
#include &lt;random&gt;
#include &lt;boost/chrono.hpp&gt;
using namespace std;
using namespace boost::chrono;
constexpr int ELEMENTS=1e+8; 
constexpr double p = 0.50;

int main() {
  printf(""p = %.2f\n"", p);
  int* a = new int[ELEMENTS];
  mt19937 mt(1759);
  bernoulli_distribution rnd(p);
  for (int i = 0 ; i &lt; ELEMENTS; ++i){
    a[i] = rnd(mt)? i : -i;
  }
  auto start = high_resolution_clock::now();
  int lowerElementIndex = 0;
  for (int i=0; i&lt;ELEMENTS; ++i) {
    if (a[i] &lt; a[lowerElementIndex]) {
      lowerElementIndex = i;
    }
  } 
  auto finish = high_resolution_clock::now();
  printf(""%ld  ms\n"", duration_cast&lt;milliseconds&gt;(finish-start).count());
  printf(""Ignore this line   %d\n"", a[lowerElementIndex]);
  delete[] a;
}
</code></pre>

<p>The loops of interest:</p>

<p>This will be referred to as <strong>cmov</strong></p>

<p><code>g++ -std=c++11 -O2 -lboost_chrono -lboost_system -lrt branch3.cpp</code> </p>

<pre><code>    xorl    %eax, %eax
.L30:
    movl    (%rbx,%rbp,4), %edx
    cmpl    %edx, (%rbx,%rax,4)
    movslq  %eax, %rdx
    cmovl   %rdx, %rbp
    addq    $1, %rax
    cmpq    $100000000, %rax
    jne .L30
</code></pre>

<hr>

<p>This will be referred to as <strong>no cmov</strong>, the <code>-fno-if-conversion</code> flag was pointed out by <a href=""https://stackoverflow.com/a/21057148/341970"">Turix in his answer.</a></p>

<p><code>g++ -std=c++11 -O2 -fno-if-conversion -lboost_chrono -lboost_system -lrt branch3.cpp</code></p>

<pre><code>    xorl    %eax, %eax
.L29:
    movl    (%rbx,%rbp,4), %edx
    cmpl    %edx, (%rbx,%rax,4)
    jge .L28
    movslq  %eax, %rbp
.L28:
    addq    $1, %rax
    cmpq    $100000000, %rax
    jne .L29
</code></pre>

<hr>

<p>The difference side by side </p>

<pre><code>cmpl    %edx, (%rbx,%rax,4) |     cmpl  %edx, (%rbx,%rax,4)
movslq  %eax, %rdx          |     jge   .L28
cmovl   %rdx, %rbp          |     movslq    %eax, %rbp
                            | .L28:
</code></pre>

<hr>

<p>The execution time as a function of the Bernoulli parameter <code>p</code></p>

<p><img src=""https://i.stack.imgur.com/7Tt0H.png"" alt=""effect of branch prediction""></p>

<p>The code with the <code>cmov</code> instruction is absolutely insensitive to <code>p</code>. The code <em>without</em> the <code>cmov</code> instruction is the winner if <code>p&lt;0.26</code> or <code>0.81&lt;p</code> and is at most 4.38x faster (<code>p=1</code>). Of course, the worse situation for the branch predictor is at around <code>p=0.5</code> where the code is 1.58x slower than the code with the <code>cmov</code> instruction. </p>
","the sorting algorithm of this question becomes twice faster if fprofilearcs is enabled in gcc 472 the heavily simplified c code of that question it turned out that i can initialize the array with all zeros the weird performance behavior remains but it makes the reasoning much much simpler
after playing with the optimization flags for a long while it turned out that ftreevectorize also yields this weird behavior so we can take fprofilearcs out of the question after profiling with perf i have found that the only relevant difference is
fast case gcc stdc99 o2 simpc runs in 31s
slow case gcc stdc99 o2 ftreevectorize simpc runs in 61s
as for the first snippet given that the array only contains zeros we always jump to l3 it can greatly benefit from branch prediction 
i guess the cmovl instructions cannot benefit from branch prediction 

questions

are all my above guesses correct does this make the algorithm slow
if yes how can i prevent gcc from emitting this instruction other than the trivial fnotreevectorization workaround of course but still doing as much optimizations as possible 
what is this ftreevectorization the documentation is quite
vague i would need a little more explanation to understand whats happening


update since it came up in comments the weird performance behavior wrt the ftreevectorize flag remains with random data as yakk points out for selection sort it is actually hard to create a dataset that would result in a lot of branch mispredictions
since it also came up i have a core i5 cpu

based on yakks comment i created a test the code below online without boost is of course no longer a sorting algorithm i only took out the inner loop its only goal is to examine the effect of branch prediction we skip the if branch in the for loop with probability p
the loops of interest
this will be referred to as cmov
g stdc11 o2 lboost_chrono lboost_system lrt branch3cpp 

this will be referred to as no cmov the fnoifconversion flag was pointed out by turix in his answer
g stdc11 o2 fnoifconversion lboost_chrono lboost_system lrt branch3cpp

the difference side by side 

the execution time as a function of the bernoulli parameter p

the code with the cmov instruction is absolutely insensitive to p the code without the cmov instruction is the winner if p026 or 081p and is at most 438x faster p1 of course the worse situation for the branch predictor is at around p05 where the code is 158x slower than the code with the cmov instruction 
",1,"The sorting algorithm of this question becomes twice faster(!) if -fprofile-arcs is enabled in gcc (4.7.2). The heavily simplified C code of that question (it turned out that I can initialize the array with all zeros, the weird performance behavior remains but it makes the reasoning much much simpler):
#include <time.h>
#include <stdio.h>

#define ELEMENTS 100000

int main() {
  int a[ELEMENTS] = { 0 };
  clock_t start = clock();
  for (int i = 0; i < ELEMENTS; ++i) {
    int lowerElementIndex = i;
    for (int j = i+1; j < ELEMENTS; ++j) {
      if (a[j] < a[lowerElementIndex]) {
        lowerElementIndex = j;
      }
    }
    int tmp = a[i];
    a[i] = a[lowerElementIndex];
    a[lowerElementIndex] = tmp;
  } 
  clock_t end = clock();
  float timeExec = (float)(end - start) / CLOCKS_PER_SEC;
  printf(""Time: %2.3f\n"", timeExec);
  printf(""ignore this line %d\n"", a[ELEMENTS-1]);
}

After playing with the optimization flags for a long while, it turned out that -ftree-vectorize also yields this weird behavior so we can take -fprofile-arcs out of the question. After profiling with perf I have found that the only relevant difference is:
Fast case gcc -std=c99 -O2 simp.c (runs in 3.1s)
    cmpl    %esi, %ecx
    jge .L3
    movl    %ecx, %esi
    movslq  %edx, %rdi
.L3:

Slow case gcc -std=c99 -O2 -ftree-vectorize simp.c (runs in 6.1s)
    cmpl    %ecx, %esi
    cmovl   %edx, %edi
    cmovl   %esi, %ecx

As for the first snippet: Given that the array only contains zeros, we always jump to .L3. It can greatly benefit from branch prediction. 
I guess the cmovl instructions cannot benefit from branch prediction. 

Questions:

Are all my above guesses correct? Does this make the algorithm slow?
If yes, how can I prevent gcc from emitting this instruction (other than the trivial -fno-tree-vectorization workaround of course) but still doing as much optimizations as possible? 
What is this -ftree-vectorization? The documentation is quite
vague, I would need a little more explanation to understand what's happening.


Update: Since it came up in comments: The weird performance behavior w.r.t. the -ftree-vectorize flag remains with random data. As Yakk points out, for selection sort, it is actually hard to create a dataset that would result in a lot of branch mispredictions.
Since it also came up: I have a Core i5 CPU.

Based on Yakk's comment, I created a test. The code below (online without boost) is of course no longer a sorting algorithm; I only took out the inner loop. Its only goal is to examine the effect of branch prediction: We skip the if branch in the for loop with probability p.
#include <algorithm>
#include <cstdio>
#include <random>
#include <boost/chrono.hpp>
using namespace std;
using namespace boost::chrono;
constexpr int ELEMENTS=1e+8; 
constexpr double p = 0.50;

int main() {
  printf(""p = %.2f\n"", p);
  int* a = new int[ELEMENTS];
  mt19937 mt(1759);
  bernoulli_distribution rnd(p);
  for (int i = 0 ; i < ELEMENTS; ++i){
    a[i] = rnd(mt)? i : -i;
  }
  auto start = high_resolution_clock::now();
  int lowerElementIndex = 0;
  for (int i=0; i<ELEMENTS; ++i) {
    if (a[i] < a[lowerElementIndex]) {
      lowerElementIndex = i;
    }
  } 
  auto finish = high_resolution_clock::now();
  printf(""%ld  ms\n"", duration_cast<milliseconds>(finish-start).count());
  printf(""Ignore this line   %d\n"", a[lowerElementIndex]);
  delete[] a;
}

The loops of interest:
This will be referred to as cmov
g++ -std=c++11 -O2 -lboost_chrono -lboost_system -lrt branch3.cpp 
    xorl    %eax, %eax
.L30:
    movl    (%rbx,%rbp,4), %edx
    cmpl    %edx, (%rbx,%rax,4)
    movslq  %eax, %rdx
    cmovl   %rdx, %rbp
    addq    $1, %rax
    cmpq    $100000000, %rax
    jne .L30


This will be referred to as no cmov, the -fno-if-conversion flag was pointed out by Turix in his answer.
g++ -std=c++11 -O2 -fno-if-conversion -lboost_chrono -lboost_system -lrt branch3.cpp
    xorl    %eax, %eax
.L29:
    movl    (%rbx,%rbp,4), %edx
    cmpl    %edx, (%rbx,%rax,4)
    jge .L28
    movslq  %eax, %rbp
.L28:
    addq    $1, %rax
    cmpq    $100000000, %rax
    jne .L29


The difference side by side 
cmpl    %edx, (%rbx,%rax,4) |     cmpl  %edx, (%rbx,%rax,4)
movslq  %eax, %rdx          |     jge   .L28
cmovl   %rdx, %rbp          |     movslq    %eax, %rbp
                            | .L28:


The execution time as a function of the Bernoulli parameter p

The code with the cmov instruction is absolutely insensitive to p. The code without the cmov instruction is the winner if p<0.26 or 0.81<p and is at most 4.38x faster (p=1). Of course, the worse situation for the branch predictor is at around p=0.5 where the code is 1.58x slower than the code with the cmov instruction. 
","Why does tree vectorization make this sorting algorithm 2x slower?
The sorting algorithm of this question becomes twice faster(!) if -fprofile-arcs is enabled in gcc (4.7.2). The heavily simplified C code of that question (it turned out that I can initialize the array with all zeros, the weird performance behavior remains but it makes the reasoning much much simpler):
#include <time.h>
#include <stdio.h>

#define ELEMENTS 100000

int main() {
  int a[ELEMENTS] = { 0 };
  clock_t start = clock();
  for (int i = 0; i < ELEMENTS; ++i) {
    int lowerElementIndex = i;
    for (int j = i+1; j < ELEMENTS; ++j) {
      if (a[j] < a[lowerElementIndex]) {
        lowerElementIndex = j;
      }
    }
    int tmp = a[i];
    a[i] = a[lowerElementIndex];
    a[lowerElementIndex] = tmp;
  } 
  clock_t end = clock();
  float timeExec = (float)(end - start) / CLOCKS_PER_SEC;
  printf(""Time: %2.3f\n"", timeExec);
  printf(""ignore this line %d\n"", a[ELEMENTS-1]);
}

After playing with the optimization flags for a long while, it turned out that -ftree-vectorize also yields this weird behavior so we can take -fprofile-arcs out of the question. After profiling with perf I have found that the only relevant difference is:
Fast case gcc -std=c99 -O2 simp.c (runs in 3.1s)
    cmpl    %esi, %ecx
    jge .L3
    movl    %ecx, %esi
    movslq  %edx, %rdi
.L3:

Slow case gcc -std=c99 -O2 -ftree-vectorize simp.c (runs in 6.1s)
    cmpl    %ecx, %esi
    cmovl   %edx, %edi
    cmovl   %esi, %ecx

As for the first snippet: Given that the array only contains zeros, we always jump to .L3. It can greatly benefit from branch prediction. 
I guess the cmovl instructions cannot benefit from branch prediction. 

Questions:

Are all my above guesses correct? Does this make the algorithm slow?
If yes, how can I prevent gcc from emitting this instruction (other than the trivial -fno-tree-vectorization workaround of course) but still doing as much optimizations as possible? 
What is this -ftree-vectorization? The documentation is quite
vague, I would need a little more explanation to understand what's happening.


Update: Since it came up in comments: The weird performance behavior w.r.t. the -ftree-vectorize flag remains with random data. As Yakk points out, for selection sort, it is actually hard to create a dataset that would result in a lot of branch mispredictions.
Since it also came up: I have a Core i5 CPU.

Based on Yakk's comment, I created a test. The code below (online without boost) is of course no longer a sorting algorithm; I only took out the inner loop. Its only goal is to examine the effect of branch prediction: We skip the if branch in the for loop with probability p.
#include <algorithm>
#include <cstdio>
#include <random>
#include <boost/chrono.hpp>
using namespace std;
using namespace boost::chrono;
constexpr int ELEMENTS=1e+8; 
constexpr double p = 0.50;

int main() {
  printf(""p = %.2f\n"", p);
  int* a = new int[ELEMENTS];
  mt19937 mt(1759);
  bernoulli_distribution rnd(p);
  for (int i = 0 ; i < ELEMENTS; ++i){
    a[i] = rnd(mt)? i : -i;
  }
  auto start = high_resolution_clock::now();
  int lowerElementIndex = 0;
  for (int i=0; i<ELEMENTS; ++i) {
    if (a[i] < a[lowerElementIndex]) {
      lowerElementIndex = i;
    }
  } 
  auto finish = high_resolution_clock::now();
  printf(""%ld  ms\n"", duration_cast<milliseconds>(finish-start).count());
  printf(""Ignore this line   %d\n"", a[lowerElementIndex]);
  delete[] a;
}

The loops of interest:
This will be referred to as cmov
g++ -std=c++11 -O2 -lboost_chrono -lboost_system -lrt branch3.cpp 
    xorl    %eax, %eax
.L30:
    movl    (%rbx,%rbp,4), %edx
    cmpl    %edx, (%rbx,%rax,4)
    movslq  %eax, %rdx
    cmovl   %rdx, %rbp
    addq    $1, %rax
    cmpq    $100000000, %rax
    jne .L30


This will be referred to as no cmov, the -fno-if-conversion flag was pointed out by Turix in his answer.
g++ -std=c++11 -O2 -fno-if-conversion -lboost_chrono -lboost_system -lrt branch3.cpp
    xorl    %eax, %eax
.L29:
    movl    (%rbx,%rbp,4), %edx
    cmpl    %edx, (%rbx,%rax,4)
    jge .L28
    movslq  %eax, %rbp
.L28:
    addq    $1, %rax
    cmpq    $100000000, %rax
    jne .L29


The difference side by side 
cmpl    %edx, (%rbx,%rax,4) |     cmpl  %edx, (%rbx,%rax,4)
movslq  %eax, %rdx          |     jge   .L28
cmovl   %rdx, %rbp          |     movslq    %eax, %rbp
                            | .L28:


The execution time as a function of the Bernoulli parameter p

The code with the cmov instruction is absolutely insensitive to p. The code without the cmov instruction is the winner if p<0.26 or 0.81<p and is at most 4.38x faster (p=1). Of course, the worse situation for the branch predictor is at around p=0.5 where the code is 1.58x slower than the code with the cmov instruction. 
",1,1,0,0,0,0,1,,,,1,1,Algorithm Optimization,Algorithm Optimization,,Algorithm Optimization,1,1,0,0,0,0,,,Architecture,Architecture,,Architecture,Mild,Mild,,Mild
50078674,50078674,Mybatis string value insert into oracle CLOB,"<p>I'm trying to insert list of records into oracle database by using mybatis orm.
Below exception throws while insert into database.</p>

<blockquote>
  <p>Error updating database.  Cause: java.sql.SQLException: ORA-01461: can bind a LONG value only for insert into a LONG column</p>
</blockquote>

<p>Query I have written to insert the data.</p>

<pre><code>&lt;insert id=""insertCusHistoryData"" parameterType=""com.dpworld.tos.admin.model.CusHistory""&gt;   
      INSERT INTO 
         CUS_ACTION_HISTORY_T(SEQ_NO, GLOBAL_ID, REQUEST_ID, CUS_ACTION_CODE, CUS_REQUEST_TIME, ENTITY_TYPE, ENTITY_KEY, PARENT_ENTITY_KEY,
             DELTA_CHANGE, TOTAL_PROCESSING_TIME, TOTAL_SERVER_PROCESSING_TIME, USER_AN, UPDATE_MODE, TOTAL_TIME)
            select  A.*  from(
     &lt;foreach collection=""cusHistory"" item=""history"" separator=""UNION ALL"" &gt;  
        SELECT 
                #{history.sequenceNo} as SEQ_NO,
                #{history.globalId} as GLOBAL_ID,
                #{history.requestId} as REQUEST_ID,
                #{history.cusActionCode} as CUS_ACTION_CODE,
                #{history.cusRequestTime} as CUS_REQUEST_TIME,
                #{history.entityType} as ENTITY_TYPE,
                #{history.entityKey} as ENTITY_KEY,
                #{history.parentEntityKey} as PARENT_ENTITY_KEY,
                #{history.deltaChange, jdbcType=CLOB},
                #{history.totalProcessingTime} as TOTAL_PROCESSING_TIME,
                #{history.totalServerProcessingTime} as TOTAL_SERVER_PROCESSING_TIME,
                #{history.userAn} as USER_AN,
                #{history.updateMode} as UPDATE_MODE,
                #{history.totalTime} as TOTAL_TIME
            FROM DUAL 
    &lt;/foreach&gt;)A 
</code></pre>

<p>  </p>

<p>But when I modify insertion code like below it inserts property, but it takes around 23 seconds tom complete the job.</p>

<pre><code>BEGIN
     &lt;foreach collection=""cusHistory"" item=""history"" separator="""" &gt; 
         INSERT INTO 
         CUS_ACTION_HISTORY_T(SEQ_NO, GLOBAL_ID, REQUEST_ID, CUS_ACTION_CODE, CUS_REQUEST_TIME, ENTITY_TYPE, ENTITY_KEY, PARENT_ENTITY_KEY,
            DELTA_CHANGE, TOTAL_PROCESSING_TIME, TOTAL_SERVER_PROCESSING_TIME, USER_AN, UPDATE_MODE, TOTAL_TIME)
         VALUES(
                #{history.sequenceNo},
                #{history.globalId},
                #{history.requestId},
                #{history.cusActionCode},
                #{history.cusRequestTime},
                #{history.entityType},
                #{history.entityKey},
                #{history.parentEntityKey},
                #{history.deltaChange, jdbcType=CLOB},
                #{history.totalProcessingTime},
                #{history.totalServerProcessingTime},
                #{history.userAn},
                #{history.updateMode},
                #{history.totalTime});
    &lt;/foreach&gt;
      END;
</code></pre>

<p>I have tried by removing the CLOB column from the first and second query, first query took around 8 seconds and second query took around 20 seconds to complete, huge performance issue comes in this case.</p>

<p>Please advice.</p>
","im trying to insert list of records into oracle database by using mybatis orm
below exception throws while insert into database

error updating database  cause javasqlsqlexception ora01461 can bind a long value only for insert into a long column

query i have written to insert the data
 
but when i modify insertion code like below it inserts property but it takes around 23 seconds tom complete the job
i have tried by removing the clob column from the first and second query first query took around 8 seconds and second query took around 20 seconds to complete huge performance issue comes in this case
please advice
",0,"I'm trying to insert list of records into oracle database by using mybatis orm.
Below exception throws while insert into database.

Error updating database.  Cause: java.sql.SQLException: ORA-01461: can bind a LONG value only for insert into a LONG column

Query I have written to insert the data.
<insert id=""insertCusHistoryData"" parameterType=""com.dpworld.tos.admin.model.CusHistory"">   
      INSERT INTO 
         CUS_ACTION_HISTORY_T(SEQ_NO, GLOBAL_ID, REQUEST_ID, CUS_ACTION_CODE, CUS_REQUEST_TIME, ENTITY_TYPE, ENTITY_KEY, PARENT_ENTITY_KEY,
             DELTA_CHANGE, TOTAL_PROCESSING_TIME, TOTAL_SERVER_PROCESSING_TIME, USER_AN, UPDATE_MODE, TOTAL_TIME)
            select  A.*  from(
     <foreach collection=""cusHistory"" item=""history"" separator=""UNION ALL"" >  
        SELECT 
                #{history.sequenceNo} as SEQ_NO,
                #{history.globalId} as GLOBAL_ID,
                #{history.requestId} as REQUEST_ID,
                #{history.cusActionCode} as CUS_ACTION_CODE,
                #{history.cusRequestTime} as CUS_REQUEST_TIME,
                #{history.entityType} as ENTITY_TYPE,
                #{history.entityKey} as ENTITY_KEY,
                #{history.parentEntityKey} as PARENT_ENTITY_KEY,
                #{history.deltaChange, jdbcType=CLOB},
                #{history.totalProcessingTime} as TOTAL_PROCESSING_TIME,
                #{history.totalServerProcessingTime} as TOTAL_SERVER_PROCESSING_TIME,
                #{history.userAn} as USER_AN,
                #{history.updateMode} as UPDATE_MODE,
                #{history.totalTime} as TOTAL_TIME
            FROM DUAL 
    </foreach>)A 

 
But when I modify insertion code like below it inserts property, but it takes around 23 seconds tom complete the job.
BEGIN
     <foreach collection=""cusHistory"" item=""history"" separator="""" > 
         INSERT INTO 
         CUS_ACTION_HISTORY_T(SEQ_NO, GLOBAL_ID, REQUEST_ID, CUS_ACTION_CODE, CUS_REQUEST_TIME, ENTITY_TYPE, ENTITY_KEY, PARENT_ENTITY_KEY,
            DELTA_CHANGE, TOTAL_PROCESSING_TIME, TOTAL_SERVER_PROCESSING_TIME, USER_AN, UPDATE_MODE, TOTAL_TIME)
         VALUES(
                #{history.sequenceNo},
                #{history.globalId},
                #{history.requestId},
                #{history.cusActionCode},
                #{history.cusRequestTime},
                #{history.entityType},
                #{history.entityKey},
                #{history.parentEntityKey},
                #{history.deltaChange, jdbcType=CLOB},
                #{history.totalProcessingTime},
                #{history.totalServerProcessingTime},
                #{history.userAn},
                #{history.updateMode},
                #{history.totalTime});
    </foreach>
      END;

I have tried by removing the CLOB column from the first and second query, first query took around 8 seconds and second query took around 20 seconds to complete, huge performance issue comes in this case.
Please advice.
","Mybatis string value insert into oracle CLOB
I'm trying to insert list of records into oracle database by using mybatis orm.
Below exception throws while insert into database.

Error updating database.  Cause: java.sql.SQLException: ORA-01461: can bind a LONG value only for insert into a LONG column

Query I have written to insert the data.
<insert id=""insertCusHistoryData"" parameterType=""com.dpworld.tos.admin.model.CusHistory"">   
      INSERT INTO 
         CUS_ACTION_HISTORY_T(SEQ_NO, GLOBAL_ID, REQUEST_ID, CUS_ACTION_CODE, CUS_REQUEST_TIME, ENTITY_TYPE, ENTITY_KEY, PARENT_ENTITY_KEY,
             DELTA_CHANGE, TOTAL_PROCESSING_TIME, TOTAL_SERVER_PROCESSING_TIME, USER_AN, UPDATE_MODE, TOTAL_TIME)
            select  A.*  from(
     <foreach collection=""cusHistory"" item=""history"" separator=""UNION ALL"" >  
        SELECT 
                #{history.sequenceNo} as SEQ_NO,
                #{history.globalId} as GLOBAL_ID,
                #{history.requestId} as REQUEST_ID,
                #{history.cusActionCode} as CUS_ACTION_CODE,
                #{history.cusRequestTime} as CUS_REQUEST_TIME,
                #{history.entityType} as ENTITY_TYPE,
                #{history.entityKey} as ENTITY_KEY,
                #{history.parentEntityKey} as PARENT_ENTITY_KEY,
                #{history.deltaChange, jdbcType=CLOB},
                #{history.totalProcessingTime} as TOTAL_PROCESSING_TIME,
                #{history.totalServerProcessingTime} as TOTAL_SERVER_PROCESSING_TIME,
                #{history.userAn} as USER_AN,
                #{history.updateMode} as UPDATE_MODE,
                #{history.totalTime} as TOTAL_TIME
            FROM DUAL 
    </foreach>)A 

 
But when I modify insertion code like below it inserts property, but it takes around 23 seconds tom complete the job.
BEGIN
     <foreach collection=""cusHistory"" item=""history"" separator="""" > 
         INSERT INTO 
         CUS_ACTION_HISTORY_T(SEQ_NO, GLOBAL_ID, REQUEST_ID, CUS_ACTION_CODE, CUS_REQUEST_TIME, ENTITY_TYPE, ENTITY_KEY, PARENT_ENTITY_KEY,
            DELTA_CHANGE, TOTAL_PROCESSING_TIME, TOTAL_SERVER_PROCESSING_TIME, USER_AN, UPDATE_MODE, TOTAL_TIME)
         VALUES(
                #{history.sequenceNo},
                #{history.globalId},
                #{history.requestId},
                #{history.cusActionCode},
                #{history.cusRequestTime},
                #{history.entityType},
                #{history.entityKey},
                #{history.parentEntityKey},
                #{history.deltaChange, jdbcType=CLOB},
                #{history.totalProcessingTime},
                #{history.totalServerProcessingTime},
                #{history.userAn},
                #{history.updateMode},
                #{history.totalTime});
    </foreach>
      END;

I have tried by removing the CLOB column from the first and second query, first query took around 8 seconds and second query took around 20 seconds to complete, huge performance issue comes in this case.
Please advice.
",1,0,0,0,0,0,1,- Suboptimal code ,,,1,1,Database Performance,Database Performance,,Database Performance,1,0,0,0,0,0,,,Code,Code,,Code,Moderate,Moderate,,Moderate
24810199,24810199,Efficient Methods of Comparing Text Files Simultaneously,"<p>I did check to see if any existing questions matched mine but I didn't see any, if I did, my mistake.</p>

<p>I have two text files to compare against each other, one is a temporary log file that is overwritten sometimes, and the other is a permanent log, which will collect and append all of the contents of the temp log into one file (it will collect new lines in the log since when it last checked and append the new lines to the end of the complete log). However after a point this may lead to the complete log becoming quite large and therefore not so efficient to compare against so i have been thinking about different methods to approach this.</p>

<p>my first idea is to ""buffer"" the temp log (being that it will normally be the smaller of the two) strings into a list and simply loop through the archive log and do something like:</p>

<pre><code>List&lt;String&gt; bufferedlines = new List&lt;string&gt;();
using (StreamReader ArchiveStream = new StreamReader(ArchivePath))
{
    if (bufferedlines.Contains(ArchiveStream.ReadLine()))
    {

    }
}
</code></pre>

<p>Now there is a couple of ways I could proceed from here, I could create yet another list to store the inconsistencies, close the read stream (I'm not sure you can both read and write at the same time, if you can that might make things easier for my options) then open a write stream in append mode and write the list to the file. alternatively, cutting out the buffering the inconsistencies, i could open a write stream while the files are being compared and on the spot write the lines that aren't matched.</p>

<p>The other method i could think of was limited by my knowledge of whether it could be done or not, which was rather than buffer either file, compare the streams side by side as they are read and append the lines on the fly. Something like:</p>

<pre><code>using (StreamReader ArchiveStream = new StreamReader(ArchivePath))
{
    using (StreamReader templogStream = new StreamReader(tempPath))
    {
        if (!(ArchiveStream.ReadAllLines.Contains(TemplogStream.ReadLine())))
        {
            //write the line to the file
        }
    }
}
</code></pre>

<p>As I said I'm not sure whether that would work or that it may be more efficient than the first method, so i figured i'd ask, see if anyone had insight into how this might properly be implemented, and whether it was the most efficient way or there was a better method out there.</p>
","i did check to see if any existing questions matched mine but i didnt see any if i did my mistake
i have two text files to compare against each other one is a temporary log file that is overwritten sometimes and the other is a permanent log which will collect and append all of the contents of the temp log into one file it will collect new lines in the log since when it last checked and append the new lines to the end of the complete log however after a point this may lead to the complete log becoming quite large and therefore not so efficient to compare against so i have been thinking about different methods to approach this
my first idea is to buffer the temp log being that it will normally be the smaller of the two strings into a list and simply loop through the archive log and do something like
now there is a couple of ways i could proceed from here i could create yet another list to store the inconsistencies close the read stream im not sure you can both read and write at the same time if you can that might make things easier for my options then open a write stream in append mode and write the list to the file alternatively cutting out the buffering the inconsistencies i could open a write stream while the files are being compared and on the spot write the lines that arent matched
the other method i could think of was limited by my knowledge of whether it could be done or not which was rather than buffer either file compare the streams side by side as they are read and append the lines on the fly something like
as i said im not sure whether that would work or that it may be more efficient than the first method so i figured id ask see if anyone had insight into how this might properly be implemented and whether it was the most efficient way or there was a better method out there
",1,"I did check to see if any existing questions matched mine but I didn't see any, if I did, my mistake.
I have two text files to compare against each other, one is a temporary log file that is overwritten sometimes, and the other is a permanent log, which will collect and append all of the contents of the temp log into one file (it will collect new lines in the log since when it last checked and append the new lines to the end of the complete log). However after a point this may lead to the complete log becoming quite large and therefore not so efficient to compare against so i have been thinking about different methods to approach this.
my first idea is to ""buffer"" the temp log (being that it will normally be the smaller of the two) strings into a list and simply loop through the archive log and do something like:
List<String> bufferedlines = new List<string>();
using (StreamReader ArchiveStream = new StreamReader(ArchivePath))
{
    if (bufferedlines.Contains(ArchiveStream.ReadLine()))
    {

    }
}

Now there is a couple of ways I could proceed from here, I could create yet another list to store the inconsistencies, close the read stream (I'm not sure you can both read and write at the same time, if you can that might make things easier for my options) then open a write stream in append mode and write the list to the file. alternatively, cutting out the buffering the inconsistencies, i could open a write stream while the files are being compared and on the spot write the lines that aren't matched.
The other method i could think of was limited by my knowledge of whether it could be done or not, which was rather than buffer either file, compare the streams side by side as they are read and append the lines on the fly. Something like:
using (StreamReader ArchiveStream = new StreamReader(ArchivePath))
{
    using (StreamReader templogStream = new StreamReader(tempPath))
    {
        if (!(ArchiveStream.ReadAllLines.Contains(TemplogStream.ReadLine())))
        {
            //write the line to the file
        }
    }
}

As I said I'm not sure whether that would work or that it may be more efficient than the first method, so i figured i'd ask, see if anyone had insight into how this might properly be implemented, and whether it was the most efficient way or there was a better method out there.
","Efficient Methods of Comparing Text Files Simultaneously
I did check to see if any existing questions matched mine but I didn't see any, if I did, my mistake.
I have two text files to compare against each other, one is a temporary log file that is overwritten sometimes, and the other is a permanent log, which will collect and append all of the contents of the temp log into one file (it will collect new lines in the log since when it last checked and append the new lines to the end of the complete log). However after a point this may lead to the complete log becoming quite large and therefore not so efficient to compare against so i have been thinking about different methods to approach this.
my first idea is to ""buffer"" the temp log (being that it will normally be the smaller of the two) strings into a list and simply loop through the archive log and do something like:
List<String> bufferedlines = new List<string>();
using (StreamReader ArchiveStream = new StreamReader(ArchivePath))
{
    if (bufferedlines.Contains(ArchiveStream.ReadLine()))
    {

    }
}

Now there is a couple of ways I could proceed from here, I could create yet another list to store the inconsistencies, close the read stream (I'm not sure you can both read and write at the same time, if you can that might make things easier for my options) then open a write stream in append mode and write the list to the file. alternatively, cutting out the buffering the inconsistencies, i could open a write stream while the files are being compared and on the spot write the lines that aren't matched.
The other method i could think of was limited by my knowledge of whether it could be done or not, which was rather than buffer either file, compare the streams side by side as they are read and append the lines on the fly. Something like:
using (StreamReader ArchiveStream = new StreamReader(ArchivePath))
{
    using (StreamReader templogStream = new StreamReader(tempPath))
    {
        if (!(ArchiveStream.ReadAllLines.Contains(TemplogStream.ReadLine())))
        {
            //write the line to the file
        }
    }
}

As I said I'm not sure whether that would work or that it may be more efficient than the first method, so i figured i'd ask, see if anyone had insight into how this might properly be implemented, and whether it was the most efficient way or there was a better method out there.
",0,1,0,1,0,0,1,,,,1,1,I/O Optimization,I/O Optimization,,I/O Optimization,0,1,0,1,0,0,,,Design,Design,,Design,Mild,Mild,,Mild
27762339,27762339,impact of bypassing variables,"<p>I don't know the right way to search for this question, but it's something I do a lot and wanted to figure out if I've been silently submarining my code all this time. Is something like this:</p>

<pre><code>Int32 x = 5;
Int32 y = 7;
Int32 z = x+y;

Console.Write(z.ToString());
</code></pre>

<p>The same as this:</p>

<pre><code>Int32 x = 5;
Int32 y = 7;

Console.Write((x+y).ToString());
</code></pre>

<p>I tend to write the first one much more than the second. It's more verbose, but I find debugging it a lot simpler because I can see what the result of a method is. </p>

<ul>
<li>Is there any difference, performance or JIT compiler wise, to those?</li>
<li>Does it matter whether I'm using value or reference types?</li>
</ul>

<p>I'm building and deploying a .net 4.5 application, 64 bit.</p>
","i dont know the right way to search for this question but its something i do a lot and wanted to figure out if ive been silently submarining my code all this time is something like this
the same as this
i tend to write the first one much more than the second its more verbose but i find debugging it a lot simpler because i can see what the result of a method is 

is there any difference performance or jit compiler wise to those
does it matter whether im using value or reference types

im building and deploying a net 45 application 64 bit
",1,"I don't know the right way to search for this question, but it's something I do a lot and wanted to figure out if I've been silently submarining my code all this time. Is something like this:
Int32 x = 5;
Int32 y = 7;
Int32 z = x+y;

Console.Write(z.ToString());

The same as this:
Int32 x = 5;
Int32 y = 7;

Console.Write((x+y).ToString());

I tend to write the first one much more than the second. It's more verbose, but I find debugging it a lot simpler because I can see what the result of a method is. 

Is there any difference, performance or JIT compiler wise, to those?
Does it matter whether I'm using value or reference types?

I'm building and deploying a .net 4.5 application, 64 bit.
","impact of bypassing variables
I don't know the right way to search for this question, but it's something I do a lot and wanted to figure out if I've been silently submarining my code all this time. Is something like this:
Int32 x = 5;
Int32 y = 7;
Int32 z = x+y;

Console.Write(z.ToString());

The same as this:
Int32 x = 5;
Int32 y = 7;

Console.Write((x+y).ToString());

I tend to write the first one much more than the second. It's more verbose, but I find debugging it a lot simpler because I can see what the result of a method is. 

Is there any difference, performance or JIT compiler wise, to those?
Does it matter whether I'm using value or reference types?

I'm building and deploying a .net 4.5 application, 64 bit.
",0,0,1,1,0,0,1,,,,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,0,1,1,0,0,,,Code,Code,,Code,Mild,Mild,,Mild
64195109,64195109,How to speedup this vectorized numpy-function?,"<p>I've been using
<code>@Divakars</code> function 'justify_nd' the last days and it works very well. Its very efficient when ran on <strong>large</strong> datasets.
I now have another problem:</p>
<p>The function sometimes also has to be called very often on smaller splits (millions of times).
Therefore I thought of adding a <code>@jit</code> decorator it via numba. unfortunately <code>numba</code> doesnt support <code>np.moveaxis()</code>.</p>
<p>So: is there a way to do this with <code>np.transpose()</code>?</p>
<h2>@Divakars function:</h2>
<pre><code>def justify_nd(a, invalid_val, axis, side):    
    &quot;&quot;&quot;
    Justify ndarray for the valid elements (that are not invalid_val).

    Parameters
    ----------
    A : ndarray
        Input array to be justified
    invalid_val : scalar
        invalid value
    axis : int
        Axis along which justification is to be made
    side : str
        Direction of justification. Must be 'front' or 'end'.
        So, with 'front', valid elements are pushed to the front and
        with 'end' valid elements are pushed to the end along specified axis.
    &quot;&quot;&quot;
    
    pushax = lambda a: np.moveaxis(a, axis, -1)
    if invalid_val is np.nan:
        mask = ~np.isnan(a)
    else:
        mask = a!=invalid_val
    justified_mask = np.sort(mask,axis=axis)
    
    if side=='front':
        justified_mask = np.flip(justified_mask,axis=axis)
            
    out = np.full(a.shape, invalid_val)
    if (axis==-1) or (axis==a.ndim-1):
        out[justified_mask] = a[mask]
    else:
        pushax(out)[pushax(justified_mask)] = pushax(a)[pushax(mask)]
    return out
</code></pre>
<h2>minimal reproducable sample:</h2>
<pre><code>import numpy as np, pandas as pd, random
len_ = 6; drop_Beginning_LEN = 3  ;np.random.seed(777); seed_2 = 5

for i in range(1000):# just calling it 1000 times

   ''' generating sample data '''
   
   a = pd.DataFrame({  &quot;A&quot;:np.random.uniform(low=0.0005, high=13.12333, size=(len_)),  &quot;B&quot;:np.random.uniform(low=0.0005, high=133.12333,       size=(len_)), &quot;C&quot;: np.random.uniform(low=0.0001, high=13.12333, size=(len_)), &quot;D&quot;:np.random.uniform(low=0.000005,    high=633242.12333,     size=(len_))}, dtype=np.float64)
   for col in a.columns: # place nans randomly
       a[col].iloc[random.sample(range(0, len_-1), int(len_*0.3))] = np.nan; random.seed(seed_2 ); seed_2+=1
  
   nans = np.asarray(pd.DataFrame(np.nan, index=range(len(a)), columns=a.columns))
   nans = list((map(lambda i: nans[:i], range(0,nans.shape[0]))))
   nans    = nans[::-1] # turning it around
   a = np.asarray(a) 
   a = list((map(lambda i: a[:i], range(1, a.shape[0]+1))))
   a = np.asarray([np.concatenate((x,y)) for x,y in zip(a,nans)])   
   a = np.asarray(a[(drop_Beginning_LEN-1):])

   ''' function '''
   a = justify_generic(a, invalid_val=np.nan, axis=1, side=&quot;end&quot;)
   #print(a)

</code></pre>
<h2>expected outcome:</h2>
<p>(when called with <code>len_ = 6; drop_Beginning_LEN =3 ;np.random.seed(777)</code>)</p>
<pre><code>''' 
as we see: the nans are pushed to the top, but the order of the valid values remains.
[[[           nan            nan            nan            nan]
  [           nan            nan            nan            nan]
  [2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]]

 [[           nan            nan            nan            nan]
  [2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]
  [1.09613882e+01 1.24306122e+01 8.94724630e+00 2.47374828e+05]]

 [[2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]
  [1.09613882e+01 1.24306122e+01 8.94724630e+00 2.47374828e+05]
  [1.21653247e+01 1.06085106e+01 7.24749319e+00 1.22318572e+05]]]
</code></pre>
<h2>Important:</h2>
<p>I only need the parametersets <code>invalid_val=np.nan, axis=1, side=&quot;end&quot;)</code></p>
<p>Edit: I forgot to place random nans in the dataframe.-&gt; Fixed.</p>
","ive been using
divakars function justify_nd the last days and it works very well its very efficient when ran on large datasets
i now have another problem
the function sometimes also has to be called very often on smaller splits millions of times
therefore i thought of adding a jit decorator it via numba unfortunately numba doesnt support npmoveaxis
so is there a way to do this with nptranspose
divakars function
minimal reproducable sample
expected outcome
when called with len_  6 drop_beginning_len 3 nprandomseed777
important
i only need the parametersets invalid_valnpnan axis1 sideend
edit i forgot to place random nans in the dataframe fixed
",1,"I've been using
@Divakars function 'justify_nd' the last days and it works very well. Its very efficient when ran on large datasets.
I now have another problem:
The function sometimes also has to be called very often on smaller splits (millions of times).
Therefore I thought of adding a @jit decorator it via numba. unfortunately numba doesnt support np.moveaxis().
So: is there a way to do this with np.transpose()?
@Divakars function:
def justify_nd(a, invalid_val, axis, side):    
    """"""
    Justify ndarray for the valid elements (that are not invalid_val).

    Parameters
    ----------
    A : ndarray
        Input array to be justified
    invalid_val : scalar
        invalid value
    axis : int
        Axis along which justification is to be made
    side : str
        Direction of justification. Must be 'front' or 'end'.
        So, with 'front', valid elements are pushed to the front and
        with 'end' valid elements are pushed to the end along specified axis.
    """"""
    
    pushax = lambda a: np.moveaxis(a, axis, -1)
    if invalid_val is np.nan:
        mask = ~np.isnan(a)
    else:
        mask = a!=invalid_val
    justified_mask = np.sort(mask,axis=axis)
    
    if side=='front':
        justified_mask = np.flip(justified_mask,axis=axis)
            
    out = np.full(a.shape, invalid_val)
    if (axis==-1) or (axis==a.ndim-1):
        out[justified_mask] = a[mask]
    else:
        pushax(out)[pushax(justified_mask)] = pushax(a)[pushax(mask)]
    return out

minimal reproducable sample:
import numpy as np, pandas as pd, random
len_ = 6; drop_Beginning_LEN = 3  ;np.random.seed(777); seed_2 = 5

for i in range(1000):# just calling it 1000 times

   ''' generating sample data '''
   
   a = pd.DataFrame({  ""A"":np.random.uniform(low=0.0005, high=13.12333, size=(len_)),  ""B"":np.random.uniform(low=0.0005, high=133.12333,       size=(len_)), ""C"": np.random.uniform(low=0.0001, high=13.12333, size=(len_)), ""D"":np.random.uniform(low=0.000005,    high=633242.12333,     size=(len_))}, dtype=np.float64)
   for col in a.columns: # place nans randomly
       a[col].iloc[random.sample(range(0, len_-1), int(len_*0.3))] = np.nan; random.seed(seed_2 ); seed_2+=1
  
   nans = np.asarray(pd.DataFrame(np.nan, index=range(len(a)), columns=a.columns))
   nans = list((map(lambda i: nans[:i], range(0,nans.shape[0]))))
   nans    = nans[::-1] # turning it around
   a = np.asarray(a) 
   a = list((map(lambda i: a[:i], range(1, a.shape[0]+1))))
   a = np.asarray([np.concatenate((x,y)) for x,y in zip(a,nans)])   
   a = np.asarray(a[(drop_Beginning_LEN-1):])

   ''' function '''
   a = justify_generic(a, invalid_val=np.nan, axis=1, side=""end"")
   #print(a)


expected outcome:
(when called with len_ = 6; drop_Beginning_LEN =3 ;np.random.seed(777))
''' 
as we see: the nans are pushed to the top, but the order of the valid values remains.
[[[           nan            nan            nan            nan]
  [           nan            nan            nan            nan]
  [2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]]

 [[           nan            nan            nan            nan]
  [2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]
  [1.09613882e+01 1.24306122e+01 8.94724630e+00 2.47374828e+05]]

 [[2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]
  [1.09613882e+01 1.24306122e+01 8.94724630e+00 2.47374828e+05]
  [1.21653247e+01 1.06085106e+01 7.24749319e+00 1.22318572e+05]]]

Important:
I only need the parametersets invalid_val=np.nan, axis=1, side=""end"")
Edit: I forgot to place random nans in the dataframe.-> Fixed.
","How to speedup this vectorized numpy-function?
I've been using
@Divakars function 'justify_nd' the last days and it works very well. Its very efficient when ran on large datasets.
I now have another problem:
The function sometimes also has to be called very often on smaller splits (millions of times).
Therefore I thought of adding a @jit decorator it via numba. unfortunately numba doesnt support np.moveaxis().
So: is there a way to do this with np.transpose()?
@Divakars function:
def justify_nd(a, invalid_val, axis, side):    
    """"""
    Justify ndarray for the valid elements (that are not invalid_val).

    Parameters
    ----------
    A : ndarray
        Input array to be justified
    invalid_val : scalar
        invalid value
    axis : int
        Axis along which justification is to be made
    side : str
        Direction of justification. Must be 'front' or 'end'.
        So, with 'front', valid elements are pushed to the front and
        with 'end' valid elements are pushed to the end along specified axis.
    """"""
    
    pushax = lambda a: np.moveaxis(a, axis, -1)
    if invalid_val is np.nan:
        mask = ~np.isnan(a)
    else:
        mask = a!=invalid_val
    justified_mask = np.sort(mask,axis=axis)
    
    if side=='front':
        justified_mask = np.flip(justified_mask,axis=axis)
            
    out = np.full(a.shape, invalid_val)
    if (axis==-1) or (axis==a.ndim-1):
        out[justified_mask] = a[mask]
    else:
        pushax(out)[pushax(justified_mask)] = pushax(a)[pushax(mask)]
    return out

minimal reproducable sample:
import numpy as np, pandas as pd, random
len_ = 6; drop_Beginning_LEN = 3  ;np.random.seed(777); seed_2 = 5

for i in range(1000):# just calling it 1000 times

   ''' generating sample data '''
   
   a = pd.DataFrame({  ""A"":np.random.uniform(low=0.0005, high=13.12333, size=(len_)),  ""B"":np.random.uniform(low=0.0005, high=133.12333,       size=(len_)), ""C"": np.random.uniform(low=0.0001, high=13.12333, size=(len_)), ""D"":np.random.uniform(low=0.000005,    high=633242.12333,     size=(len_))}, dtype=np.float64)
   for col in a.columns: # place nans randomly
       a[col].iloc[random.sample(range(0, len_-1), int(len_*0.3))] = np.nan; random.seed(seed_2 ); seed_2+=1
  
   nans = np.asarray(pd.DataFrame(np.nan, index=range(len(a)), columns=a.columns))
   nans = list((map(lambda i: nans[:i], range(0,nans.shape[0]))))
   nans    = nans[::-1] # turning it around
   a = np.asarray(a) 
   a = list((map(lambda i: a[:i], range(1, a.shape[0]+1))))
   a = np.asarray([np.concatenate((x,y)) for x,y in zip(a,nans)])   
   a = np.asarray(a[(drop_Beginning_LEN-1):])

   ''' function '''
   a = justify_generic(a, invalid_val=np.nan, axis=1, side=""end"")
   #print(a)


expected outcome:
(when called with len_ = 6; drop_Beginning_LEN =3 ;np.random.seed(777))
''' 
as we see: the nans are pushed to the top, but the order of the valid values remains.
[[[           nan            nan            nan            nan]
  [           nan            nan            nan            nan]
  [2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]]

 [[           nan            nan            nan            nan]
  [2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]
  [1.09613882e+01 1.24306122e+01 8.94724630e+00 2.47374828e+05]]

 [[2.00388024e+00 9.67793310e+01 7.73773690e+00 1.70253514e+05]
  [3.96827439e+00 1.02304892e+02 4.50583685e+00 2.36363566e+05]
  [8.14593324e-01 3.58378403e+01 1.29773491e+01 1.41167463e+05]
  [6.03516909e+00 8.57355020e+01 8.22145197e+00 1.18063308e+05]
  [1.09613882e+01 1.24306122e+01 8.94724630e+00 2.47374828e+05]
  [1.21653247e+01 1.06085106e+01 7.24749319e+00 1.22318572e+05]]]

Important:
I only need the parametersets invalid_val=np.nan, axis=1, side=""end"")
Edit: I forgot to place random nans in the dataframe.-> Fixed.
",0,1,1,0,0,0,0,- User just doesn't know how to approach a problem,"Label: 1

Reasoning: This question expresses technical debt because the user is attempting to optimize an existing function (`justify_nd`) for better performance. The function is already efficient when dealing with large datasets, but the user now needs to call it millions of times on smaller splits, which becomes a bottleneck. The user is exploring ways to further optimize the function using a `@jit` decorator from numba, which doesn't support `np.moveaxis()`, so they're asking if there's a way to achieve the same result using `np.transpose()`.

The presence of technical debt is evident in the need for optimization to improve performance, which suggests that the current implementation may not be scalable or efficient enough to handle the increased frequency of calls. While the function works well for large datasets, the user is now facing performance issues when calling it repeatedly on smaller splits. This indicates a need for refactoring or optimization to improve the function's design or efficiency, which is a characteristic of technical debt.",Yes,1,1,Runtime Efficiency,Runtime Efficiency,,Runtime Efficiency,0,1,1,0,0,0,,,Design,Design,,Design,Severe,Severe,,Severe
